{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import scipy\n",
    "import numpy as np \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('./utils/tecplot.mplstyle')\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# import neuralop \n",
    "# from utilities import *\n",
    "from utils.neuraloperator import * \n",
    "\n",
    "from pathlib import Path\n",
    "from torch import Tensor\n",
    "from typing import Any, List, Tuple, Mapping, Optional, Iterable, Union, Dict, Literal\n",
    "\n",
    "## libraries for CFD\n",
    "# import cupy as cp\n",
    "import h5py \n",
    "import yaml\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'name': '2dHIT_FNO', \n",
    "    'model': {\n",
    "        'name': 'FNO',\n",
    "        'params': {\n",
    "            'n_modes': (32, 32), \n",
    "            'in_channels': 1, \n",
    "            'out_channels': 1, \n",
    "            'hidden_channels': 32, \n",
    "            'lifting_channel_ratio': 4, \n",
    "            'projection_channel_ratio': 4, \n",
    "\n",
    "            # 'factorization':'tucker',\n",
    "            # 'implementation':'factorized',\n",
    "            # 'rank': 2, \n",
    "\n",
    "            'norm': 'instance_norm', # (None, 'instance_norm', 'group_norm', 'ada_in'); 'ada_in'은 에러남 \n",
    "            # 'fno_skip': 'linear', # ('linear', 'soft-gating', 'identity')\n",
    "            # 'channel_mlp_skip': 'linear', # ('soft-gating', 'linear', 'identity')\n",
    "            # 'positional_embedding': None, # (None, 'grid', GridEmbedding2D, GridEmbeddingnD)\n",
    "            # 'implementation': 'factorized', # ('factorized', 'reconstructed')\n",
    "            # 'fft_norm': 'forward', # ('forward', 'ortho', 'backward')\n",
    "            # 'fno_block_precision': 'full', # ('full', 'mixed', 'half')\n",
    "\n",
    "            # 'SpectralConv_initializer': 'zeros', # ('normal', 'uniform', 'constant', 'ones', 'zeros', 'eye', 'dirac', 'xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'trunc_normal', 'orthogonal', 'sparse')\n",
    "            # # 'SpectralConv_initializer_param': 0.1, # (None, ) \n",
    "            # 'SpectralConv_non_linearity': 'gelu', # ('gelu', 'relu', 'silu', 'tanh', 'sigmoid', 'leakyrelu') \n",
    "        \n",
    "        },\n",
    "        'device': device,\n",
    "    },\n",
    "    \n",
    "    'epochs': 50, \n",
    "    'optimizer': {\n",
    "        'loss_fn': 'h1',\n",
    "        'name': 'Adam',\n",
    "        'params': {\n",
    "            'lr': 1e-3,\n",
    "        },\n",
    "\n",
    "        'scheduler': {\n",
    "            'name': 'CosineAnnealingLR', \n",
    "        },\n",
    "    },\n",
    "    'data': [\n",
    "       { 'nu': 0.000225, 'leadtime': 0.25, # 'idx_leadtime': 1, \n",
    "         'stage': 'fit',\n",
    "        'params': {\n",
    "            'base_path': r'D:\\RESEARCH\\2DIso\\Data\\nu=0.00025_n=256_fDNS=64/', \n",
    "            'dataset_name': f'2dHIT_nu=0.000225_n=256_T=14.5_fDNS=64', \n",
    "            \n",
    "            'n_input': 1, \n",
    "            'n_output':1, \n",
    "            'batch_size': 64,\n",
    "            'num_workers': 0, \n",
    "\n",
    "            'Ndata_train':500,  \n",
    "            'Ndata_val':50, \n",
    "        },\n",
    "        'normalization': {\n",
    "            'normalization_path': r'D:\\RESEARCH\\2DIso\\Data\\nu=0.00025_n=256_fDNS=64/' + 'config.yaml',\n",
    "        }\n",
    "        },\n",
    "        {\n",
    "         'nu': 0.000225, 'leadtime': 0.25, # 'idx_leadtime': 1, \n",
    "         'stage': 'valid',\n",
    "        'params': {\n",
    "            'base_path': r'D:\\RESEARCH\\2DIso\\Data\\nu=0.00025_n=256/', \n",
    "            'dataset_name': f'2dHIT_nu=0.000225_n=256_T=14.5', \n",
    "            \n",
    "            'n_input': 1, \n",
    "            'n_output':1, \n",
    "            'batch_size': 64,\n",
    "            'num_workers': 0, \n",
    "\n",
    "            'Ndata_val':1, \n",
    "            # 'load_data': False, \n",
    "        },\n",
    "        'normalization': {\n",
    "            'normalization_path': r'D:\\RESEARCH\\2DIso\\Data\\nu=0.00025_n=256/' + 'config.yaml',\n",
    "        }\n",
    "        },\n",
    "        # {\n",
    "        #  'nu': 0.000225, 'leadtime': 0.25, # 'idx_leadtime': 1, \n",
    "        #  'stage': 'valid',\n",
    "        # 'params': {\n",
    "        #     'base_path': r'D:\\RESEARCH\\2DIso\\Data\\nu=0.00025_n=256_fDNS=64/', \n",
    "        #     'dataset_name': f'2dHIT_nu=0.000225_n=256_T=14.5_fDNS=64', \n",
    "        #     'target_base_path': r'D:\\RESEARCH\\2DIso\\Data\\nu=0.00025_n=256/', \n",
    "        #     'target_dataset_name': f'2dHIT_nu=0.000225_n=256_T=14.5', \n",
    "            \n",
    "        #     'n_input': 1, \n",
    "        #     'n_output':1, \n",
    "        #     'batch_size': 64,\n",
    "        #     'num_workers': 0, \n",
    "\n",
    "        #     'Ndata_val':1, \n",
    "        #     # 'load_data': False, \n",
    "        # },\n",
    "        # 'normalization': {\n",
    "        #     'normalization_path': r'D:\\RESEARCH\\2DIso\\Data\\nu=0.00025_n=256/' + 'config.yaml',\n",
    "        # }\n",
    "        # },\n",
    "       \n",
    "    ]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CFD equation parameters ###\n",
    "integral_timescales = {\n",
    "    1e-3: 1.870162606239319,\n",
    "    5e-4: 1.5808453559875488,\n",
    "    0.000225: 1.3786518573760986, \n",
    "    1e-4: 1.3038111189,\n",
    "    5e-05: 1.15829598903656, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.CFDFunction import *\n",
    "from utils.Losses import *\n",
    "from utils.Plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import torch \n",
    "import h5py\n",
    "import glob\n",
    "import time\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "def high2low_box(\n",
    "    x: torch.Tensor,\n",
    "    scale_factors: Union[float, Sequence[float]] = 0.5,\n",
    "    *,\n",
    "    keepdim: bool = False,\n",
    "    dim: int = 2,\n",
    "    fft_norm: str = \"backward\",\n",
    "    last_var_axis: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Fourier‑domain box‑style scaling (down / up sampling).\n",
    "\n",
    "    The function supports tensors shaped either as\n",
    "        • (batch, channels, *spatial_dims)\n",
    "        • (batch, channels, *spatial_dims, num_var)\n",
    "\n",
    "    In the second case the last axis (``num_var``) is *not* included in the FFT\n",
    "    and therefore remains unchanged.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(scale_factors, float):\n",
    "        scale_factors = [scale_factors] * dim\n",
    "    scale_factors = list(scale_factors)\n",
    "    if len(scale_factors) != dim:\n",
    "        raise ValueError(\"`scale_factors` length must equal `dim`.\")\n",
    "\n",
    "    if x.ndim < dim + 2:\n",
    "        raise ValueError(\n",
    "            f\"Input tensor must have at least {dim + 2} dimensions (got {x.ndim}).\"\n",
    "        )\n",
    "\n",
    "    if last_var_axis:\n",
    "        spatial_axes = list(range(-(dim + 1), -1))\n",
    "    else:\n",
    "        spatial_axes = list(range(-dim, 0))\n",
    "    spatial_shape = [x.shape[i] for i in spatial_axes]\n",
    "\n",
    "    sf_tensor = torch.tensor(scale_factors, dtype=x.dtype, device=x.device)\n",
    "    if torch.allclose(sf_tensor, torch.ones_like(sf_tensor)):\n",
    "        return x.clone()\n",
    "\n",
    "    Fx = torch.fft.fftn(x, dim=spatial_axes, norm=fft_norm)\n",
    "\n",
    "    if (sf_tensor < 1).any():\n",
    "        Fx = torch.fft.fftshift(Fx, dim=spatial_axes)\n",
    "\n",
    "        if keepdim:\n",
    "            mask = torch.ones_like(Fx, dtype=torch.bool)\n",
    "            for rel_ax, (n, sf) in enumerate(zip(spatial_shape, scale_factors)):\n",
    "                trim = int(round((1 - sf) * n / 2))\n",
    "                if trim == 0:\n",
    "                    continue\n",
    "                abs_ax = spatial_axes[rel_ax] % Fx.ndim  # positive index\n",
    "                low = torch.arange(trim, device=x.device)\n",
    "                high = torch.arange(n - trim, n, device=x.device)\n",
    "                mask.index_fill_(abs_ax, low, False)\n",
    "                mask.index_fill_(abs_ax, high, False)\n",
    "            Fx = Fx * mask\n",
    "        else:\n",
    "            slices = [slice(None)] * Fx.ndim\n",
    "            for rel_ax, (n, sf) in enumerate(zip(spatial_shape, scale_factors)):\n",
    "                trim = int(round((1 - sf) * n / 2))\n",
    "                slices[spatial_axes[rel_ax]] = slice(trim, n - trim)\n",
    "            Fx = Fx[tuple(slices)]\n",
    "\n",
    "        Fx = torch.fft.ifftshift(Fx, dim=spatial_axes)\n",
    "\n",
    "    # ----------------------------- up‑sampling branch -------------------------\n",
    "    if (sf_tensor > 1).any():\n",
    "        Fx = torch.fft.fftshift(Fx, dim=spatial_axes)\n",
    "\n",
    "        # F.pad pads the *last* k dims; build list accordingly\n",
    "        last_axes = list(range(-len(spatial_axes) - (1 if last_var_axis else 0), 0))\n",
    "        pads: List[int] = []\n",
    "        for ax in Reflectiond(last_axes):\n",
    "            if ax in spatial_axes:\n",
    "                rel = spatial_axes.index(ax)\n",
    "                n = spatial_shape[rel]\n",
    "                sf = scale_factors[rel]\n",
    "                extra = int(round((sf - 1) * n))\n",
    "                pads.extend([extra // 2, extra - extra // 2])\n",
    "            else:  # var axis → no pad\n",
    "                pads.extend([0, 0])\n",
    "        Fx = F.pad(Fx, pads, mode=\"constant\", value=0.0)\n",
    "        Fx = torch.fft.ifftshift(Fx, dim=spatial_axes)\n",
    "\n",
    "    # --------------------------------------------------- inverse FFT ----------\n",
    "    x_out = torch.fft.ifftn(Fx, dim=spatial_axes, norm=fft_norm).real\n",
    "\n",
    "    # ------------------------------ match statistics --------------------------\n",
    "    def _mom(t: torch.Tensor):\n",
    "        mean = t.mean(dim=spatial_axes, keepdim=True)\n",
    "        std = t.std(dim=spatial_axes, unbiased=False, keepdim=True).clamp_min(1e-12)\n",
    "        return mean, std\n",
    "\n",
    "    mean_in, std_in = _mom(x)\n",
    "    mean_out, std_out = _mom(x_out)\n",
    "\n",
    "    x_out = (x_out - mean_out) / std_out * std_in + mean_in\n",
    "    return x_out\n",
    "\n",
    "class HIT2dDataset(Dataset):\n",
    "    def __init__(self, \n",
    "        path: Optional[str] = None,\n",
    "        base_path: Optional[str] = None,\n",
    "        dataset_name: Optional[str] = None,\n",
    "        split_name: Optional[str] = None,\n",
    "\n",
    "        target_path: Optional[str] = None,\n",
    "        target_base_path: Optional[str] = None,\n",
    "        target_dataset_name: Optional[str] = None,\n",
    "        \n",
    "        normalization:Optional[callable] = None,\n",
    "        transform:Optional[callable] = None,\n",
    "\n",
    "        n_input: int = 1,\n",
    "        n_output: int = 1,\n",
    "        n_stride: int = 0,\n",
    "        max_rollout_steps=100,\n",
    "        max_n_sim: Optional[int] = None,\n",
    "        # batch_size:int=32, \n",
    "        \n",
    "        # num_iteration_per_data:int=None, \n",
    "        # isDataAugmentation:bool=False,\n",
    "        load_data:bool=True,\n",
    "        verbose:bool=True,\n",
    "        **kwargs, \n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.base_path = base_path\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split_name = split_name\n",
    "        if not path: \n",
    "            path = os.path.join(self.base_path, self.split_name, self.dataset_name) # f\"{self.base_path}/{self.split_name}/{self.dataset_name}*\"\n",
    "        self.path = sorted(glob.glob(f\"{path}*\"))\n",
    "        assert self.path, f\"Error: Dataset path {path} does not exist.\"\n",
    "\n",
    "        self.target_path = self.path\n",
    "        if target_path is not None or target_base_path is not None:\n",
    "            self.target_base_path = target_base_path\n",
    "            self.target_dataset_name = target_dataset_name\n",
    "            self.target_split_name = split_name\n",
    "            if not target_path: \n",
    "                target_path = os.path.join(self.target_base_path, self.split_name, self.target_dataset_name) # f\"{self.base_path}/{self.split_name}/{self.dataset_name}*\"\n",
    "            self.target_path = sorted(glob.glob(f\"{target_path}*\"))\n",
    "            assert self.target_path, f\"Error: Dataset path {target_path} does not exist.\"\n",
    "\n",
    "        self.normalization = normalization\n",
    "        self.transform = transform\n",
    "\n",
    "        self.max_rollout_steps = max_rollout_steps\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.n_stride = n_stride\n",
    "        self.max_n_sim = max_n_sim if max_n_sim else np.inf\n",
    "        \n",
    "        self.verbose=verbose\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        self._build_metadata()\n",
    "        self._calc_len()\n",
    "        self.load_data = load_data\n",
    "        if load_data:\n",
    "            self.data = self._load_data(self.path)\n",
    "            self.target_data = self._load_data(self.target_path) if self.target_path is not None else self.data\n",
    "\n",
    "    def _calc_len(self):\n",
    "        self.n_sim = min(self.max_n_sim, sum(self.n_sim_per_file)) # self.n_sim = sum(self.n_sim_per_file) # len(self.data)\n",
    "        self.n_steps_per_sim = self.Nt\n",
    "        self.n_windows_per_sim = self.n_steps_per_sim - (self.n_input + self.n_output + self.n_stride) + 1\n",
    "        self.len = self.n_sim * self.n_windows_per_sim\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        data = self._load_one_sample(idx)\n",
    "        data = self._preprocess_data(data)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        if self.normalization:\n",
    "            data = self.normalization(data)      \n",
    "        data = self._postprocess_data(data)\n",
    "        return data\n",
    "    \n",
    "    def _load_one_sample(self, idx:int):\n",
    "        isim = idx // self.n_windows_per_sim\n",
    "        it = idx % self.n_windows_per_sim\n",
    "        if self.load_data:\n",
    "            data = self.data[isim, it:it + self.n_input] # (n_in, channels, *datashape)\n",
    "            target = self.target_data[isim, it + self.n_input + self.n_stride:it + self.n_input + self.n_stride + self.n_output] # (n_out, channels, *datashape)\n",
    "        else: \n",
    "            for i in range(len(self.n_sim_per_file)):\n",
    "                ifile = i \n",
    "                if isim < sum(self.n_sim_per_file[:i+1]): break \n",
    "            \n",
    "            isim = isim - sum(self.n_sim_per_file[:i])\n",
    "            with h5py.File(self.path[ifile], 'r') as f:\n",
    "                data = f[\"fields\"]['vorticity'][isim, it:it + self.n_input] # (n_in, channels, *datashape)\n",
    "                data = torch.from_numpy(data)\n",
    "                \n",
    "            with h5py.File(self.target_path[ifile], 'r') as f:\n",
    "                target = f[\"fields\"]['vorticity'][isim, it + self.n_input + self.n_stride:it + self.n_input + self.n_stride + self.n_output] # (n_out, channels, *datashape)\n",
    "                target = torch.from_numpy(target)\n",
    "        return data, target\n",
    "\n",
    "    def _load_data(self, path=None,):\n",
    "        if not path: path = self.path\n",
    "        data = []\n",
    "        self.n_sim = 0\n",
    "        if self.verbose: start_time = time.time()\n",
    "        for p in path:\n",
    "            with h5py.File(p, 'r') as f:\n",
    "                _n_sim = f.attrs[\"n_trajectories\"]\n",
    "                \n",
    "                end = min(self.max_n_sim - self.n_sim, _n_sim)\n",
    "                vorticity = f[\"fields\"]['vorticity'][:end]# vorticity = f[\"fields\"]['vorticity'][:]\n",
    "                if self.verbose: \n",
    "                    print(f'Data Loaded from{p}. shape: {vorticity.shape}, memory: {vorticity.nbytes/1e6} (MB)')\n",
    "            \n",
    "            data.append(vorticity)\n",
    "            self.n_sim += len(vorticity)\n",
    "            if self.max_n_sim <= self.n_sim: break\n",
    "            \n",
    "        data = np.concatenate(data, axis=0)\n",
    "        memory = data.nbytes\n",
    "        data = torch.from_numpy(data)\n",
    "        # self.data = self.data.reshape(self.data.size(0) * self.data.size(1), self.data.size()[2:]) # self.data = torch.concat(self.data, dim=0)\n",
    "        if self.verbose: print(f'Data Loaded. shape: {data.shape}, dtype: {data.dtype}, time: {time.time() - start_time} (sec), memory: {memory/1e6} (MB)')\n",
    "        self._calc_len()\n",
    "        return data\n",
    "        \n",
    "    def _build_metadata(self):\n",
    "        \n",
    "        with h5py.File(self.path[0], 'r') as f:\n",
    "            self.nu = f['scalars']['nu'][()]\n",
    "\n",
    "            self.t = t = f[\"dimensions\"][\"t\"][:]\n",
    "            self.x = x = f[\"dimensions\"][\"x\"][:]\n",
    "            self.y = y = f[\"dimensions\"][\"y\"][:]\n",
    "            self.X, self.Y = np.meshgrid(x, y)\n",
    "\n",
    "            self.Lx = x[-1] - x[0]\n",
    "            self.Ly = y[-1] - y[0]\n",
    "            self.dx = x[1] - x[0]\n",
    "            self.dy = y[1] - y[0]\n",
    "            self.Nx = len(x)\n",
    "            self.Ny = len(y)\n",
    "            self.dt = t[1] - t[0]\n",
    "            self.Nt = len(t)\n",
    "            self.T = t[-1]\n",
    "            self.t0 = t[0]\n",
    "\n",
    "        self.n_sim_per_file = []\n",
    "        for path in self.path:\n",
    "            with h5py.File(path, 'r') as f:\n",
    "                n_sim = f.attrs[\"n_trajectories\"]\n",
    "                self.n_sim_per_file.append(n_sim)\n",
    "                if self.max_n_sim <= sum(self.n_sim_per_file): break\n",
    "        \n",
    "        self.metadata = {\n",
    "            'nu': self.nu,\n",
    "\n",
    "            'Lx': self.Lx,\n",
    "            'Ly': self.Ly,\n",
    "            'dx': self.dx,\n",
    "            'dy': self.dy,\n",
    "            'Nx': self.Nx, \n",
    "            'Ny': self.Ny,\n",
    "            'x': self.x,\n",
    "            'y': self.y,\n",
    "\n",
    "            'dt': self.dt,\n",
    "            'Nt': self.Nt,\n",
    "            'T': self.T,\n",
    "            't0': self.t0,\n",
    "            't': self.t,\n",
    "        }\n",
    "        if self.verbose:\n",
    "            print(f\"Loaded dataset metadata: {self.metadata.keys()}\")\n",
    "        return self.metadata\n",
    "    \n",
    "    def _preprocess_data(self, data):\n",
    "        data, target = data\n",
    "\n",
    "        x = [data, target] # x = torch.cat([data, target], dim=0)\n",
    "\n",
    "        return x\n",
    "    def _postprocess_data(self, data):\n",
    "        data, target = data[0], data[1]\n",
    "\n",
    "        # data = data.flatten(0, 1) # data.reshape(data.shape[0] * data.shape[1], *data.shape[2:]) # (n_in * channels, *datashape)\n",
    "        # target = target.flatten(0, 1) # target.reshape(target.shape[0] * target.shape[1], *target.shape[2:]) # (n_out * channels, *datashape)\n",
    "\n",
    "        return data, target\n",
    "\n",
    "class CustomDataModule:\n",
    "    def __init__(self, \n",
    "                 batch_size:int=32, \n",
    "                 Ndata_train:int=500, \n",
    "                 Ndata_val:int=50, \n",
    "                 Ndata_test:int=100, \n",
    "                 num_workers:int=0,\n",
    "                 transform:Optional[callable]=None,\n",
    "                 normalization:Optional[callable]=None,\n",
    "                 *args, **kwargs\n",
    "                 ):\n",
    "        self.batch_size = batch_size\n",
    "        self.Ndata_train = Ndata_train\n",
    "        self.Ndata_val = Ndata_val\n",
    "        self.Ndata_test = Ndata_test\n",
    "\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.transform = transform\n",
    "        self.normalization = normalization\n",
    "\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    \n",
    "    def setup(self, stage:str=None):\n",
    "        if stage in ['train', 'fit', None]:\n",
    "            self.train_dataset = HIT2dDataset(split_name='train', max_n_sim=self.Ndata_train, transform=self.transform, normalization=self.normalization, *self.args, **self.kwargs, )\n",
    "            print(f'Train dataset: {len(self.train_dataset)}')\n",
    "            \n",
    "        if stage in ['valid', 'fit', None]:\n",
    "            self.val_dataset = HIT2dDataset(split_name='valid', max_n_sim=self.Ndata_val, transform=None, normalization=self.normalization, *self.args, **self.kwargs, )\n",
    "            print(f'Val dataset: {len(self.val_dataset)}')\n",
    "\n",
    "        if stage in ['test', None]:\n",
    "            self.test_dataset = HIT2dDataset(split_name='valid', max_n_sim=self.Ndata_test, transform=None, normalization=self.normalization, *self.args, **self.kwargs, )\n",
    "            print(f'Test dataset: {len(self.test_dataset)}')\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.train_dataset is None: self.setup(stage='train')\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, \n",
    "                                           batch_size = self.batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=self.num_workers, \n",
    "                                           pin_memory=True,\n",
    "                                           drop_last=True, \n",
    "                                           )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        if self.val_dataset is None: self.setup(stage='valid')\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, \n",
    "                                           batch_size = self.batch_size, \n",
    "                                           shuffle=False, \n",
    "                                           num_workers=self.num_workers,\n",
    "                                           pin_memory=True,\n",
    "                                           drop_last=True,\n",
    "                                           )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataset is None:self.setup(stage='test')\n",
    "        test_batch_size = self.batch_size # self.test_dataset.n_windows_per_sim # // 8\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, \n",
    "                                           batch_size = test_batch_size, \n",
    "                                           shuffle=False, \n",
    "                                           num_workers=self.num_workers, \n",
    "                                           pin_memory=True,\n",
    "                                           drop_last=True,\n",
    "                                           )\n",
    "\n",
    "\n",
    "class RandomShift(nn.Module):\n",
    "    def __init__(self, \n",
    "        shifts:Union[float, Sequence[float]]=(0.5, 0.5), \n",
    "        dims:Union[int, Sequence[int]]=(-2, -1),\n",
    "        *args, **kwargs\n",
    "        ):\n",
    "        super(RandomShift, self).__init__()\n",
    "        self.shifts = shifts\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, \n",
    "        x, \n",
    "        shifts:Union[float, Sequence[float]]=None, \n",
    "        dims:Union[int, Sequence[int]]=None, \n",
    "        ):\n",
    "        if shifts is None: shifts = self.shifts\n",
    "        if dims is None: dims = self.dims\n",
    "        \n",
    "        return self.RandomShift(x, shifts=self.shifts, dims=self.dims)\n",
    "    \n",
    "    def RandomShift(self,\n",
    "        x, \n",
    "        shifts:Union[float, Sequence[float]]=(0.5, 0.5), \n",
    "        dims:Union[int, Sequence[int]]=(-2, -1)\n",
    "        ):\n",
    "        '''\n",
    "        Random shift the input tensor along the spatial dimensions. \n",
    "        Parameters:\n",
    "        - x (torch.tensor): input tensor, shape \n",
    "        - shifts (float or sequence of float): maximum shift fraction along each dimension. \n",
    "            If float, the same shift fraction is applied to all dimensions.\n",
    "            If sequence of float, the length must be equal to the number of spatial dimensions.\n",
    "            The shift fraction is relative to the size of the dimension.\n",
    "        - dims (int or sequence of int): dimensions to apply the shift. \n",
    "            If int, the same dimension is applied to all spatial dimensions.\n",
    "            If sequence of int, the length must be equal to the number of spatial dimensions.\n",
    "        \n",
    "        Returns:\n",
    "        - x (torch.tensor): shifted tensor, shape \n",
    "        '''\n",
    "        if isinstance(shifts, float): shifts = [shifts] * len(dims)\n",
    "        assert len(shifts) == len(dims), \"Length of shifts and dims must be equal to the number of spatial dimensions.\"\n",
    "\n",
    "        shifts = [int(np.random.uniform(-s, s) * x.shape[d]) for s, d in zip(shifts, dims)]\n",
    "        x = torch.roll(x, shifts=shifts, dims=dims)\n",
    "        return x\n",
    "\n",
    "class Reflection(nn.Module):\n",
    "    def __init__(self, \n",
    "        p: float=0.5,\n",
    "        dims:Union[int, Sequence[int]]=(-2, -1),\n",
    "        *args, **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, \n",
    "        x, \n",
    "        p = None, \n",
    "        dims:Union[int, Sequence[int]]=None, \n",
    "        ):\n",
    "        if dims is None: dims = self.dims\n",
    "        if p is None: p = self.p\n",
    "        \n",
    "        return self.Reflection(x, p=p, dims=self.dims)\n",
    "    \n",
    "    def Reflection(self,\n",
    "        x, \n",
    "        p: float=0.5, \n",
    "        dims:Union[int, Sequence[int]]=(-2, -1)\n",
    "        ):\n",
    "        '''\n",
    "        Reflection the input tensor along the spatial dimensions. \n",
    "        Parameters:\n",
    "        - x (torch.tensor): input tensor, shape \n",
    "        - dims (int or sequence of int): dimensions to apply the Reflection. \n",
    "            If int, the same dimension is applied to all spatial dimensions.\n",
    "            If sequence of int, the length must be equal to the number of spatial dimensions.\n",
    "        \n",
    "        Returns:\n",
    "        - x (torch.tensor): Reflection tensor, shape \n",
    "        '''\n",
    "        if isinstance(dims, int): dims = [dims]\n",
    "        \n",
    "        for d in dims:\n",
    "            if np.random.rand() < p: x = torch.flip(x, dims=(d,))\n",
    "        return x\n",
    "\n",
    "class Reverse(nn.Module):\n",
    "    def __init__(self, \n",
    "        p: float=0.5,\n",
    "        *args, **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, \n",
    "        x, \n",
    "        p = None, \n",
    "        ):\n",
    "        if p is None: p = self.p\n",
    "        \n",
    "        return self.Reverse(x, p=p)\n",
    "    \n",
    "    def Reverse(self,\n",
    "        x, \n",
    "        p: float=0.5, \n",
    "        ):\n",
    "        '''\n",
    "        Reverse the input tensor along the time dimension. \n",
    "        Parameters:\n",
    "        - x (torch.tensor): input tensor, shape \n",
    "        - dim (int): dimension to apply the Reverse. \n",
    "        \n",
    "        Returns:\n",
    "        - x (torch.tensor): Reverse tensor, shape \n",
    "        '''\n",
    "        if np.random.rand() < p: x = -x \n",
    "        return x\n",
    "    \n",
    "class CustomTransform(nn.Module):\n",
    "    def __init__(self, \n",
    "                *args, **kwargs, \n",
    "                ):\n",
    "        super(CustomTransform, self).__init__()\n",
    "        # self.file_dir = file_dir\n",
    "        # self.Ndata = Ndata\n",
    "        self.kwargs = kwargs \n",
    "        self.build_transform()\n",
    "\n",
    "    def build_transform(self):\n",
    "        self.transforms = nn.Sequential(\n",
    "            RandomShift(shifts=0.5, dims=(-2, -1)),\n",
    "            Reflection(p=0.5, dims=(-2, -1)),\n",
    "            Reverse(p=0.5),\n",
    "        )\n",
    "        return self.transforms\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transform(x)\n",
    "        return x\n",
    "    \n",
    "    def transform(self, x):\n",
    "        for t in self.transforms:\n",
    "            x = t(x)\n",
    "        return x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        for t in self.transforms[::-1]:\n",
    "            x = t.inverse_transform(x)\n",
    "        return x\n",
    "\n",
    "class Standardize(nn.Module):\n",
    "    def __init__(self, \n",
    "        mean:Optional[float]=None, std:Optional[float]=None, \n",
    "        normalization_path:Optional[str]=None,\n",
    "\n",
    "        base_path: Optional[str] = None,\n",
    "        dataset_name: Optional[str] = None,\n",
    "        split_name: Optional[str] = None,\n",
    "        *args, **kwargs, \n",
    "        ):\n",
    "        super(Standardize, self).__init__()\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        ## build_transform\n",
    "        if mean is not None and std is not None: \n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "        elif normalization_path is not None:\n",
    "            self.load_stats(normalization_path)\n",
    "        elif dataset_name is not None: \n",
    "            path = os.path.join(base_path, split_name, dataset_name)\n",
    "            self.calc_stats_from_dataset(path)\n",
    "        else:\n",
    "            assert False, \"Error: must provide either (mean, std), normalization_path, or (base_path, dataset_name, split_name) to calculate mean and std.\"\n",
    "    \n",
    "    def load_stats(self, path=None):\n",
    "        assert os.path.exists(path), f\"Error: normalization path {path} does not exist.\"\n",
    "        with open(path, \"r\") as f:\n",
    "            stats = yaml.safe_load(f)['statistics']\n",
    "\n",
    "            self.mean = stats['mean']\n",
    "            self.std = stats['std']\n",
    "        return self.mean, self.std\n",
    "    \n",
    "    def calc_stats_from_dataset(self, path=None):\n",
    "        dataset = HIT2dDataset(path=path, load_data=True, *self.args, **self.kwargs).load_data()\n",
    "        self.mean = dataset.data.mean().item()\n",
    "        self.std = dataset.data.std().item()\n",
    "        return self.mean, self.std\n",
    "\n",
    "    def forward(self, x):\n",
    "        data, target = x\n",
    "        data = self.normalize(data, params=None)[0]\n",
    "        target = self.normalize(target, params=None)[0]\n",
    "        return [data, target]\n",
    "    \n",
    "    def normalize(self, inpt: Any, params: Dict[str, Any]):\n",
    "        return (inpt - self.mean) / self.std\n",
    "\n",
    "    def denormalize(self, inpt: Any, params: Dict[str, Any]):\n",
    "        return inpt * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuraloperator import FNO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_optimizer \n",
    "from utils.refer.soap import SOAP\n",
    "\n",
    "SCHEDULERS = {\n",
    "    'StepLR': lambda optimizer, **params: torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, **(params | {'step_size': 10, 'gamma': 0.1})\n",
    "    ),\n",
    "    'ExponentialLR': lambda optimizer, **params: torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optimizer, **(params | {'gamma': 0.95})\n",
    "    ),\n",
    "    'MultiStepLR': lambda optimizer, **params: torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, **(params | {'milestones': [30, 80], 'gamma': 0.1})\n",
    "    ),\n",
    "    'CosineAnnealingLR': lambda optimizer, **params: torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, **(params | {'T_max': 10})\n",
    "    ),\n",
    "    'CosineAnnealingWarmRestarts': lambda optimizer, **params: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, **(params | {'T_0': 10, 'T_mult': 2})\n",
    "    ),\n",
    "    'OneCycleLR': lambda optimizer, **params: torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, **(params | {'max_lr': 0.1, 'total_steps': 100})\n",
    "    ),\n",
    "    'ReduceLROnPlateau': lambda optimizer, **params: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, **(params | {'mode': 'min', 'factor': 0.1, 'patience': 10})\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    'SGD': lambda model, **params: torch.optim.SGD(\n",
    "        model, **({'lr': 0.01, 'momentum': 0.9} | params)\n",
    "    ),\n",
    "    'Adam': lambda model, **params: torch.optim.Adam(\n",
    "        model, **({'lr': 1e-3, 'betas': (0.9, 0.999)} | params)\n",
    "    ),\n",
    "    'AdamW': lambda model, **params: torch.optim.AdamW(\n",
    "        model, **({'lr': 1e-3, 'weight_decay': 1e-2} | params)\n",
    "    ),\n",
    "    'RMSprop': lambda model, **params: torch.optim.RMSprop(\n",
    "        model, **({'lr': 1e-2, 'alpha': 0.99, 'momentum': 0.9} | params)\n",
    "    ),\n",
    "    'Adagrad': lambda model, **params: torch.optim.Adagrad(\n",
    "        model, **({'lr': 1e-2} | params)\n",
    "    ),\n",
    "    'Adadelta': lambda model, **params: torch.optim.Adadelta(\n",
    "        model, **({'lr': 1.0, 'rho': 0.9} | params)\n",
    "    ),\n",
    "    'Adamax': lambda model, **params: torch.optim.Adamax(\n",
    "        model, **({'lr': 2e-3} | params)\n",
    "    ),\n",
    "    'NAdam': lambda model, **params: torch.optim.NAdam(\n",
    "        model, **({'lr': 2e-3, 'betas': (0.9, 0.999)} | params)\n",
    "    ),\n",
    "    'Shampoo': (\n",
    "        (lambda model, **params: torch_optimizer.Shampoo(\n",
    "            model, **({'lr': 1e-3, 'momentum': 0.9, 'weight_decay': 0.0} | params)\n",
    "        )) \n",
    "    ),\n",
    "    'SOAP': (\n",
    "        (lambda model, **params: SOAP(\n",
    "            model, **({'lr': 1e-3,} | params)\n",
    "        )) \n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomModule(nn.Module):\n",
    "    def __init__(self, \n",
    "                 model:Optional[callable]= None,\n",
    "                 modelconfig:Optional[dict]=None, \n",
    "                 optconfig:Optional[dict]=None, \n",
    "                 \n",
    "                 loss_fn: Optional[callable] = nn.MSELoss(),\n",
    "                 transformer:Optional[callable]=None, \n",
    "                 **kwargs, \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.modelconfig = modelconfig\n",
    "        self.optconfig = optconfig\n",
    "        self.hparams = kwargs\n",
    "        self.device = device\n",
    "\n",
    "        self.model = self.build_model() if model is None else model.to(device)\n",
    "\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        if optconfig is not None: self.configure_optimizers()\n",
    "\n",
    "        self.loss_fn = None \n",
    "        if optconfig is not None: \n",
    "            self.loss_fn = loss_fn\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = FNO(**self.modelconfig['params'],).to(self.device)\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer is not None: \n",
    "            return self.optimizer, self.scheduler\n",
    "        self.optimizer = OPTIMIZERS[self.optconfig['name']](\n",
    "            self.model.parameters(), \n",
    "            **self.optconfig['params']\n",
    "        )\n",
    "\n",
    "        self.scheduler = None\n",
    "        if self.optconfig['scheduler']: \n",
    "            self.scheduler = SCHEDULERS[self.optconfig['scheduler']['name']](\n",
    "                self.optimizer, \n",
    "                **self.optconfig['scheduler'].get('params', {}),\n",
    "                ) \n",
    "        return self.optimizer, self.scheduler\n",
    "    \n",
    "    def forward(self, data, output_shape=None):\n",
    "        return self.model(data.to(self.device), output_shape=output_shape)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, ret_log:bool=False):\n",
    "        if self.transformer: batch = self.transformer(batch)\n",
    "        data, target = batch\n",
    "        \n",
    "        # data = data[:, 0]\n",
    "        # target = target[:, 0]\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.forward(data.to(self.device), output_shape=target.shape[2:])\n",
    "        loss = self.loss_fn(output, target.to(self.device))\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.log('loss', loss.item(), on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx, ret_log:bool=True):\n",
    "        if self.transformer: batch = self.transformer(batch)\n",
    "        data, target = batch\n",
    "\n",
    "        # data = data[:, 0]\n",
    "        # target = target[:, 0]\n",
    "\n",
    "        output = self.forward(data.to(self.device), output_shape=target.shape[2:])\n",
    "        loss = self.loss_fn(output, target.to(self.device))\n",
    "\n",
    "        self.log('val_loss', loss.item(), on_step=False)\n",
    "        if ret_log: \n",
    "            log = {}\n",
    "            log.update({\"loss\": loss.item(), \"output\": output[:].detach().cpu(), 'batch': [data, target.detach().cpu()]})\n",
    "            return log\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_step(self, batch, batch_idx, ret_log:bool=True):\n",
    "        if self.transformer: batch = self.transformer(batch)\n",
    "        \n",
    "        data, target = batch\n",
    "\n",
    "        # data = data[:, 0]\n",
    "        # target = target[:, 0]\n",
    "\n",
    "        output = self.forward(data.to(self.device), output_shape=target.shape[2:])\n",
    "\n",
    "        if ret_log: \n",
    "            log = {}\n",
    "            log.update({# \"loss\": loss.item(), \n",
    "                        \"output\": output[:].detach().cpu(), 'batch': [data.detach().cpu(), target.detach().cpu()]})\n",
    "            return log\n",
    "        return output\n",
    "\n",
    "    def save(self, path, checkpoint:dict={}):\n",
    "        checkpoint['model_state_dict'] = self.model.state_dict()\n",
    "        checkpoint['optimizer_state_dict'] = self.optimizer.state_dict()\n",
    "        if self.scheduler: checkpoint[f'scheduler'] = self.scheduler.state_dict()\n",
    "\n",
    "        torch.save(checkpoint, path + '.pth')\n",
    "\n",
    "    def load(self,\n",
    "        path, \n",
    "        verbose=True, \n",
    "        load_opt:bool=True,\n",
    "        freeze:bool=False,\n",
    "        ):\n",
    "        if os.path.exists(path + '.pth') and os.path.getsize(path + '.pth'):\n",
    "            checkpoint = torch.load(path + '.pth', weights_only=False)\n",
    "\n",
    "            self.model.load_state_dict(\n",
    "                checkpoint['model_state_dict'], \n",
    "                strict=False,\n",
    "                )\n",
    "            if freeze: \n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if name in checkpoint['model_state_dict']:\n",
    "                        param.requires_grad = False\n",
    "            if verbose: print(f'model loaded from {path}')\n",
    "            if load_opt:\n",
    "                if self.optimizer and 'optimizer_state_dict' in checkpoint: \n",
    "                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                    if verbose: print(f'optimizer loaded from {path}')\n",
    "                if self.scheduler and 'scheduler' in checkpoint: \n",
    "                    self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "                    if verbose: print(f'scheduler loaded from {path}')\n",
    "    \n",
    "    def log(self, name, value, prog_bar=False, logger=None, on_step=None, on_epoch=None, reduce_fx='mean', enable_graph=False, sync_dist=False, sync_dist_group=None, add_dataloader_idx=True, batch_size=None, metric_attribute=None, rank_zero_only=False):\n",
    "        commit = False if on_step is False else True\n",
    "        wandb.log({name: value}, commit=commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.BaseCallback import BaseCallback\n",
    "from utils.CFDFunction import calc_energy_spectrum, calc_pdf, calc_phase_error\n",
    "from utils.Plots import plot_2d_surface, plot_spectrum, plot_pdf\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, \n",
    "                 criterions, \n",
    "                 file_dir:str='', \n",
    "                 fname:str ='',\n",
    "                 device:str =None, \n",
    "                 val_every_n_iter:int=None,\n",
    "                 ):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.criterions = criterions # DL criterion + CFD criterion + DL part-of-loss\n",
    "        self.file_dir = file_dir\n",
    "        self.fname = fname\n",
    "        self.device=device\n",
    "        \n",
    "    def on_validation_epoch_start(self, trainer, pl_module):\n",
    "        self.time_stamp = time.time()\n",
    "        trainer._current_val_return = {'loss': [], 'time': []}\n",
    "        for key in self.criterions.keys():\n",
    "            trainer._current_val_return[key] = []\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n",
    "        data, target = outputs['batch'] # batch\n",
    "        # trainer._current_val_return['batch'] = outputs['batch'] # batch\n",
    "        # trainer._current_val_return['output'] = outputs['output']\n",
    "        # trainer._current_val_return['loss'].append(outputs['loss'])\n",
    "\n",
    "        for key in self.criterions.keys():\n",
    "            value = self.criterions[key](outputs['output'].to(self.device), target.to(self.device)).item()\n",
    "\n",
    "            trainer._current_val_return[key].append(value)\n",
    "            \n",
    "        self.N = target.shape[-1]\n",
    "    \n",
    "    def on_validation_end(self, trainer, pl_module, fname='') -> None:\n",
    "        fname = self.fname + fname\n",
    "        # data, target = trainer._current_val_return['batch']\n",
    "        # output = trainer._current_val_return['output']\n",
    "        # if trainer.logger: trainer.logger.log({'val_loss': np.mean(trainer._current_val_return['loss'])})\n",
    "\n",
    "        epoch = trainer.current_epoch\n",
    "        print(f'  Validation Epoch: {epoch}', end='')\n",
    "        for key in self.criterions.keys():\n",
    "            value = np.mean(trainer._current_val_return[key])\n",
    "            print(f\", {key}: {value:.6f}\", end='')\n",
    "\n",
    "            if trainer.logger: trainer.logger.log({f'Val_{self.N}/epoch'+key: value})\n",
    "        print()\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "    def on_test_batch_start(self, trainer, pl_module, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n",
    "        self.time_stamp = time.time()\n",
    "\n",
    "    def on_test_batch_end(self, trainer, pl_module, outputs, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n",
    "        data, target = batch\n",
    "        trainer._current_test_return['batch'] = batch\n",
    "        output = trainer._current_test_return['output'] = outputs['output']\n",
    "        \n",
    "        runtime = time.time() - self.time_stamp\n",
    "        if trainer.logger: trainer.logger.log({'Test/time': runtime})\n",
    "        print(f'Test sample {batch_idx}(time: {runtime:.6f}, batch size: {len(output)})', end='')\n",
    "        for key in self.criterions.keys():\n",
    "            value = self.criterions[key](outputs['output'].to(self.device), target.to(self.device)).item()\n",
    "\n",
    "            trainer._current_test_return[key].append(value)\n",
    "            print(f\", {key}: {value:.6f}\", end='')\n",
    "        print()\n",
    "        \n",
    "        ## enstrophy spectrum\n",
    "        spectrum1 = calc_energy_spectrum(target[:,0,:], dim=2) # np.mean([calc_enstrophy_spectrum(target[i,0,:]) for i in range(len(target))], axis=0); \n",
    "        spectrum2 = calc_energy_spectrum(output[:,0,:], dim=2) # np.mean([calc_enstrophy_spectrum(output[i,0,:]) for i in range(len(output))], axis=0)\n",
    "        ## vorticity pdf\n",
    "        bin1, pdf1 = calc_pdf(target[:,0,:], dim=2) # np.mean([calc_pdf(target[i,0,:]) for i in range(len(target))], axis=0); \n",
    "        bin2, pdf2 = calc_pdf(output[:,0,:], dim=2) # np.mean([calc_pdf(output[i,0,:]) for i in range(len(output))], axis=0)\n",
    "\n",
    "        # ### solution field \n",
    "        plot_2d_surface(target[0,0,...,0], output[0,0,...,0], # axes=[ax1,ax2], \n",
    "                        kwargs={'figsize': (8, 4)}\n",
    "                        )\n",
    "        plt.savefig(self.file_dir + self.fname + f'_Test_sample={batch_idx}_field.png')\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        ax = fig.add_subplot(1, 2, 1)\n",
    "        plot_spectrum(spectrum1, spectrum2, ax=ax, kwargs={'title': f'enstrophy spectrum'})\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, 2)\n",
    "        plot_pdf(bin1, pdf1, bin2, pdf2, ax=ax, kwargs={'title': f'pdf'})\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.file_dir + self.fname + f'_Test_sample={batch_idx}_stat.png')\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        ## save .wandb\n",
    "        Nt = len(output)\n",
    "        for it in range(Nt):\n",
    "            image = np.concatenate((target[it,0,:], output[it,0,:]), axis=1)\n",
    "            if trainer.logger: trainer.logger.log({'Test/'+f'sample_{batch_idx}/'+'field': wandb.Image(image)})\n",
    "        if trainer.logger: trainer.logger.log({'Test/spectrum': wandb.plot.line_series(\n",
    "            xs=[np.arange(0, spectrum1.shape[-1]+1), np.arange(0, spectrum2.shape[-1]+1)], \n",
    "            ys = [spectrum1, spectrum2],\n",
    "            keys = ['ground truth', 'prediction'],\n",
    "            xname = ['k', 'enstrophy spectrum'],\n",
    "            title='enstrophy_spectrum', \n",
    "            )})\n",
    "        if trainer.logger: trainer.logger.log({'Test/pdf': wandb.plot.line_series(\n",
    "            xs=[bin1, bin2], \n",
    "            ys = [pdf1, pdf2],\n",
    "            keys = ['ground truth', 'prediction'],\n",
    "            xname = ['w', 'pdf'],\n",
    "            title='pdf', \n",
    "            )})\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module) -> None:\n",
    "        data, target = trainer._current_test_return['batch']\n",
    "        output = trainer._current_test_return['output']\n",
    "\n",
    "        print('Test ')\n",
    "        for key in self.criterions.keys():\n",
    "            value = np.mean(trainer._current_test_return[key])\n",
    "            print(f\", {key}: {value:.6f}\", end='')\n",
    "\n",
    "            if trainer.logger: trainer.logger.log({'Test/'+key: value})\n",
    "        print()\n",
    "\n",
    "    def preprocessing(self, x):\n",
    "        n_modes = self.n_modes\n",
    "        x = self.split(x, modes=n_modes)\n",
    "        # x = self.scaling(x, dim=2, isNormalize=False)\n",
    "        return x \n",
    "    \n",
    "    def postprocessing(self, x):\n",
    "        n_modes = self.n_modes\n",
    "        # x = self.unscaling(x)\n",
    "        x = self.unsplit(x, modes=n_modes)\n",
    "        return x\n",
    "    \n",
    "    def scaling(self, x, \n",
    "                dim=1, \n",
    "                isNormalize:bool=False):\n",
    "        dim = list(range(-dim, 0))\n",
    "        if isNormalize:\n",
    "            self.mean = x.amin(dim=dim, keepdim=True)\n",
    "            self.std = x.amax(dim=dim, keepdim=True) - self.mean\n",
    "        else: \n",
    "            self.mean = x.mean(dim=dim, keepdim=True)\n",
    "            self.std = x.std(dim=dim, keepdim=True)\n",
    "        return (x - self.mean) / self.std\n",
    "    \n",
    "    def unscaling(self, x):\n",
    "        return x * self.std + self.mean\n",
    "    \n",
    "    def split(self, x, \n",
    "              modes=None\n",
    "              ):\n",
    "\n",
    "        b, c, *data_shape = x.shape\n",
    "        fft_dims = list(range(-len(data_shape), 0))\n",
    "        Fx = torch.fft.fft2(x, dim=fft_dims, norm='forward')\n",
    "        Fx_ = torch.zeros((len(modes), b, c, *data_shape), dtype=Fx.dtype, device=x.device)\n",
    "        \n",
    "        k = [torch.fft.fftfreq(n, d=1./n) for n in data_shape]\n",
    "        # k += [torch.fft.rfftfreq(data_shape[-1], d=1./data_shape[-1])] if Fx.dtype in [torch.float16, torch.float32, torch.float64] else [torch.fft.fftfreq(data_shape[-1], d=1./data_shape[-1])]\n",
    "\n",
    "\n",
    "        k = torch.meshgrid(k, indexing='ij')\n",
    "        k = torch.stack(k)\n",
    "        k = torch.sqrt(torch.sum(k**2, axis=0)).to(x.device)\n",
    "        for i in range(len(modes)):\n",
    "            k1, k2 = modes[i]\n",
    "            idx = (k1 <= k) & (k <= k2)\n",
    "            Fx_[i] = Fx * idx\n",
    "\n",
    "        x_ = torch.fft.ifft2(Fx_, dim=fft_dims, norm='forward').real # x_ = Fx_ # # \n",
    "        \n",
    "        return x_\n",
    "\n",
    "    def unsplit(self, x_, \n",
    "              modes=None\n",
    "              ):\n",
    "        n, b, c, *data_shape = x_.shape\n",
    "        fft_dims = list(range(-len(data_shape), 0))\n",
    "        \n",
    "        Fx_ = torch.fft.fft2(x_, dim=fft_dims, norm='forward') # Fx_ = x_ # \n",
    "        \n",
    "        Fx = torch.zeros((b, c, *data_shape), dtype=Fx_.dtype, device=x_.device)\n",
    "        k = [torch.fft.fftfreq(n, d=1./n) for n in data_shape[:-1]]\n",
    "        k += [torch.fft.rfftfreq(data_shape[-1], d=1./data_shape[-1])] if Fx.dtype in [torch.float16, torch.float32, torch.float64] else [torch.fft.fftfreq(data_shape[-1], d=1./data_shape[-1])]\n",
    "        k = torch.meshgrid(k, indexing='ij')\n",
    "        k = torch.stack(k)\n",
    "        k = torch.sqrt(torch.sum(k**2, axis=0)).to(x_.device)\n",
    "        for i in range(len(modes)):\n",
    "            k1, k2 = modes[i]\n",
    "            idx = (k1 <= k) & (k <= k2)\n",
    "            Fx[..., idx] = Fx_[i, ..., idx]\n",
    "\n",
    "        x = torch.fft.ifft2(Fx, dim=fft_dims, norm='forward').real\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset metadata: dict_keys(['nu', 'Lx', 'Ly', 'dx', 'dy', 'Nx', 'Ny', 'x', 'y', 'dt', 'Nt', 'T', 't0', 't'])\n",
      "{'nu': 0.000225, 'Lx': 1.5402125848364265, 'Ly': 1.5402125848364265, 'dx': 0.024447818806927406, 'dy': 0.024447818806927406, 'Nx': 64, 'Ny': 64, 'x': array([0.        , 0.02444782, 0.04889564, 0.07334346, 0.09779128,\n",
      "       0.12223909, 0.14668691, 0.17113473, 0.19558255, 0.22003037,\n",
      "       0.24447819, 0.26892601, 0.29337383, 0.31782164, 0.34226946,\n",
      "       0.36671728, 0.3911651 , 0.41561292, 0.44006074, 0.46450856,\n",
      "       0.48895638, 0.51340419, 0.53785201, 0.56229983, 0.58674765,\n",
      "       0.61119547, 0.63564329, 0.66009111, 0.68453893, 0.70898675,\n",
      "       0.73343456, 0.75788238, 0.7823302 , 0.80677802, 0.83122584,\n",
      "       0.85567366, 0.88012148, 0.9045693 , 0.92901711, 0.95346493,\n",
      "       0.97791275, 1.00236057, 1.02680839, 1.05125621, 1.07570403,\n",
      "       1.10015185, 1.12459967, 1.14904748, 1.1734953 , 1.19794312,\n",
      "       1.22239094, 1.24683876, 1.27128658, 1.2957344 , 1.32018222,\n",
      "       1.34463003, 1.36907785, 1.39352567, 1.41797349, 1.44242131,\n",
      "       1.46686913, 1.49131695, 1.51576477, 1.54021258]), 'y': array([0.        , 0.02444782, 0.04889564, 0.07334346, 0.09779128,\n",
      "       0.12223909, 0.14668691, 0.17113473, 0.19558255, 0.22003037,\n",
      "       0.24447819, 0.26892601, 0.29337383, 0.31782164, 0.34226946,\n",
      "       0.36671728, 0.3911651 , 0.41561292, 0.44006074, 0.46450856,\n",
      "       0.48895638, 0.51340419, 0.53785201, 0.56229983, 0.58674765,\n",
      "       0.61119547, 0.63564329, 0.66009111, 0.68453893, 0.70898675,\n",
      "       0.73343456, 0.75788238, 0.7823302 , 0.80677802, 0.83122584,\n",
      "       0.85567366, 0.88012148, 0.9045693 , 0.92901711, 0.95346493,\n",
      "       0.97791275, 1.00236057, 1.02680839, 1.05125621, 1.07570403,\n",
      "       1.10015185, 1.12459967, 1.14904748, 1.1734953 , 1.19794312,\n",
      "       1.22239094, 1.24683876, 1.27128658, 1.2957344 , 1.32018222,\n",
      "       1.34463003, 1.36907785, 1.39352567, 1.41797349, 1.44242131,\n",
      "       1.46686913, 1.49131695, 1.51576477, 1.54021258]), 'dt': 0.009999999999999787, 'Nt': 400, 'T': 14.489999999999915, 't0': 10.5, 't': array([10.5 , 10.51, 10.52, 10.53, 10.54, 10.55, 10.56, 10.57, 10.58,\n",
      "       10.59, 10.6 , 10.61, 10.62, 10.63, 10.64, 10.65, 10.66, 10.67,\n",
      "       10.68, 10.69, 10.7 , 10.71, 10.72, 10.73, 10.74, 10.75, 10.76,\n",
      "       10.77, 10.78, 10.79, 10.8 , 10.81, 10.82, 10.83, 10.84, 10.85,\n",
      "       10.86, 10.87, 10.88, 10.89, 10.9 , 10.91, 10.92, 10.93, 10.94,\n",
      "       10.95, 10.96, 10.97, 10.98, 10.99, 11.  , 11.01, 11.02, 11.03,\n",
      "       11.04, 11.05, 11.06, 11.07, 11.08, 11.09, 11.1 , 11.11, 11.12,\n",
      "       11.13, 11.14, 11.15, 11.16, 11.17, 11.18, 11.19, 11.2 , 11.21,\n",
      "       11.22, 11.23, 11.24, 11.25, 11.26, 11.27, 11.28, 11.29, 11.3 ,\n",
      "       11.31, 11.32, 11.33, 11.34, 11.35, 11.36, 11.37, 11.38, 11.39,\n",
      "       11.4 , 11.41, 11.42, 11.43, 11.44, 11.45, 11.46, 11.47, 11.48,\n",
      "       11.49, 11.5 , 11.51, 11.52, 11.53, 11.54, 11.55, 11.56, 11.57,\n",
      "       11.58, 11.59, 11.6 , 11.61, 11.62, 11.63, 11.64, 11.65, 11.66,\n",
      "       11.67, 11.68, 11.69, 11.7 , 11.71, 11.72, 11.73, 11.74, 11.75,\n",
      "       11.76, 11.77, 11.78, 11.79, 11.8 , 11.81, 11.82, 11.83, 11.84,\n",
      "       11.85, 11.86, 11.87, 11.88, 11.89, 11.9 , 11.91, 11.92, 11.93,\n",
      "       11.94, 11.95, 11.96, 11.97, 11.98, 11.99, 12.  , 12.01, 12.02,\n",
      "       12.03, 12.04, 12.05, 12.06, 12.07, 12.08, 12.09, 12.1 , 12.11,\n",
      "       12.12, 12.13, 12.14, 12.15, 12.16, 12.17, 12.18, 12.19, 12.2 ,\n",
      "       12.21, 12.22, 12.23, 12.24, 12.25, 12.26, 12.27, 12.28, 12.29,\n",
      "       12.3 , 12.31, 12.32, 12.33, 12.34, 12.35, 12.36, 12.37, 12.38,\n",
      "       12.39, 12.4 , 12.41, 12.42, 12.43, 12.44, 12.45, 12.46, 12.47,\n",
      "       12.48, 12.49, 12.5 , 12.51, 12.52, 12.53, 12.54, 12.55, 12.56,\n",
      "       12.57, 12.58, 12.59, 12.6 , 12.61, 12.62, 12.63, 12.64, 12.65,\n",
      "       12.66, 12.67, 12.68, 12.69, 12.7 , 12.71, 12.72, 12.73, 12.74,\n",
      "       12.75, 12.76, 12.77, 12.78, 12.79, 12.8 , 12.81, 12.82, 12.83,\n",
      "       12.84, 12.85, 12.86, 12.87, 12.88, 12.89, 12.9 , 12.91, 12.92,\n",
      "       12.93, 12.94, 12.95, 12.96, 12.97, 12.98, 12.99, 13.  , 13.01,\n",
      "       13.02, 13.03, 13.04, 13.05, 13.06, 13.07, 13.08, 13.09, 13.1 ,\n",
      "       13.11, 13.12, 13.13, 13.14, 13.15, 13.16, 13.17, 13.18, 13.19,\n",
      "       13.2 , 13.21, 13.22, 13.23, 13.24, 13.25, 13.26, 13.27, 13.28,\n",
      "       13.29, 13.3 , 13.31, 13.32, 13.33, 13.34, 13.35, 13.36, 13.37,\n",
      "       13.38, 13.39, 13.4 , 13.41, 13.42, 13.43, 13.44, 13.45, 13.46,\n",
      "       13.47, 13.48, 13.49, 13.5 , 13.51, 13.52, 13.53, 13.54, 13.55,\n",
      "       13.56, 13.57, 13.58, 13.59, 13.6 , 13.61, 13.62, 13.63, 13.64,\n",
      "       13.65, 13.66, 13.67, 13.68, 13.69, 13.7 , 13.71, 13.72, 13.73,\n",
      "       13.74, 13.75, 13.76, 13.77, 13.78, 13.79, 13.8 , 13.81, 13.82,\n",
      "       13.83, 13.84, 13.85, 13.86, 13.87, 13.88, 13.89, 13.9 , 13.91,\n",
      "       13.92, 13.93, 13.94, 13.95, 13.96, 13.97, 13.98, 13.99, 14.  ,\n",
      "       14.01, 14.02, 14.03, 14.04, 14.05, 14.06, 14.07, 14.08, 14.09,\n",
      "       14.1 , 14.11, 14.12, 14.13, 14.14, 14.15, 14.16, 14.17, 14.18,\n",
      "       14.19, 14.2 , 14.21, 14.22, 14.23, 14.24, 14.25, 14.26, 14.27,\n",
      "       14.28, 14.29, 14.3 , 14.31, 14.32, 14.33, 14.34, 14.35, 14.36,\n",
      "       14.37, 14.38, 14.39, 14.4 , 14.41, 14.42, 14.43, 14.44, 14.45,\n",
      "       14.46, 14.47, 14.48, 14.49])}\n"
     ]
    }
   ],
   "source": [
    "##### set parameters #####\n",
    "### save parameters ### \n",
    "data_dir = config['data'][0]['params']['base_path'] # '../Data/nu=0.001_n=128/'\n",
    "data_fname = config['data'][0]['params']['dataset_name'] # f'2dHIT_nu=0.001_n=128_T=11.5'\n",
    "\n",
    "metadata = HIT2dDataset(path=data_dir + 'train/' + data_fname, load_data=False).metadata\n",
    "print(metadata)\n",
    "\n",
    "nu = metadata['nu']\n",
    "dt = metadata['dt']\n",
    "N = metadata['Nx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset metadata: dict_keys(['nu', 'Lx', 'Ly', 'dx', 'dy', 'Nx', 'Ny', 'x', 'y', 'dt', 'Nt', 'T', 't0', 't'])\n",
      "Train dataset: 182500\n",
      "Loaded dataset metadata: dict_keys(['nu', 'Lx', 'Ly', 'dx', 'dy', 'Nx', 'Ny', 'x', 'y', 'dt', 'Nt', 'T', 't0', 't'])\n",
      "Val dataset: 18250\n",
      "Loaded dataset metadata: dict_keys(['nu', 'Lx', 'Ly', 'dx', 'dy', 'Nx', 'Ny', 'x', 'y', 'dt', 'Nt', 'T', 't0', 't'])\n",
      "Val dataset: 365\n"
     ]
    }
   ],
   "source": [
    "from utils.Losses import (RMSLoss, TKELoss, DissipationLoss, RelambdaLoss, R2, \n",
    "                          BSMSE, )\n",
    "from utils.utilities import HsLoss\n",
    "\n",
    "\n",
    "test_criterion={\n",
    "    'l2': nn.MSELoss(), \n",
    "    'h1': H1Loss(d=2, reduction='mean'), \n",
    "    'h2': HsLoss(k=2, reduction='mean'), # HsLoss(k=2, group=False, size_average=True), \n",
    "    \n",
    "    'fRMS_k<8': BSMSE(kmax=8, dim=(-2, -1), mode='spectral', isRelative=True),\n",
    "    'fRMS_8<k<16': BSMSE(kmin=8, kmax=16, dim=(-2, -1), mode='spectral', isRelative=True),\n",
    "    'fRMS_k>16': BSMSE(kmin=16, dim=(-2, -1), mode='spectral', isRelative=True),\n",
    "    'fRMS_k<kmax': BSMSE(kmax=16, dim=(-2, -1), mode='spectral', isRelative=True),\n",
    "    'fRMS_k>kmax': BSMSE(kmin=16, dim=(-2, -1), mode='spectral', isRelative=True),\n",
    "    'fRMS_k>train': BSMSE(kmin=32, dim=(-2, -1), mode='spectral', isRelative=True),\n",
    "    'fRMS_k<train': BSMSE(kmax=32, dim=(-2, -1), mode='spectral', isRelative=True),\n",
    "\n",
    "    'vor_rms': RMSLoss(dim=2, isRelative=True),\n",
    "    # 'tke': TKELoss(dim=2, isRelative=True), \n",
    "    # 'dissipation': DissipationLoss(nu=nu, dim=2, isRelative=True), \n",
    "    # 'R_lambda': RelambdaLoss(nu=nu, dim=2, isRelative=True), \n",
    "    'R_squared': R2(), \n",
    "    }\n",
    "\n",
    "##### setup dataset #####\n",
    "dataloaders = {'train':[], 'valid': [], 'test': []}\n",
    "for i, data_config in enumerate(config['data']):\n",
    "    transform = CustomTransform()\n",
    "    normalization = Standardize(normalization_path=data_config['normalization']['normalization_path'])\n",
    "\n",
    "    idx_leadtime = data_config['idx_leadtime'] if 'idx_leadtime' in data_config else int(data_config['leadtime'] * integral_timescales[nu] / dt) \n",
    "    dm = CustomDataModule(**data_config['params'], \n",
    "        n_stride=idx_leadtime, \n",
    "        # Ndata_train=500, # 500, \n",
    "        # Ndata_val=50, \n",
    "        # Ndata_test=50,\n",
    "        # transform=transform,\n",
    "        normalization=normalization,\n",
    "        load_data=False,\n",
    "        )\n",
    "    stage = data_config['stage']\n",
    "    dm.setup(stage=stage)\n",
    "    if stage in ['fit', 'valid']:\n",
    "        dataloaders['valid'].append(dm.val_dataloader())\n",
    "    if stage in ['fit', 'train']:\n",
    "        dataloaders['train'].append(dm.train_dataloader())\n",
    "    if stage in ['test']:\n",
    "        dataloaders['test'].append(dm.test_dataloader())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "============================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
      "============================================================================================================================================\n",
      "FNO                                      [64, 1, 64, 64]           [64, 1, 64, 64]           --                        True\n",
      "├─GridEmbeddingND: 1-1                   [64, 1, 64, 64]           [64, 3, 64, 64]           --                        --\n",
      "├─ChannelMLP: 1-2                        [64, 3, 64, 64]           [64, 32, 64, 64]          --                        True\n",
      "│    └─ModuleList: 2-1                   --                        --                        --                        True\n",
      "│    │    └─Conv1d: 3-1                  [64, 3, 4096]             [64, 128, 4096]           512                       True\n",
      "│    │    └─Conv1d: 3-2                  [64, 128, 4096]           [64, 32, 4096]            4,128                     True\n",
      "├─FNOBlocks: 1-3                         [64, 32, 64, 64]          [64, 32, 64, 64]          1,677,648                 True\n",
      "│    └─ModuleList: 2-20                  --                        --                        (recursive)               True\n",
      "│    │    └─Flattened1dConv: 3-3         [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
      "│    │    │    └─Conv1d: 4-1             [64, 32, 4096]            [64, 32, 4096]            1,024                     True\n",
      "│    └─ModuleList: 2-21                  --                        --                        (recursive)               True\n",
      "│    │    └─SoftGating: 3-4              [64, 32, 64, 64]          [64, 32, 64, 64]          32                        True\n",
      "│    └─ModuleList: 2-22                  --                        --                        (recursive)               True\n",
      "│    │    └─SpectralConv: 3-5            [64, 32, 64, 64]          [64, 32, 64, 64]          557,088                   True\n",
      "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
      "│    │    └─InstanceNorm: 3-6            [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
      "│    └─ModuleList: 2-24                  --                        --                        (recursive)               True\n",
      "│    │    └─ChannelMLP: 3-7              [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
      "│    │    │    └─ModuleList: 4-2         --                        --                        --                        True\n",
      "│    │    │    │    └─Conv1d: 5-1        [64, 32, 4096]            [64, 16, 4096]            528                       True\n",
      "│    │    │    │    └─Conv1d: 5-2        [64, 16, 4096]            [64, 32, 4096]            544                       True\n",
      "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
      "│    │    └─InstanceNorm: 3-8            [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
      "├─FNOBlocks: 1-4                         [64, 32, 64, 64]          [64, 32, 64, 64]          (recursive)               True\n",
      "│    └─ModuleList: 2-20                  --                        --                        (recursive)               True\n",
      "│    │    └─Flattened1dConv: 3-9         [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
      "│    │    │    └─Conv1d: 4-3             [64, 32, 4096]            [64, 32, 4096]            1,024                     True\n",
      "│    └─ModuleList: 2-21                  --                        --                        (recursive)               True\n",
      "│    │    └─SoftGating: 3-10             [64, 32, 64, 64]          [64, 32, 64, 64]          32                        True\n",
      "│    └─ModuleList: 2-22                  --                        --                        (recursive)               True\n",
      "│    │    └─SpectralConv: 3-11           [64, 32, 64, 64]          [64, 32, 64, 64]          557,088                   True\n",
      "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
      "│    │    └─InstanceNorm: 3-12           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
      "│    └─ModuleList: 2-24                  --                        --                        (recursive)               True\n",
      "│    │    └─ChannelMLP: 3-13             [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
      "│    │    │    └─ModuleList: 4-4         --                        --                        --                        True\n",
      "│    │    │    │    └─Conv1d: 5-3        [64, 32, 4096]            [64, 16, 4096]            528                       True\n",
      "│    │    │    │    └─Conv1d: 5-4        [64, 16, 4096]            [64, 32, 4096]            544                       True\n",
      "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
      "│    │    └─InstanceNorm: 3-14           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
      "├─FNOBlocks: 1-5                         [64, 32, 64, 64]          [64, 32, 64, 64]          (recursive)               True\n",
      "│    └─ModuleList: 2-20                  --                        --                        (recursive)               True\n",
      "│    │    └─Flattened1dConv: 3-15        [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
      "│    │    │    └─Conv1d: 4-5             [64, 32, 4096]            [64, 32, 4096]            1,024                     True\n",
      "│    └─ModuleList: 2-21                  --                        --                        (recursive)               True\n",
      "│    │    └─SoftGating: 3-16             [64, 32, 64, 64]          [64, 32, 64, 64]          32                        True\n",
      "│    └─ModuleList: 2-22                  --                        --                        (recursive)               True\n",
      "│    │    └─SpectralConv: 3-17           [64, 32, 64, 64]          [64, 32, 64, 64]          557,088                   True\n",
      "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
      "│    │    └─InstanceNorm: 3-18           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
      "│    └─ModuleList: 2-24                  --                        --                        (recursive)               True\n",
      "│    │    └─ChannelMLP: 3-19             [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
      "│    │    │    └─ModuleList: 4-6         --                        --                        --                        True\n",
      "│    │    │    │    └─Conv1d: 5-5        [64, 32, 4096]            [64, 16, 4096]            528                       True\n",
      "│    │    │    │    └─Conv1d: 5-6        [64, 16, 4096]            [64, 32, 4096]            544                       True\n",
      "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
      "│    │    └─InstanceNorm: 3-20           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
      "├─FNOBlocks: 1-6                         [64, 32, 64, 64]          [64, 32, 64, 64]          (recursive)               True\n",
      "│    └─ModuleList: 2-20                  --                        --                        (recursive)               True\n",
      "│    │    └─Flattened1dConv: 3-21        [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
      "│    │    │    └─Conv1d: 4-7             [64, 32, 4096]            [64, 32, 4096]            1,024                     True\n",
      "│    └─ModuleList: 2-21                  --                        --                        (recursive)               True\n",
      "│    │    └─SoftGating: 3-22             [64, 32, 64, 64]          [64, 32, 64, 64]          32                        True\n",
      "│    └─ModuleList: 2-22                  --                        --                        (recursive)               True\n",
      "│    │    └─SpectralConv: 3-23           [64, 32, 64, 64]          [64, 32, 64, 64]          557,088                   True\n",
      "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
      "│    │    └─InstanceNorm: 3-24           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
      "│    └─ModuleList: 2-24                  --                        --                        (recursive)               True\n",
      "│    │    └─ChannelMLP: 3-25             [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
      "│    │    │    └─ModuleList: 4-8         --                        --                        --                        True\n",
      "│    │    │    │    └─Conv1d: 5-7        [64, 32, 4096]            [64, 16, 4096]            528                       True\n",
      "│    │    │    │    └─Conv1d: 5-8        [64, 16, 4096]            [64, 32, 4096]            544                       True\n",
      "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
      "│    │    └─InstanceNorm: 3-26           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
      "├─ChannelMLP: 1-7                        [64, 32, 64, 64]          [64, 1, 64, 64]           --                        True\n",
      "│    └─ModuleList: 2-26                  --                        --                        --                        True\n",
      "│    │    └─Conv1d: 3-27                 [64, 32, 4096]            [64, 128, 4096]           4,224                     True\n",
      "│    │    └─Conv1d: 3-28                 [64, 128, 4096]           [64, 1, 4096]             129                       True\n",
      "============================================================================================================================================\n",
      "Total params: 3,923,505\n",
      "Trainable params: 3,923,505\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 4.56\n",
      "============================================================================================================================================\n",
      "Input size (MB): 1.05\n",
      "Forward/backward pass size (MB): 1545.60\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 1546.72\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
       "============================================================================================================================================\n",
       "FNO                                      [64, 1, 64, 64]           [64, 1, 64, 64]           --                        True\n",
       "├─GridEmbeddingND: 1-1                   [64, 1, 64, 64]           [64, 3, 64, 64]           --                        --\n",
       "├─ChannelMLP: 1-2                        [64, 3, 64, 64]           [64, 32, 64, 64]          --                        True\n",
       "│    └─ModuleList: 2-1                   --                        --                        --                        True\n",
       "│    │    └─Conv1d: 3-1                  [64, 3, 4096]             [64, 128, 4096]           512                       True\n",
       "│    │    └─Conv1d: 3-2                  [64, 128, 4096]           [64, 32, 4096]            4,128                     True\n",
       "├─FNOBlocks: 1-3                         [64, 32, 64, 64]          [64, 32, 64, 64]          1,677,648                 True\n",
       "│    └─ModuleList: 2-20                  --                        --                        (recursive)               True\n",
       "│    │    └─Flattened1dConv: 3-3         [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
       "│    │    │    └─Conv1d: 4-1             [64, 32, 4096]            [64, 32, 4096]            1,024                     True\n",
       "│    └─ModuleList: 2-21                  --                        --                        (recursive)               True\n",
       "│    │    └─SoftGating: 3-4              [64, 32, 64, 64]          [64, 32, 64, 64]          32                        True\n",
       "│    └─ModuleList: 2-22                  --                        --                        (recursive)               True\n",
       "│    │    └─SpectralConv: 3-5            [64, 32, 64, 64]          [64, 32, 64, 64]          557,088                   True\n",
       "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
       "│    │    └─InstanceNorm: 3-6            [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
       "│    └─ModuleList: 2-24                  --                        --                        (recursive)               True\n",
       "│    │    └─ChannelMLP: 3-7              [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
       "│    │    │    └─ModuleList: 4-2         --                        --                        --                        True\n",
       "│    │    │    │    └─Conv1d: 5-1        [64, 32, 4096]            [64, 16, 4096]            528                       True\n",
       "│    │    │    │    └─Conv1d: 5-2        [64, 16, 4096]            [64, 32, 4096]            544                       True\n",
       "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
       "│    │    └─InstanceNorm: 3-8            [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
       "├─FNOBlocks: 1-4                         [64, 32, 64, 64]          [64, 32, 64, 64]          (recursive)               True\n",
       "│    └─ModuleList: 2-20                  --                        --                        (recursive)               True\n",
       "│    │    └─Flattened1dConv: 3-9         [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
       "│    │    │    └─Conv1d: 4-3             [64, 32, 4096]            [64, 32, 4096]            1,024                     True\n",
       "│    └─ModuleList: 2-21                  --                        --                        (recursive)               True\n",
       "│    │    └─SoftGating: 3-10             [64, 32, 64, 64]          [64, 32, 64, 64]          32                        True\n",
       "│    └─ModuleList: 2-22                  --                        --                        (recursive)               True\n",
       "│    │    └─SpectralConv: 3-11           [64, 32, 64, 64]          [64, 32, 64, 64]          557,088                   True\n",
       "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
       "│    │    └─InstanceNorm: 3-12           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
       "│    └─ModuleList: 2-24                  --                        --                        (recursive)               True\n",
       "│    │    └─ChannelMLP: 3-13             [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
       "│    │    │    └─ModuleList: 4-4         --                        --                        --                        True\n",
       "│    │    │    │    └─Conv1d: 5-3        [64, 32, 4096]            [64, 16, 4096]            528                       True\n",
       "│    │    │    │    └─Conv1d: 5-4        [64, 16, 4096]            [64, 32, 4096]            544                       True\n",
       "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
       "│    │    └─InstanceNorm: 3-14           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
       "├─FNOBlocks: 1-5                         [64, 32, 64, 64]          [64, 32, 64, 64]          (recursive)               True\n",
       "│    └─ModuleList: 2-20                  --                        --                        (recursive)               True\n",
       "│    │    └─Flattened1dConv: 3-15        [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
       "│    │    │    └─Conv1d: 4-5             [64, 32, 4096]            [64, 32, 4096]            1,024                     True\n",
       "│    └─ModuleList: 2-21                  --                        --                        (recursive)               True\n",
       "│    │    └─SoftGating: 3-16             [64, 32, 64, 64]          [64, 32, 64, 64]          32                        True\n",
       "│    └─ModuleList: 2-22                  --                        --                        (recursive)               True\n",
       "│    │    └─SpectralConv: 3-17           [64, 32, 64, 64]          [64, 32, 64, 64]          557,088                   True\n",
       "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
       "│    │    └─InstanceNorm: 3-18           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
       "│    └─ModuleList: 2-24                  --                        --                        (recursive)               True\n",
       "│    │    └─ChannelMLP: 3-19             [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
       "│    │    │    └─ModuleList: 4-6         --                        --                        --                        True\n",
       "│    │    │    │    └─Conv1d: 5-5        [64, 32, 4096]            [64, 16, 4096]            528                       True\n",
       "│    │    │    │    └─Conv1d: 5-6        [64, 16, 4096]            [64, 32, 4096]            544                       True\n",
       "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
       "│    │    └─InstanceNorm: 3-20           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
       "├─FNOBlocks: 1-6                         [64, 32, 64, 64]          [64, 32, 64, 64]          (recursive)               True\n",
       "│    └─ModuleList: 2-20                  --                        --                        (recursive)               True\n",
       "│    │    └─Flattened1dConv: 3-21        [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
       "│    │    │    └─Conv1d: 4-7             [64, 32, 4096]            [64, 32, 4096]            1,024                     True\n",
       "│    └─ModuleList: 2-21                  --                        --                        (recursive)               True\n",
       "│    │    └─SoftGating: 3-22             [64, 32, 64, 64]          [64, 32, 64, 64]          32                        True\n",
       "│    └─ModuleList: 2-22                  --                        --                        (recursive)               True\n",
       "│    │    └─SpectralConv: 3-23           [64, 32, 64, 64]          [64, 32, 64, 64]          557,088                   True\n",
       "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
       "│    │    └─InstanceNorm: 3-24           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
       "│    └─ModuleList: 2-24                  --                        --                        (recursive)               True\n",
       "│    │    └─ChannelMLP: 3-25             [64, 32, 64, 64]          [64, 32, 64, 64]          --                        True\n",
       "│    │    │    └─ModuleList: 4-8         --                        --                        --                        True\n",
       "│    │    │    │    └─Conv1d: 5-7        [64, 32, 4096]            [64, 16, 4096]            528                       True\n",
       "│    │    │    │    └─Conv1d: 5-8        [64, 16, 4096]            [64, 32, 4096]            544                       True\n",
       "│    └─ModuleList: 2-25                  --                        --                        --                        --\n",
       "│    │    └─InstanceNorm: 3-26           [64, 32, 64, 64]          [64, 32, 64, 64]          --                        --\n",
       "├─ChannelMLP: 1-7                        [64, 32, 64, 64]          [64, 1, 64, 64]           --                        True\n",
       "│    └─ModuleList: 2-26                  --                        --                        --                        True\n",
       "│    │    └─Conv1d: 3-27                 [64, 32, 4096]            [64, 128, 4096]           4,224                     True\n",
       "│    │    └─Conv1d: 3-28                 [64, 128, 4096]           [64, 1, 4096]             129                       True\n",
       "============================================================================================================================================\n",
       "Total params: 3,923,505\n",
       "Trainable params: 3,923,505\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.56\n",
       "============================================================================================================================================\n",
       "Input size (MB): 1.05\n",
       "Forward/backward pass size (MB): 1545.60\n",
       "Params size (MB): 0.07\n",
       "Estimated Total Size (MB): 1546.72\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from utils.neuraloperator import FNO\n",
    "from torchinfo import summary\n",
    "\n",
    "model = FNO(**config['model']['params'])\n",
    "pl_module = CustomModule(\n",
    "        model=model, \n",
    "        optconfig=config['optimizer'], \n",
    "        )\n",
    "batch_size = config['data'][0]['params']['batch_size']\n",
    "print(pl_module.device)\n",
    "summary(\n",
    "        pl_module.model,\n",
    "        input_size=(batch_size, 1, N, N),       # batch_size, in_channels, H, W\n",
    "        col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    "        depth=5,                           # how many nested modules to show\n",
    "        verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Trainer import Trainer\n",
    "from utils.DLCallbacks import CheckPoint, EarlyStopping\n",
    "from utils.CFDCallback_Spectrum import CFDCallback_Spectrum\n",
    "from utils.CFDCallback_PhaseError import CFDCallback_PhaseError\n",
    "from utils.CFDCallback_PDF import CFDCallback_PDF\n",
    "from utils.CFDCallback_Field import CFDCallback_Field\n",
    "\n",
    "def main(device='cuda'):\n",
    "    logger = None \n",
    "    logger = wandb.init(project=config['name'], \n",
    "                        # name=config['name'], \n",
    "                        config=config\n",
    "                        )\n",
    "    if 'seed' in config: torch.manual_seed(config['seed'])\n",
    "\n",
    "    data_config = config['data'][0]\n",
    "    fname=f\"2dHIT_{config['model']['name']}_nu={data_config['nu']}\"\n",
    "    fname += f\"_T={data_config['idx_leadtime']}\" if 'idx_leadtime' in data_config else f\"_T={data_config['leadtime']}TL\"\n",
    "    if logger: fname += f'_{logger.name}'\n",
    "\n",
    "    optconfig = config['optimizer']\n",
    "    if 'loss_fn' in config: optconfig['loss_fn'] = config['loss_fn']\n",
    "    if 'lr' in config: optconfig['params']['lr'] = config['lr']\n",
    "    modelconfig = {**config['model'], }\n",
    "    modelconfig['params'].update({k: v for k, v in config.items() if k in config['model']['params']})\n",
    "    \n",
    "\n",
    "    model = FNO(**config['model']['params'])\n",
    "    pl_module = CustomModule(\n",
    "            model=model, \n",
    "            optconfig=config['optimizer'], \n",
    "            )\n",
    "    # pl_module.load('./pretrain/2dHIT_FNO_nu=5e-05_T=0.1TL_different-dust-234_top0', \n",
    "    #     # load_opt=False, \n",
    "    #     # freeze=True, \n",
    "    #     )\n",
    "    if logger: wandb.watch(pl_module, log=\"all\", log_freq=100)\n",
    "    callbacks = [\n",
    "            CFDCallback_Spectrum(file_dir='./result/', fname=fname), \n",
    "            CFDCallback_PhaseError(file_dir='./result/', fname=fname), \n",
    "            CFDCallback_PDF(file_dir='./result/', fname=fname), \n",
    "            CFDCallback_Field(file_dir='./result/', fname=fname), \n",
    "            CustomCallback(criterions=test_criterion, file_dir='./result/', fname=fname, device=device,\n",
    "                        ),\n",
    "            CheckPoint(\n",
    "                ckpt_name=fname, ckpt_path='./checkpoint/', \n",
    "                every_n_epoch = 1,\n",
    "                criterion=nn.MSELoss(), mode = 'min', \n",
    "                load_ckpt=False, \n",
    "                ),\n",
    "            EarlyStopping(criterion=nn.MSELoss(), # neuralop.H1Loss(d=2, reductions='mean'),# \n",
    "                            mode='min', min_delta=1e-5, \n",
    "                            patience=5, verbose=True, divergence_threshold=1e3, \n",
    "                            stopping_threshold=1e-3, \n",
    "                            ),\n",
    "            ]\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config['epochs'],\n",
    "        check_val_every_n_epochs=1,\n",
    "        # check_val_every_n_iter=30000, \n",
    "        enable_progress_bar=True,\n",
    "        callbacks=callbacks, \n",
    "        logger=logger,\n",
    "        )\n",
    "    \n",
    "    trainer.fit(\n",
    "        model=pl_module,\n",
    "        train_dataloaders=dataloaders['train'], \n",
    "        val_dataloaders=dataloaders['valid'],\n",
    "        # datamodule = datamodules,\n",
    "        )\n",
    "    if logger: wandb.finish()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: hjungwon034 (jungwonheo) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\RESEARCH\\2DIso\\DL_FNO\\wandb\\run-20251015_092025-9aoudgmy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jungwonheo/2dHIT_FNO/runs/9aoudgmy' target=\"_blank\">swift-monkey-266</a></strong> to <a href='https://wandb.ai/jungwonheo/2dHIT_FNO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jungwonheo/2dHIT_FNO' target=\"_blank\">https://wandb.ai/jungwonheo/2dHIT_FNO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jungwonheo/2dHIT_FNO/runs/9aoudgmy' target=\"_blank\">https://wandb.ai/jungwonheo/2dHIT_FNO/runs/9aoudgmy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 99/2851 [00:09<04:05, 11.19it/s]c:\\Users\\JungwonHeo\\anaconda3\\envs\\FNOv1\\lib\\site-packages\\wandb\\integration\\torch\\wandb_torch.py:197: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Copy.cpp:250.)\n",
      "  flat = flat.type(torch.cuda.FloatTensor)\n",
      "100%|██████████| 2851/2851 [06:29<00:00,  7.32it/s]\n",
      "c:\\Users\\JungwonHeo\\anaconda3\\envs\\FNOv1\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\JungwonHeo\\anaconda3\\envs\\FNOv1\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 285/285 [00:41<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Epoch: 0, l2: 0.049116, h1: 0.374333, h2: 0.744673, fRMS_k<8: 0.003922, fRMS_8<k<16: 0.034504, fRMS_k>16: 0.353342, fRMS_k<kmax: 0.009065, fRMS_k>kmax: 0.353342, fRMS_k>train: 0.905721, fRMS_k<train: 0.042560, vor_rms: 0.023264, R_squared: 0.950771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Epoch: 0, l2: 0.108379, h1: 0.796934, h2: 0.993781, fRMS_k<8: 0.005050, fRMS_8<k<16: 0.037208, fRMS_k>16: 0.589371, fRMS_k<kmax: 0.011046, fRMS_k>kmax: 0.589371, fRMS_k>train: 0.938619, fRMS_k<train: 0.055409, vor_rms: 0.029998, R_squared: 0.895725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2851/2851 [06:17<00:00,  7.55it/s]\n",
      "100%|██████████| 285/285 [00:40<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Epoch: 1, l2: 0.039128, h1: 0.332830, h2: 0.684872, fRMS_k<8: 0.002820, fRMS_8<k<16: 0.026979, fRMS_k>16: 0.278233, fRMS_k<kmax: 0.006879, fRMS_k>kmax: 0.278233, fRMS_k>train: 0.816578, fRMS_k<train: 0.033237, vor_rms: 0.013988, R_squared: 0.960767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Epoch: 1, l2: 0.095504, h1: 0.771436, h2: 0.984941, fRMS_k<8: 0.003750, fRMS_8<k<16: 0.029875, fRMS_k>16: 0.525552, fRMS_k<kmax: 0.008608, fRMS_k>kmax: 0.525552, fRMS_k>train: 0.917860, fRMS_k<train: 0.043530, vor_rms: 0.025283, R_squared: 0.908083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 275/2851 [00:38<05:59,  7.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 66\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m     38\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     39\u001b[0m         CFDCallback_Spectrum(file_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./result/\u001b[39m\u001b[38;5;124m'\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname), \n\u001b[0;32m     40\u001b[0m         CFDCallback_PhaseError(file_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./result/\u001b[39m\u001b[38;5;124m'\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m                         ),\n\u001b[0;32m     56\u001b[0m         ]\n\u001b[0;32m     57\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     58\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     59\u001b[0m     check_val_every_n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[0;32m     64\u001b[0m     )\n\u001b[1;32m---> 66\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpl_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# datamodule = datamodules,\u001b[39;49;00m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger: wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\RESEARCH\\2DIso\\DL_FNO\\utils\\Trainer.py:88\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_loader \u001b[38;5;129;01min\u001b[39;00m train_dataloaders:\n\u001b[0;32m     87\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m## validation \u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_dataloaders \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_val_every_n_epochs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32md:\\RESEARCH\\2DIso\\DL_FNO\\utils\\Trainer.py:111\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, model, dataloaders, datamodule)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callbacks\u001b[38;5;241m.\u001b[39mcall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_start\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, pl_module\u001b[38;5;241m=\u001b[39mmodel) \n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar_wrapper(\u001b[38;5;28menumerate\u001b[39m(dataloaders), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloaders)):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callbacks\u001b[38;5;241m.\u001b[39mcall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_batch_start\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, pl_module\u001b[38;5;241m=\u001b[39mmodel, batch\u001b[38;5;241m=\u001b[39mbatch, batch_idx\u001b[38;5;241m=\u001b[39mbatch_idx) \n\u001b[0;32m    113\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining_step(batch, batch_idx)\n",
      "File \u001b[1;32mc:\\Users\\JungwonHeo\\anaconda3\\envs\\FNOv1\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JungwonHeo\\anaconda3\\envs\\FNOv1\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\JungwonHeo\\anaconda3\\envs\\FNOv1\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\JungwonHeo\\anaconda3\\envs\\FNOv1\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\JungwonHeo\\anaconda3\\envs\\FNOv1\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 185\u001b[0m, in \u001b[0;36mHIT2dDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx:\u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 185\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_one_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "Cell \u001b[1;32mIn[5], line 211\u001b[0m, in \u001b[0;36mHIT2dDataset._load_one_sample\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    208\u001b[0m         data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(data)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_path[ifile], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 211\u001b[0m         target \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvorticity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43misim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_stride\u001b[49m\u001b[43m:\u001b[49m\u001b[43mit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_stride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_output\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# (n_out, channels, *datashape)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m         target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(target)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, target\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\JungwonHeo\\anaconda3\\envs\\FNOv1\\lib\\site-packages\\h5py\\_hl\\dataset.py:781\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, args, new_dtype)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 781\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FNOv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
