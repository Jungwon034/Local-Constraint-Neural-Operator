{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import scipy\n",
    "import numpy as np \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('./utils/tecplot.mplstyle')\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# import neuralop \n",
    "# from utilities import *\n",
    "from utils.neuraloperator import * \n",
    "\n",
    "from pathlib import Path\n",
    "from torch import Tensor\n",
    "from typing import Any, List, Tuple, Mapping, Optional, Iterable, Union, Dict, Literal\n",
    "\n",
    "## libraries for CFD\n",
    "# import cupy as cp\n",
    "import h5py \n",
    "import yaml\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'name': '2dHIT_UNet', \n",
    "    'architecture': 'UNet',\n",
    "    'model': {\n",
    "        'params': {\n",
    "            'in_channels': 1, \n",
    "            'out_channels': 1, \n",
    "            'activation': 'GELU', # nn.LeakyReLU(0.2), \n",
    "            'pooling': nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            'norm': 'instancenorm',\n",
    "            'embedding_channels': 1024, \n",
    "            # 'channels': None, # 1,\n",
    "        },\n",
    "        'device': device,\n",
    "    },\n",
    "    \n",
    "    'epochs': 50, \n",
    "    'optimizer': {\n",
    "        'loss_fn': 'h1',\n",
    "        'params': {\n",
    "            'weight_decay': 5e-4, \n",
    "            'lr': 1e-4,\n",
    "            # 'amp_autocast': False, \n",
    "        },\n",
    "\n",
    "        'scheduler': {\n",
    "            'name': 'ExponentialLR', \n",
    "            'params': {\n",
    "                'gamma': 0.5, \n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    'data': [\n",
    "        { 'nu': 5e-05, 'leadtime': 0.1, # 'idx_leadtime': 1, \n",
    "         'stage': 'fit',\n",
    "        'params': {\n",
    "            'base_path': '../Data/nu=5e-05_n=512_fDNS=128/', \n",
    "            'dataset_name': f'2dHIT_nu=5e-05_n=512_T=17.0_fDNS=128', \n",
    "            \n",
    "            'n_input': 1, \n",
    "            'n_output':1, \n",
    "            'batch_size': 64,\n",
    "            'num_workers': 0, \n",
    "\n",
    "            'Ndata_train':500,  \n",
    "            'Ndata_val':30, \n",
    "        },\n",
    "        'normalization': {\n",
    "            'normalization_path': '../Data/nu=5e-05_n=512_fDNS=128/config.yaml',\n",
    "        }\n",
    "        },\n",
    "    #     { 'nu': 5e-05, 'leadtime': 0.1, # 'idx_leadtime': 1, \n",
    "    #      'stage': 'valid',\n",
    "    #     'params': {\n",
    "    #         'base_path': '../Data/nu=5e-05_n=512/', \n",
    "    #         'dataset_name': f'2dHIT_nu=5e-05_n=512_T=17.0', \n",
    "            \n",
    "    #         'n_input': 1, \n",
    "    #         'n_output':1, \n",
    "    #         'batch_size': 64,\n",
    "    #         'num_workers': 0,\n",
    "\n",
    "    #         'Ndata_val':1,  \n",
    "    #     },\n",
    "    #     'normalization': {\n",
    "    #         'normalization_path': '../Data/nu=5e-05_n=512/config.yaml',\n",
    "    #     }\n",
    "    # },\n",
    "    ]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CFD equation parameters ###\n",
    "integral_timescales = {\n",
    "    1e-3: 1.870162606239319,\n",
    "    5e-4: 1.5808453559875488,\n",
    "    1e-4: 1.3038111189,\n",
    "    5e-05: 1.15829598903656, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rms(vel:torch.Tensor, \n",
    "             dim:int=2,\n",
    "             ) -> torch.Tensor: \n",
    "    \"\"\"\n",
    "    Compute the root mean square. \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if not dim: \n",
    "        dim = vel.ndim - 1\n",
    "        vel = vel.unsqueeze(0) \n",
    "    batch_shape, channels, shape = vel.shape[:-(dim+1)], vel.shape[-(dim+1)], vel.shape[-dim:] \n",
    "    fft_dim = tuple(range(-dim, 0))\n",
    "\n",
    "    vel_fluc = vel - vel.mean(dim=fft_dim, keepdim=True)\n",
    "    vel_rms = vel_fluc.pow(2).mean(dim=fft_dim)\n",
    "    vel_rms = torch.sqrt(vel_rms.mean(dim=-1))\n",
    "    return vel_rms\n",
    "\n",
    "def vor2vel(vor: torch.Tensor, \n",
    "            dim: int=None, \n",
    "            ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert vorticity field to vector field.\n",
    "        ð‘¢Ì‚(ð‘˜) = (ð‘– Â· (ð‘˜ Ã— ðœ”Ì‚(ð‘˜)))â„âˆ£ð‘˜âˆ£Â²\n",
    "        \n",
    "    Parameters:\n",
    "    - vor (torch.tensor): vorticity field, shape (batch_shape, 1, Nx, Ny) for 2d or (batch_shape, 3, Nx, Ny, Nz) for 3d. \n",
    "    - dim (int): dimension of the field. \n",
    "    \n",
    "    Returns:\n",
    "    - vec (torch.tensor): vector field, shape (batch_shape, 2, Nx, Ny) for 2d or (batch_shape, 3, Nx, Ny, Nz) for 3d. \n",
    "    \"\"\"\n",
    "    if not dim: \n",
    "        dim = vor.ndim - 1\n",
    "        vor = vor.unsqueeze(0) \n",
    "    batch_shape, channels, shape = vor.shape[:-(dim+1)], vor.shape[-(dim+1)], vor.shape[-dim:] \n",
    "\n",
    "    ## Compute the wavenumber \n",
    "    k = [torch.fft.fftfreq(n, 1/n, device=vor.device) for n in shape] \n",
    "    k = torch.meshgrid(k, indexing='ij')\n",
    "    k = torch.stack(k)\n",
    "    k2 = torch.sum(k**2, axis=0); k2[tuple(0 for _ in k2.shape)] = 1\n",
    "\n",
    "    ## Compute velocity field\n",
    "    fft_dim = tuple(range(-dim, 0)) \n",
    "    vor_k = torch.fft.fftn(vor, dim=fft_dim, norm='forward') # forard norm. refer: Pope (2000) p.678\n",
    "    \n",
    "    if dim == 2: \n",
    "        cross = torch.concat([k[1] * vor_k, - k[0] * vor_k], dim=len(batch_shape))\n",
    "    else: \n",
    "        k = k.to(vor_k.dtype).unsqueeze(0)\n",
    "        cross = torch.cross(k, vor_k, dim=1)\n",
    "    k2 = k2.unsqueeze(0).unsqueeze(0) \n",
    "    \n",
    "    vel_k = 1j * cross / k2 ## ð‘¢Ì‚(ð‘˜) = (ð‘– Â· (ð‘˜ Ã— ðœ”Ì‚(ð‘˜)))â„âˆ£ð‘˜âˆ£Â²\n",
    "    vel = torch.fft.ifftn(vel_k, dim=fft_dim, norm='forward').real\n",
    "    return vel\n",
    "\n",
    "def calc_tke(\n",
    "        vor:torch.Tensor=None,\n",
    "        vel:torch.Tensor=None, \n",
    "        dim:int=2, \n",
    "        ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the turbulent kinetic energy (TKE) from velocity field.\n",
    "        TKE = 0.5 * âŸ¨uÂ² + vÂ² + wÂ²âŸ©\n",
    "    \n",
    "    Parameters:\n",
    "    - vel (torch.tensor): velocity field, shape (B, 2, Nx, Ny) for 2d or (B, 3, Nx, Ny, Nz) for 3d.\n",
    "    - dim (int): dimension of the field. \n",
    "\n",
    "    Returns:\n",
    "    - tke (torch.tensor): TKE, shape (B,) \n",
    "    \"\"\"\n",
    "    if vor is not None: vel = vor2vel(vor, dim=dim)\n",
    "    if not dim or dim == vel.ndim - 1: \n",
    "        dim = vel.ndim - 1\n",
    "        vel = vel.unsqueeze(0) \n",
    "    batch_shape, channels, shape = vel.shape[:-(dim+1)], vel.shape[-(dim+1)], vel.shape[-dim:] \n",
    "    \n",
    "    fft_dim = tuple(range(-dim, 0))\n",
    "    tke = 0.5 * torch.mean((vel - torch.mean(vel, dim=fft_dim, keepdims=True))**2, dim=(-(dim+1),*fft_dim))\n",
    "    return tke\n",
    "\n",
    "def calc_dissipation(\n",
    "        nu:float,\n",
    "        vel:torch.Tensor=None, \n",
    "        vor:torch.Tensor=None,\n",
    "        dim:int=2, \n",
    "        ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the dissipation rate from vorticity and velocity field.\n",
    "        for 3d, Îµ = 2 * Î½ * âŸ¨sáµ¢â±¼ * sáµ¢â±¼âŸ© \n",
    "                sáµ¢â±¼ = 0.5 * (duáµ¢/dxâ±¼ + duâ±¼/dxáµ¢)\n",
    "        for 3d HIT, âŸ¨sáµ¢â±¼ * sáµ¢â±¼âŸ© = (S11Â² + S22Â² + S33Â² + 2*(0.5*(S12+S21))Â² + 2*(0.5*(S13+S31))Â² + 2*(0.5*(S23+S32))Â²)\n",
    "        for 2d HIT, Îµ = 2 * Î½ * Î©\n",
    "    Parameters:\n",
    "    - nu (float): kinematic viscosity.\n",
    "    - vel (torch.tensor): velocity field, shape (B, 2, Nx, Ny) for 2d or (B, 3, Nx, Ny, Nz) for 3d.\n",
    "    - vor (torch.tensor): vorticity field, shape (B, 1, Nx, Ny) for 2d or (B, 3, Nx, Ny, Nz) for 3d. \n",
    "    - dim (int): dimension of the field. \n",
    "\n",
    "    Returns:\n",
    "    - dissipation (torch.tensor): dissipation rate, shape (B,) if reduction is 'mean' or (B, 1, Nx, Ny) for 2d or (B, 1, Nx, Ny, Nz) for 3d.\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        if vel is not None: \n",
    "            dim = vel.ndim - 1\n",
    "            vel = vel.unsqueeze(0) \n",
    "        else: \n",
    "            dim = vor.ndim - 1\n",
    "            vor = vor.unsqueeze(0) \n",
    "    batch_shape, channels, shape = (\n",
    "        (vel.shape[:-(dim+1)], vel.shape[-(dim+1)], vel.shape[-dim:])\n",
    "        if vel is not None\n",
    "        else (vor.shape[:-(dim+1)], vor.shape[-(dim+1)], vor.shape[-dim:])\n",
    "    )\n",
    "    fft_dim = tuple(range(-dim, 0))\n",
    "    \n",
    "    if dim == 2:\n",
    "        enstrophy = 0.5 * (vor - torch.mean(vor, dim=fft_dim, keepdim=True))**2\n",
    "        ## dissipation rate field for 2d HIT Îµ = 2 * Î½ * Î©\n",
    "        dissipation = 2 * nu * torch.mean(enstrophy, dim=fft_dim).mean(dim=-1)\n",
    "\n",
    "    elif dim == 3:\n",
    "        if vor is not None: vel = vor2vel(vor, dim=dim)\n",
    "        ## Compute the wavenumber \n",
    "        k = [torch.fft.fftfreq(n, 1/n, device=vel.device) for n in shape] \n",
    "        k = torch.meshgrid(k, indexing='ij')\n",
    "        k = torch.all(k)\n",
    "\n",
    "        ## Compute velocity gradient\n",
    "        vel_k = torch.fft.fftn(vel, dim=fft_dim, norm='forward') # forard norm. refer: Pope (2000) p.678        \n",
    "        vel_grad_k = torch.all([1j * k[i].unsqueeze(0).unsqueeze(0) * vel_k for i in range(dim)], dim=1) # (B, 3(i), 3(j), Nx, Ny, Nz)\n",
    "        vel_grad = torch.fft.ifftn(vel_grad_k, dim=fft_dim, norm='forward').real\n",
    "        u_grad, v_grad, w_grad = vel_grad.unbind(dim=1) # (B, 3(j), Nx, Ny, Nz)\n",
    "        u_x, u_y, u_z = u_grad.unbind(dim=1) # (B, Nx, Ny, Nz)\n",
    "        v_x, v_y, v_z = v_grad.unbind(dim=1)    \n",
    "        w_x, w_y, w_z = w_grad.unbind(dim=1)\n",
    "        ## Stain rate tensor\n",
    "        s_ij2 = (\n",
    "            u_x**2 + v_y**2 + w_z**2 # S11, S22, S33\n",
    "            + 2 * (0.5 * (u_y + v_x))**2 # S12\n",
    "            + 2 * (0.5 * (v_z + w_y))**2 # S23\n",
    "            + 2 * (0.5 * (u_z + w_x))**2 # S13\n",
    "        ).unsqueeze(1)\n",
    "        ## dissipation rate field \n",
    "        dissipation = 2 * nu * torch.mean(s_ij2, dim=fft_dim)\n",
    "    return dissipation\n",
    "\n",
    "def calc_Kolmogorov_lengthscale(dissipation:Union[torch.Tensor, float],\n",
    "                                nu:float, \n",
    "                                ) -> Union[torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Compute the Kolmogorov lengthscale \n",
    "        Î· = Î½^(3/4)/Îµ^(1/4)\n",
    "    \n",
    "    Parameters \n",
    "    - dissipation (torch.Tensor or float): dissipation rate. shape (B, )\n",
    "    - nu (float): kinematic viscosity\n",
    "\n",
    "    Returns: \n",
    "    - eta (torch.Tensor or float): Kolmogorov lenghscale. shape (B, )\n",
    "    \"\"\"\n",
    "    return nu**(3/4) / dissipation**(1/4)\n",
    "\n",
    "def calc_k(shape:tuple) -> torch.Tensor:\n",
    "    '''\n",
    "    Compute the wavenumbers from the shape of a 2D or 3D grid\n",
    "\n",
    "    Parameters: \n",
    "    - shape (Tuple): The shape of the grid. (Nx, Ny) for 2D or (Nx, Ny, Nz) for 3D\n",
    "\n",
    "    Returns: \n",
    "    - k (torch.tensor): The wavenumbers, shape (Nx, Ny) for 2D or (Nx, Ny, Nz) for 3D\n",
    "    '''\n",
    "    global k\n",
    "    if 'k' in globals(): return k\n",
    "    k = [torch.fft.fftfreq(n, 1/n) for n in shape] \n",
    "    k = torch.meshgrid(k, indexing='ij')\n",
    "    k = torch.stack(k)\n",
    "    # k = torch.sqrt(torch.sum(k**2, axis=0)) # tip: faster than torch.sum.\n",
    "    \n",
    "    return k\n",
    "\n",
    "def calc_energy_spectrum(vel: torch.Tensor, dim:int=2) -> torch.Tensor:\n",
    "    '''\n",
    "    Compute the energy spectrum from a 2D or 3D velocity field\n",
    "        for 3d, E(k) = âŸ¨|u(k)|Â²âŸ©     \n",
    "        for 2d, E(k) = Ï€ k âŸ¨|u(k)|Â²âŸ© \n",
    "    Parameters: \n",
    "    - vel (torch.tensor): The velocity field. \n",
    "        shape:  \n",
    "            (B, 2, Nx, Ny) for 2d velocity field (with batch)\n",
    "            (B, 3, Nx, Ny, Nz) for 3d velocity field\n",
    "            (B, 1, Nx, Ny) for 2d vorticify field (enstrophy spectrum)\n",
    "            (B, 3, Nx, Ny, Nz) for 3d vorticity field (enstrophy spectrum)\n",
    "    - dim (int): The dimension of the velocity field. if None, the dimension is inferred from the shape of vel\n",
    "\n",
    "    Returns: \n",
    "    - spectrum (torch.tensor): The energy spectrum. shape (B, Nk,) if batch, or (Nk,)\n",
    "    '''\n",
    "    if not dim: \n",
    "        dim = vel.ndim - 1\n",
    "        vel = vel.unsqueeze(0) \n",
    "    batch_shape, channels, shape = vel.shape[:-(dim+1)], vel.shape[-(dim+1)], vel.shape[-dim:] \n",
    "\n",
    "    ## Compute the wavenumbers\n",
    "    k = [torch.fft.fftfreq(n, 1/n, device=vel.device) for n in shape] \n",
    "    k = torch.meshgrid(k, indexing='ij')\n",
    "    k = torch.stack(k)\n",
    "    k = torch.sqrt(torch.sum(k**2, axis=0)) \n",
    "    Nk = min(shape)//2 # int(torch.ceil(k.max())) \n",
    "\n",
    "    ## Compute the energy spectrum\n",
    "    fft_dim = tuple(range(-dim, 0)) \n",
    "    vel_k = torch.fft.fftn(vel, dim=fft_dim, norm='forward') # forard norm. refer: Pope (2000) p.678\n",
    "    if dim == 2: \n",
    "        ## E(k) = Ï€ k âŸ¨|u(k)|Â²âŸ© for 2d spectrum \n",
    "        E_k = 0.5 * (np.pi * k * torch.abs(vel_k)**2).sum(dim=-(dim+1)) # tip: faster than torch.real(vel_k * torch.conj(vel_k))\n",
    "    else: \n",
    "        ## E(k) = âŸ¨|u(k)|Â²âŸ© for 3d spectrum\n",
    "        E_k = 0.5 * (torch.abs(vel_k)**2).sum(dim=-(dim+1)) \n",
    "    \n",
    "    ## Compute index\n",
    "    idx = torch.floor(k + 0.5).long()\n",
    "    idx = torch.where((idx > Nk) | (idx < 0), torch.zeros_like(idx), idx)\n",
    "    \n",
    "    ## tips: scatter_add_ is faster than loop\n",
    "    spectrum = torch.zeros((*batch_shape, Nk + 1), device=vel.device)\n",
    "    idx = idx.view(-1)[(None,) * len(batch_shape)]\n",
    "    spectrum.scatter_add_(-1, \n",
    "                          index =idx.expand(*batch_shape, -1), \n",
    "                          src=E_k.view(*batch_shape, -1),\n",
    "                          )\n",
    "    \n",
    "    if batch_shape == 1: spectrum = spectrum.squeeze(0)\n",
    "    return spectrum\n",
    "\n",
    "def calc_enstrophy_spectrum(vor, dim:int=2):\n",
    "    return calc_energy_spectrum(vor, dim=dim)\n",
    "\n",
    "def calc_taylor_lengthscale(\n",
    "        vel:torch.Tensor=None, \n",
    "        vor:torch.Tensor=None,\n",
    "        dim:int=2, \n",
    "        nu: float=None, \n",
    "        dissipation: Union[torch.Tensor, float]=None,\n",
    "        ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the Taylor lengthscale.\n",
    "        for 2d, Î» = (uáµ£â‚˜â‚› / Ï‰áµ£â‚˜â‚›)^0.5\n",
    "        for 3d, Î» = uáµ£â‚˜â‚› * (âˆ‚ uâ‚/âˆ‚ xâ‚)^{-0.5}\n",
    "                  = (15 * Î½/Îµ)^0.5 * uáµ£â‚˜â‚›\n",
    "    Parameters:\n",
    "    - vel (torch.tensor): velocity field, shape (B, 2, Nx, Ny) for 2d or (B, 3, Nx, Ny, Nz) for 3d.\n",
    "    - vor (torch.tensor): vorticity field, shape (B, 1, Nx, Ny) for 2d or (B, 3, Nx, Ny, Nz) for 3d. \n",
    "    - dim (int): dimension of the field. \n",
    "    - nu (float): kinematic viscosity\n",
    "    - dissipation (torch.tensor): dissipation rate, shape (B, )\n",
    "\n",
    "    Returns:\n",
    "    - taylor_lengthscale (torch.tensor): taylor lengthscale, shape (B,)\n",
    "\n",
    "    Reference:\n",
    "    - Boffetta, G., & Ecke, R. E. (2011). Two-Dimensional Turbulence.\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        if vel is not None: \n",
    "            dim = vel.ndim - 1\n",
    "            vel = vel.unsqueeze(0) \n",
    "        else: \n",
    "            dim = vor.ndim - 1\n",
    "            vor = vor.unsqueeze(0) \n",
    "    batch_shape, channels, shape = (\n",
    "        (vel.shape[:-(dim+1)], vel.shape[-(dim+1)], vel.shape[-dim:])\n",
    "        if vel is not None\n",
    "        else (vor.shape[:-(dim+1)], vor.shape[-(dim+1)], vor.shape[-dim:])\n",
    "    )\n",
    "    fft_dim = tuple(range(-dim, 0))\n",
    "\n",
    "    if dim == 2: \n",
    "        if vel is None: vel = vor2vel(vor, dim=2)\n",
    "        u_rms = calc_rms(vel, dim=dim) # torch.sqrt(torch.mean((vel - vel.mean(dim=fft_dim, keepdim=True)).pow(2), dim=fft_dim, keepdim=True).mean(dim=-1, keepdim=True))\n",
    "        w_rms = calc_rms(vor, dim=dim) # torch.std(vor, dim=fft_dim)\n",
    "        taylor_lengthscale = (u_rms / w_rms) ** 0.5 \n",
    "    elif dim == 3:\n",
    "        pass\n",
    "    return taylor_lengthscale\n",
    "\n",
    "def calc_Re_lambda(\n",
    "        vel:torch.Tensor=None, \n",
    "        vor:torch.Tensor=None,\n",
    "        dim:int=2, \n",
    "        nu: float=None, \n",
    "        taylor_lengthscale: Union[torch.Tensor, float]=None,\n",
    "        ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the Taylor-scale Reynolds number Re_Î».\n",
    "        Re_Î» = uáµ£â‚˜â‚› * Î» / Î½\n",
    "\n",
    "    Parameters:\n",
    "    - vel (torch.tensor): velocity field, shape (B, 2, Nx, Ny) for 2d or (B, 3, Nx, Ny, Nz) for 3d.\n",
    "    - vor (torch.tensor): vorticity field, shape (B, 1, Nx, Ny) for 2d or (B, 3, Nx, Ny, Nz) for 3d. \n",
    "    - dim (int): dimension of the field. \n",
    "    - nu (float): kinematic viscosity\n",
    "    - taylor_lengthscale (torch.tensor): taylor lengthscale, shape (B, )\n",
    "\n",
    "    Returns:\n",
    "    - Re_lambda (torch.tensor): taylor-scale Reynolds number, shape (B,)\n",
    "\n",
    "    Reference:\n",
    "    - Boffetta, G., & Ecke, R. E. (2011). Two-Dimensional Turbulence.\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        if vel is not None: \n",
    "            dim = vel.ndim - 1\n",
    "            vel = vel.unsqueeze(0) \n",
    "        else: \n",
    "            dim = vor.ndim - 1\n",
    "            vor = vor.unsqueeze(0) \n",
    "    batch_shape, channels, shape = (\n",
    "        (vel.shape[:-(dim+1)], vel.shape[-(dim+1)], vel.shape[-dim:])\n",
    "        if vel is not None\n",
    "        else (vor.shape[:-(dim+1)], vor.shape[-(dim+1)], vor.shape[-dim:])\n",
    "    )\n",
    "    fft_dim = tuple(range(-dim, 0))\n",
    "\n",
    "    if vel is None: vel = vor2vel(vor, dim=dim)\n",
    "    if taylor_lengthscale is None: taylor_lengthscale = calc_taylor_lengthscale(vel=vel, vor=vor, nu=nu, dim=dim)\n",
    "    u_rms = calc_rms(vel, dim=dim)\n",
    "    return u_rms * taylor_lengthscale / nu\n",
    "\n",
    "def calc_pdf(u, bins=50, range=(-3, 3), dim:int=2):\n",
    "    bins = min(sum(u.shape), bins)\n",
    "    # hist, bin_edges = torch.histogram(u, bins=bins, range=range, density=True)\n",
    "    hist = torch.histc(u, bins=bins, min=range[0], max=range[1])\n",
    "    bin_edges = torch.linspace(*range, bins+1, device=u.device)\n",
    "    hist = hist / (torch.sum(hist) * torch.diff(bin_edges))\n",
    "\n",
    "    bin_edges = (bin_edges[:-1] + bin_edges[1:]) / 2 # bin centers\n",
    "    return torch.stack([bin_edges, hist], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DL ploter & logger \n",
    "def plot_2d_surface(u1, u2=None, axes=None, **kwargs):\n",
    "    '''\n",
    "    Parameters:\n",
    "     - u1 (float np.array): field of t=t1 (1, Nx, Ny)\n",
    "     - u2 (float np.array): field of t=t2 (1, Nx, Ny)\n",
    "    '''\n",
    "    if u2 is None: u2 = []\n",
    "    if not isinstance(u2, list) and u2.ndim == 2: u2 = [u2]\n",
    "    n = 1 + len(u2) if u2 is not None else 1\n",
    "    row = min(n, 5) \n",
    "    col = n // 5 + 1\n",
    "    if axes is None:\n",
    "        fig = plt.figure(figsize=kwargs.get('figsize', (4*row, 4*col))) \n",
    "        ax1 = fig.add_subplot(col, row, 1)\n",
    "        ax2 = [fig.add_subplot(col, row, i+2) for i in range(n - 1)]\n",
    "    else:\n",
    "        ax1, ax2 = axes\n",
    "    vmax = kwargs.get('vmax', u1.max())\n",
    "    vmin = kwargs.get('vmin', u1.min())\n",
    "\n",
    "    ## plot field u1\n",
    "    if torch.is_tensor(u1): u1 = u1.detach().cpu().numpy()\n",
    "    cm1 = ax1.imshow(u1, origin='lower', aspect='auto',\n",
    "                     vmax=vmax, vmin=vmin, \n",
    "                     )\n",
    "    ax1.set(\n",
    "        title=kwargs['title1'] if 'title1' in kwargs else kwargs['title'][0] if 'title' in kwargs else 'ground truth',\n",
    "        xticks=[], yticks=[], \n",
    "    )\n",
    "    if kwargs.get('cm1', False): plt.colorbar(cm1)\n",
    "\n",
    "    ## plot field u2\n",
    "    if torch.is_tensor(u2): u2 = u2.detach().cpu().numpy()\n",
    "    elif isinstance(u2, list) and torch.is_tensor(u2[0]): u2 = [u.detach().cpu().numpy() for u in u2]\n",
    "    for i in range(len(u2)):\n",
    "        cm2 = ax2[i].imshow(u2[i], origin='lower', aspect='auto', \n",
    "                         vmax=vmax, vmin=vmin, \n",
    "                         )\n",
    "        ax2[i].set(\n",
    "            title=kwargs['title2'] if 'title2' in kwargs else kwargs['title'][i+1] if 'title' in kwargs else 'prediction',\n",
    "            xticks=[], yticks=[], \n",
    "        )\n",
    "        if kwargs.get('cm2', False): plt.colorbar(cm2)\n",
    "    plt.suptitle(kwargs.get('suptitle', ''))\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_spectrum(spectrum1, spectrum2=None, ax=None, **kwargs):\n",
    "    '''\n",
    "    plot spectrum in log-log plot \n",
    "    Nk: number of wavenumbers = resolution // 2\n",
    "\n",
    "    Params:\n",
    "    - spectrum1(1d array):            spectrum data \n",
    "    - (optional) spectrum2(1d array): spectrum data. if None, plot only spectrum1 and do not plot legend. \n",
    "    - (optional) ax(matplotlib.axes): plotting axes. if None, make new figure and show\n",
    "    - (optional) kwargs(dict)\n",
    "        - 'label1'(string):    label of spectrum 1 (default: 'target')\n",
    "        - 'label2'(string):    label of spectrum 2 (default: 'prediction')\n",
    "        - 'ylim'(tuple, (2,)): y-axis range (vmin, vmax)\n",
    "        - 'title'(string):     plot title\n",
    "    Assumptions:\n",
    "    - spectrum1 and spectrum2 has same dimension.\n",
    "    '''\n",
    "    spectrum2 = [] if spectrum2 is None else [spectrum2] if not isinstance(spectrum2, list) else spectrum2\n",
    "\n",
    "    Nk = len(spectrum1)\n",
    "    k_1d = np.arange(Nk)\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=kwargs.get('figsize', (3, 3)))\n",
    "        ax = fig.add_subplot()\n",
    "    \n",
    "    ## plot spectrum 1\n",
    "    if torch.is_tensor(spectrum1): spectrum1 = spectrum1.detach().cpu().numpy()\n",
    "    ax.plot(k_1d[1:], spectrum1[1:], \n",
    "            label=kwargs.get('label1', 'target')\n",
    "            )\n",
    "\n",
    "    ## plot spectrum 2\n",
    "    if torch.is_tensor(spectrum2): spectrum2 = spectrum2.detach().cpu().numpy()\n",
    "    elif isinstance(spectrum2, list) and spectrum2 and torch.is_tensor(spectrum2[0]): spectrum2 = [s.detach().cpu().numpy() for s in spectrum2]\n",
    "    for i, spec in enumerate(spectrum2):\n",
    "        Nk = len(spec)\n",
    "        k_1d = np.arange(Nk)\n",
    "        ax.plot(k_1d[1:], spec[1:], label=kwargs.get(f'label{i+2}', 'prediction'))\n",
    "\n",
    "    Nk = max([len(spectrum1)] + [len(s) for s in spectrum2])\n",
    "    \n",
    "    \n",
    "    ax.set(xscale='log', yscale='log', \n",
    "           xticks = [2 ** (np.round(np.log2(Nk)).astype(np.int32) - i) for i in range(min(np.round(np.log2(Nk)).astype(np.int32), 7))],\n",
    "        )\n",
    "    if 'ylim' in kwargs: ax.set_ylim(kwargs['ylim'])\n",
    "    if 'title' in kwargs: ax.set_title(kwargs['title'])\n",
    "    if spectrum2: ax.legend(loc=0)\n",
    "    ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "\n",
    "def plot_pdf(bin1, pdf1, bin2=None, pdf2=None, ax=None, **kwargs):\n",
    "    '''\n",
    "    plot pdf \n",
    "\n",
    "    Params:\n",
    "    - pdf1(1d array): pdf data \n",
    "    - bin1: bin of pdf1\n",
    "    - (optional) pdf2(1d array):      pdf data. \n",
    "    - (optional) bin2(1d array):      bin of pdf2. if None, plot only pdf1 and do not plot legend. \n",
    "    - (optional) ax(matplotlib.axes): plotting axes. if None, make new figure and show\n",
    "    - (optional) kwargs(dict)\n",
    "        - 'label1'(string): label of spectrum 1 (default: 'target')\n",
    "        - 'label2'(string): label of spectrum 2 (default: 'prediction')\n",
    "        - 'title'(string):  plot title\n",
    "    Assumptions:\n",
    "    - pdf1 and pdf2 has same dimension.\n",
    "    '''\n",
    "    if bin2 is not None and type(bin2) is not list:\n",
    "        bin2 = [bin2]\n",
    "        pdf2 = [pdf2]\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=kwargs.get('figsize', (3, 3)))\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "    ## plot pdf1\n",
    "    if torch.is_tensor(pdf1): \n",
    "        bin1 = bin1.detach().cpu().numpy()\n",
    "        pdf1 = pdf1.detach().cpu().numpy()\n",
    "    ax.plot(bin1, pdf1, label=kwargs.get('label1', 'target'))\n",
    "\n",
    "    ## plot pdf2\n",
    "    if torch.is_tensor(pdf2): \n",
    "        bin2 = bin2.detach().cpu().numpy()\n",
    "        pdf2 = pdf2.detach().cpu().numpy()\n",
    "    elif isinstance(pdf2, list) and torch.is_tensor(pdf2[0]): \n",
    "        bin2 = [s.detach().cpu().numpy() for s in bin2]\n",
    "        pdf2 = [s.detach().cpu().numpy() for s in pdf2]\n",
    "    for i in range(len(bin2)):\n",
    "        ax.plot(bin2[i], pdf2[i], label=kwargs.get(f'label{i+2}', 'prediction'))\n",
    "    ax.set(yscale='log')\n",
    "\n",
    "    if 'title' in kwargs: ax.set_title(kwargs['title'])\n",
    "    if bin2 is not None:  ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import torch \n",
    "import h5py\n",
    "import glob\n",
    "import time\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "def high2low_box(\n",
    "    x: torch.Tensor,\n",
    "    scale_factors: Union[float, Sequence[float]] = 0.5,\n",
    "    *,\n",
    "    keepdim: bool = False,\n",
    "    dim: int = 2,\n",
    "    fft_norm: str = \"backward\",\n",
    "    last_var_axis: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Fourierâ€‘domain boxâ€‘style scaling (down / up sampling).\n",
    "\n",
    "    The function supports tensors shaped either as\n",
    "        â€¢ (batch, channels, *spatial_dims)\n",
    "        â€¢ (batch, channels, *spatial_dims, num_var)\n",
    "\n",
    "    In the second case the last axis (``num_var``) is *not* included in the FFT\n",
    "    and therefore remains unchanged.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(scale_factors, float):\n",
    "        scale_factors = [scale_factors] * dim\n",
    "    scale_factors = list(scale_factors)\n",
    "    if len(scale_factors) != dim:\n",
    "        raise ValueError(\"`scale_factors` length must equal `dim`.\")\n",
    "\n",
    "    if x.ndim < dim + 2:\n",
    "        raise ValueError(\n",
    "            f\"Input tensor must have at least {dim + 2} dimensions (got {x.ndim}).\"\n",
    "        )\n",
    "\n",
    "    if last_var_axis:\n",
    "        spatial_axes = list(range(-(dim + 1), -1))\n",
    "    else:\n",
    "        spatial_axes = list(range(-dim, 0))\n",
    "    spatial_shape = [x.shape[i] for i in spatial_axes]\n",
    "\n",
    "    sf_tensor = torch.tensor(scale_factors, dtype=x.dtype, device=x.device)\n",
    "    if torch.allclose(sf_tensor, torch.ones_like(sf_tensor)):\n",
    "        return x.clone()\n",
    "\n",
    "    Fx = torch.fft.fftn(x, dim=spatial_axes, norm=fft_norm)\n",
    "\n",
    "    if (sf_tensor < 1).any():\n",
    "        Fx = torch.fft.fftshift(Fx, dim=spatial_axes)\n",
    "\n",
    "        if keepdim:\n",
    "            mask = torch.ones_like(Fx, dtype=torch.bool)\n",
    "            for rel_ax, (n, sf) in enumerate(zip(spatial_shape, scale_factors)):\n",
    "                trim = int(round((1 - sf) * n / 2))\n",
    "                if trim == 0:\n",
    "                    continue\n",
    "                abs_ax = spatial_axes[rel_ax] % Fx.ndim  # positive index\n",
    "                low = torch.arange(trim, device=x.device)\n",
    "                high = torch.arange(n - trim, n, device=x.device)\n",
    "                mask.index_fill_(abs_ax, low, False)\n",
    "                mask.index_fill_(abs_ax, high, False)\n",
    "            Fx = Fx * mask\n",
    "        else:\n",
    "            slices = [slice(None)] * Fx.ndim\n",
    "            for rel_ax, (n, sf) in enumerate(zip(spatial_shape, scale_factors)):\n",
    "                trim = int(round((1 - sf) * n / 2))\n",
    "                slices[spatial_axes[rel_ax]] = slice(trim, n - trim)\n",
    "            Fx = Fx[tuple(slices)]\n",
    "\n",
    "        Fx = torch.fft.ifftshift(Fx, dim=spatial_axes)\n",
    "\n",
    "    # ----------------------------- upâ€‘sampling branch -------------------------\n",
    "    if (sf_tensor > 1).any():\n",
    "        Fx = torch.fft.fftshift(Fx, dim=spatial_axes)\n",
    "\n",
    "        # F.pad pads the *last* k dims; build list accordingly\n",
    "        last_axes = list(range(-len(spatial_axes) - (1 if last_var_axis else 0), 0))\n",
    "        pads: List[int] = []\n",
    "        for ax in Reflectiond(last_axes):\n",
    "            if ax in spatial_axes:\n",
    "                rel = spatial_axes.index(ax)\n",
    "                n = spatial_shape[rel]\n",
    "                sf = scale_factors[rel]\n",
    "                extra = int(round((sf - 1) * n))\n",
    "                pads.extend([extra // 2, extra - extra // 2])\n",
    "            else:  # var axis â†’ no pad\n",
    "                pads.extend([0, 0])\n",
    "        Fx = F.pad(Fx, pads, mode=\"constant\", value=0.0)\n",
    "        Fx = torch.fft.ifftshift(Fx, dim=spatial_axes)\n",
    "\n",
    "    # --------------------------------------------------- inverse FFT ----------\n",
    "    x_out = torch.fft.ifftn(Fx, dim=spatial_axes, norm=fft_norm).real\n",
    "\n",
    "    # ------------------------------ match statistics --------------------------\n",
    "    def _mom(t: torch.Tensor):\n",
    "        mean = t.mean(dim=spatial_axes, keepdim=True)\n",
    "        std = t.std(dim=spatial_axes, unbiased=False, keepdim=True).clamp_min(1e-12)\n",
    "        return mean, std\n",
    "\n",
    "    mean_in, std_in = _mom(x)\n",
    "    mean_out, std_out = _mom(x_out)\n",
    "\n",
    "    x_out = (x_out - mean_out) / std_out * std_in + mean_in\n",
    "    return x_out\n",
    "\n",
    "class HIT2dDataset(Dataset):\n",
    "    def __init__(self, \n",
    "        path: Optional[str] = None,\n",
    "        base_path: Optional[str] = None,\n",
    "        dataset_name: Optional[str] = None,\n",
    "        split_name: Optional[str] = None,\n",
    "        \n",
    "        normalization:Optional[callable] = None,\n",
    "        transform:Optional[callable] = None,\n",
    "\n",
    "        n_input: int = 1,\n",
    "        n_output: int = 1,\n",
    "        n_stride: int = 0,\n",
    "        max_rollout_steps=100,\n",
    "        max_n_sim: Optional[int] = None,\n",
    "        # batch_size:int=32, \n",
    "        \n",
    "        # num_iteration_per_data:int=None, \n",
    "        # isDataAugmentation:bool=False,\n",
    "        load_data:bool=True,\n",
    "        verbose:bool=True,\n",
    "        **kwargs, \n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.base_path = base_path\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split_name = split_name\n",
    "        if not path: \n",
    "            path = os.path.join(self.base_path, self.split_name, self.dataset_name) # f\"{self.base_path}/{self.split_name}/{self.dataset_name}*\"\n",
    "        self.path = sorted(glob.glob(f\"{path}*\"))\n",
    "        assert self.path, f\"Error: Dataset path {path} does not exist.\"\n",
    "\n",
    "        self.normalization = normalization\n",
    "        self.transform = transform\n",
    "\n",
    "        self.max_rollout_steps = max_rollout_steps\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.n_stride = n_stride\n",
    "        self.max_n_sim = max_n_sim if max_n_sim else np.inf\n",
    "        \n",
    "        self.verbose=verbose\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        self._build_metadata()\n",
    "        self._calc_len()\n",
    "        self.load_data = load_data\n",
    "        if load_data:\n",
    "            self._load_data()\n",
    "\n",
    "    def _calc_len(self):\n",
    "        self.n_sim = min(self.max_n_sim, sum(self.n_sim_per_file)) # self.n_sim = sum(self.n_sim_per_file) # len(self.data)\n",
    "        self.n_steps_per_sim = self.Nt\n",
    "        self.n_windows_per_sim = self.n_steps_per_sim - (self.n_input + self.n_output + self.n_stride) + 1\n",
    "        self.len = self.n_sim * self.n_windows_per_sim\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        data = self._load_one_sample(idx)\n",
    "        data = self._preprocess_data(data)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        if self.normalization:\n",
    "            data = self.normalization(data)      \n",
    "        data = self._postprocess_data(data)\n",
    "        return data\n",
    "    \n",
    "    def _load_one_sample(self, idx:int):\n",
    "        isim = idx // self.n_windows_per_sim\n",
    "        it = idx % self.n_windows_per_sim\n",
    "        if self.load_data:\n",
    "            data = self.data[isim, it:it + self.n_input] # (n_in, channels, *datashape)\n",
    "            target = self.data[isim, it + self.n_input + self.n_stride:it + self.n_input + self.n_stride + self.n_output] # (n_out, channels, *datashape)\n",
    "        else: \n",
    "            for i in range(len(self.n_sim_per_file)):\n",
    "                ifile = i \n",
    "                if isim < sum(self.n_sim_per_file[:i+1]): break \n",
    "            \n",
    "            isim = isim - sum(self.n_sim_per_file[:i])\n",
    "            with h5py.File(self.path[ifile], 'r') as f:\n",
    "                data = f[\"fields\"]['vorticity'][isim, it:it + self.n_input] # (n_in, channels, *datashape)\n",
    "                target = f[\"fields\"]['vorticity'][isim, it + self.n_input + self.n_stride:it + self.n_input + self.n_stride + self.n_output] # (n_out, channels, *datashape)\n",
    "                \n",
    "        return data, target\n",
    "\n",
    "    def _load_data(self):\n",
    "        self.data = []\n",
    "        self.n_sim = 0\n",
    "        if self.verbose: start_time = time.time()\n",
    "        for path in self.path:\n",
    "            with h5py.File(path, 'r') as f:\n",
    "                _n_sim = f.attrs[\"n_trajectories\"]\n",
    "                \n",
    "                end = min(self.max_n_sim - self.n_sim, _n_sim)\n",
    "                vorticity = f[\"fields\"]['vorticity'][:end]# vorticity = f[\"fields\"]['vorticity'][:]\n",
    "                if self.verbose: \n",
    "                    print(f'Data Loaded from{path}. shape: {vorticity.shape}, memory: {vorticity.nbytes/1e6} (MB)')\n",
    "            \n",
    "            self.data.append(vorticity)\n",
    "            self.n_sim += len(vorticity)\n",
    "            if self.max_n_sim <= self.n_sim: break\n",
    "            \n",
    "        self.data = np.concatenate(self.data, axis=0)\n",
    "        memory = self.data.nbytes\n",
    "        self.data = torch.from_numpy(self.data)\n",
    "        # self.data = self.data.reshape(self.data.size(0) * self.data.size(1), self.data.size()[2:]) # self.data = torch.concat(self.data, dim=0)\n",
    "        if self.verbose: print(f'Data Loaded. shape: {self.data.shape}, dtype: {self.data.dtype}, time: {time.time() - start_time} (sec), memory: {memory/1e6} (MB)')\n",
    "        self._calc_len()\n",
    "        \n",
    "    def _build_metadata(self):\n",
    "        \n",
    "        with h5py.File(self.path[0], 'r') as f:\n",
    "            self.nu = f['scalars']['nu'][()]\n",
    "\n",
    "            self.t = t = f[\"dimensions\"][\"t\"][:]\n",
    "            self.x = x = f[\"dimensions\"][\"x\"][:]\n",
    "            self.y = y = f[\"dimensions\"][\"y\"][:]\n",
    "            self.X, self.Y = np.meshgrid(x, y)\n",
    "\n",
    "            self.Lx = x[-1] - x[0]\n",
    "            self.Ly = y[-1] - y[0]\n",
    "            self.dx = x[1] - x[0]\n",
    "            self.dy = y[1] - y[0]\n",
    "            self.Nx = len(x)\n",
    "            self.Ny = len(y)\n",
    "            self.dt = t[1] - t[0]\n",
    "            self.Nt = len(t)\n",
    "            self.T = t[-1]\n",
    "            self.t0 = t[0]\n",
    "\n",
    "        self.n_sim_per_file = []\n",
    "        for path in self.path:\n",
    "            with h5py.File(path, 'r') as f:\n",
    "                n_sim = f.attrs[\"n_trajectories\"]\n",
    "                self.n_sim_per_file.append(n_sim)\n",
    "                if self.max_n_sim <= sum(self.n_sim_per_file): break\n",
    "        \n",
    "        self.metadata = {\n",
    "            'nu': self.nu,\n",
    "\n",
    "            'Lx': self.Lx,\n",
    "            'Ly': self.Ly,\n",
    "            'dx': self.dx,\n",
    "            'dy': self.dy,\n",
    "            'Nx': self.Nx, \n",
    "            'Ny': self.Ny,\n",
    "            'x': self.x,\n",
    "            'y': self.y,\n",
    "\n",
    "            'dt': self.dt,\n",
    "            'Nt': self.Nt,\n",
    "            'T': self.T,\n",
    "            't0': self.t0,\n",
    "            't': self.t,\n",
    "        }\n",
    "        if self.verbose:\n",
    "            print(f\"Loaded dataset metadata: {self.metadata.keys()}\")\n",
    "        return self.metadata\n",
    "    \n",
    "    def _preprocess_data(self, data):\n",
    "        data, target = data\n",
    "\n",
    "        x = torch.cat([data, target], dim=0)\n",
    "\n",
    "        return x\n",
    "    def _postprocess_data(self, data):\n",
    "        data, target = data[0], data[1]\n",
    "\n",
    "        # data = data.flatten(0, 1) # data.reshape(data.shape[0] * data.shape[1], *data.shape[2:]) # (n_in * channels, *datashape)\n",
    "        # target = target.flatten(0, 1) # target.reshape(target.shape[0] * target.shape[1], *target.shape[2:]) # (n_out * channels, *datashape)\n",
    "\n",
    "        return data, target\n",
    "\n",
    "class CustomDataModule:\n",
    "    def __init__(self, \n",
    "                 batch_size:int=32, \n",
    "                 Ndata_train:int=500, \n",
    "                 Ndata_val:int=50, \n",
    "                 Ndata_test:int=100, \n",
    "                 num_workers:int=0,\n",
    "                 transform:Optional[callable]=None,\n",
    "                 normalization:Optional[callable]=None,\n",
    "                 *args, **kwargs\n",
    "                 ):\n",
    "        self.batch_size = batch_size\n",
    "        self.Ndata_train = Ndata_train\n",
    "        self.Ndata_val = Ndata_val\n",
    "        self.Ndata_test = Ndata_test\n",
    "\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.transform = transform\n",
    "        self.normalization = normalization\n",
    "\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    \n",
    "    def setup(self, stage:str=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = HIT2dDataset(split_name='train', max_n_sim=self.Ndata_train, transform=self.transform, normalization=self.normalization, *self.args, **self.kwargs, )\n",
    "            self.val_dataset = HIT2dDataset(split_name='valid', max_n_sim=self.Ndata_val, transform=None, normalization=self.normalization, *self.args, **self.kwargs, )\n",
    "            \n",
    "            print(f'Train dataset: {len(self.train_dataset)}')\n",
    "            print(f'Val dataset: {len(self.val_dataset)}')\n",
    "\n",
    "        if stage == 'train':\n",
    "            self.train_dataset = HIT2dDataset(split_name='train', max_n_sim=self.Ndata_train, transform=self.transform, normalization=self.normalization, *self.args, **self.kwargs, )\n",
    "            print(f'Train dataset: {len(self.train_dataset)}')\n",
    "            \n",
    "        if stage == 'valid':\n",
    "            self.val_dataset = HIT2dDataset(split_name='valid', max_n_sim=self.Ndata_val, transform=None, normalization=self.normalization, *self.args, **self.kwargs, )\n",
    "            print(f'Val dataset: {len(self.val_dataset)}')\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = HIT2dDataset(split_name='valid', max_n_sim=self.Ndata_test, transform=None, normalization=self.normalization, *self.args, **self.kwargs, )\n",
    "            print(f'Test dataset: {len(self.test_dataset)}')\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.train_dataset is None: \n",
    "            # assert False, \"Error: train_dataset is None. run setup('fit') or setup('train') first.\"\n",
    "            return None\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, \n",
    "                                           batch_size = self.batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=self.num_workers, \n",
    "                                           pin_memory=True,\n",
    "                                           drop_last=True, \n",
    "                                           )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        if self.val_dataset is None: \n",
    "            # assert False, \"Error: val_dataset is None. run setup('fit') or setup('valid') first.\"\n",
    "            return None\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, \n",
    "                                           batch_size = self.batch_size, \n",
    "                                           shuffle=False, \n",
    "                                           num_workers=self.num_workers,\n",
    "                                           pin_memory=True,\n",
    "                                           drop_last=True,\n",
    "                                           )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataset is None: \n",
    "            # assert False, \"Error: test_dataset is None. run setup('test') first.\"\n",
    "            return None\n",
    "        test_batch_size = self.batch_size # self.test_dataset.n_windows_per_sim # // 8\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, \n",
    "                                           batch_size = test_batch_size, \n",
    "                                           shuffle=False, \n",
    "                                           num_workers=self.num_workers, \n",
    "                                           pin_memory=True,\n",
    "                                           drop_last=True,\n",
    "                                           )\n",
    "\n",
    "\n",
    "class RandomShift(nn.Module):\n",
    "    def __init__(self, \n",
    "        shifts:Union[float, Sequence[float]]=(0.5, 0.5), \n",
    "        dims:Union[int, Sequence[int]]=(-2, -1),\n",
    "        *args, **kwargs\n",
    "        ):\n",
    "        super(RandomShift, self).__init__()\n",
    "        self.shifts = shifts\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, \n",
    "        x, \n",
    "        shifts:Union[float, Sequence[float]]=None, \n",
    "        dims:Union[int, Sequence[int]]=None, \n",
    "        ):\n",
    "        if shifts is None: shifts = self.shifts\n",
    "        if dims is None: dims = self.dims\n",
    "        \n",
    "        return self.RandomShift(x, shifts=self.shifts, dims=self.dims)\n",
    "    \n",
    "    def RandomShift(self,\n",
    "        x, \n",
    "        shifts:Union[float, Sequence[float]]=(0.5, 0.5), \n",
    "        dims:Union[int, Sequence[int]]=(-2, -1)\n",
    "        ):\n",
    "        '''\n",
    "        Random shift the input tensor along the spatial dimensions. \n",
    "        Parameters:\n",
    "        - x (torch.tensor): input tensor, shape \n",
    "        - shifts (float or sequence of float): maximum shift fraction along each dimension. \n",
    "            If float, the same shift fraction is applied to all dimensions.\n",
    "            If sequence of float, the length must be equal to the number of spatial dimensions.\n",
    "            The shift fraction is relative to the size of the dimension.\n",
    "        - dims (int or sequence of int): dimensions to apply the shift. \n",
    "            If int, the same dimension is applied to all spatial dimensions.\n",
    "            If sequence of int, the length must be equal to the number of spatial dimensions.\n",
    "        \n",
    "        Returns:\n",
    "        - x (torch.tensor): shifted tensor, shape \n",
    "        '''\n",
    "        if isinstance(shifts, float): shifts = [shifts] * len(dims)\n",
    "        assert len(shifts) == len(dims), \"Length of shifts and dims must be equal to the number of spatial dimensions.\"\n",
    "\n",
    "        shifts = [int(np.random.uniform(-s, s) * x.shape[d]) for s, d in zip(shifts, dims)]\n",
    "        x = torch.roll(x, shifts=shifts, dims=dims)\n",
    "        return x\n",
    "\n",
    "class Reflection(nn.Module):\n",
    "    def __init__(self, \n",
    "        p: float=0.5,\n",
    "        dims:Union[int, Sequence[int]]=(-2, -1),\n",
    "        *args, **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, \n",
    "        x, \n",
    "        p = None, \n",
    "        dims:Union[int, Sequence[int]]=None, \n",
    "        ):\n",
    "        if dims is None: dims = self.dims\n",
    "        if p is None: p = self.p\n",
    "        \n",
    "        return self.Reflection(x, p=p, dims=self.dims)\n",
    "    \n",
    "    def Reflection(self,\n",
    "        x, \n",
    "        p: float=0.5, \n",
    "        dims:Union[int, Sequence[int]]=(-2, -1)\n",
    "        ):\n",
    "        '''\n",
    "        Reflection the input tensor along the spatial dimensions. \n",
    "        Parameters:\n",
    "        - x (torch.tensor): input tensor, shape \n",
    "        - dims (int or sequence of int): dimensions to apply the Reflection. \n",
    "            If int, the same dimension is applied to all spatial dimensions.\n",
    "            If sequence of int, the length must be equal to the number of spatial dimensions.\n",
    "        \n",
    "        Returns:\n",
    "        - x (torch.tensor): Reflection tensor, shape \n",
    "        '''\n",
    "        if isinstance(dims, int): dims = [dims]\n",
    "        \n",
    "        for d in dims:\n",
    "            if np.random.rand() < p: x = torch.flip(x, dims=(d,))\n",
    "        return x\n",
    "\n",
    "class Reverse(nn.Module):\n",
    "    def __init__(self, \n",
    "        p: float=0.5,\n",
    "        *args, **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, \n",
    "        x, \n",
    "        p = None, \n",
    "        ):\n",
    "        if p is None: p = self.p\n",
    "        \n",
    "        return self.Reverse(x, p=p)\n",
    "    \n",
    "    def Reverse(self,\n",
    "        x, \n",
    "        p: float=0.5, \n",
    "        ):\n",
    "        '''\n",
    "        Reverse the input tensor along the time dimension. \n",
    "        Parameters:\n",
    "        - x (torch.tensor): input tensor, shape \n",
    "        - dim (int): dimension to apply the Reverse. \n",
    "        \n",
    "        Returns:\n",
    "        - x (torch.tensor): Reverse tensor, shape \n",
    "        '''\n",
    "        if np.random.rand() < p: x = -x \n",
    "        return x\n",
    "    \n",
    "class CustomTransform(nn.Module):\n",
    "    def __init__(self, \n",
    "                *args, **kwargs, \n",
    "                ):\n",
    "        super(CustomTransform, self).__init__()\n",
    "        # self.file_dir = file_dir\n",
    "        # self.Ndata = Ndata\n",
    "        self.kwargs = kwargs \n",
    "        self.build_transform()\n",
    "\n",
    "    def build_transform(self):\n",
    "        self.transforms = nn.Sequential(\n",
    "            RandomShift(shifts=0.5, dims=(-2, -1)),\n",
    "            Reflection(p=0.5, dims=(-2, -1)),\n",
    "            Reverse(p=0.5),\n",
    "        )\n",
    "        return self.transforms\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transform(x)\n",
    "        return x\n",
    "    \n",
    "    def transform(self, x):\n",
    "        for t in self.transforms:\n",
    "            x = t(x)\n",
    "        return x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        for t in self.transforms[::-1]:\n",
    "            x = t.inverse_transform(x)\n",
    "        return x\n",
    "\n",
    "class Standardize(nn.Module):\n",
    "    def __init__(self, \n",
    "        mean:Optional[float]=None, std:Optional[float]=None, \n",
    "        normalization_path:Optional[str]=None,\n",
    "\n",
    "        base_path: Optional[str] = None,\n",
    "        dataset_name: Optional[str] = None,\n",
    "        split_name: Optional[str] = None,\n",
    "        *args, **kwargs, \n",
    "        ):\n",
    "        super(Standardize, self).__init__()\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        ## build_transform\n",
    "        if mean is not None and std is not None: \n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "        elif normalization_path is not None:\n",
    "            self.load_stats(normalization_path)\n",
    "        elif dataset_name is not None: \n",
    "            path = os.path.join(base_path, split_name, dataset_name)\n",
    "            self.calc_stats_from_dataset(path)\n",
    "        else:\n",
    "            assert False, \"Error: must provide either (mean, std), normalization_path, or (base_path, dataset_name, split_name) to calculate mean and std.\"\n",
    "    \n",
    "    def load_stats(self, path=None):\n",
    "        assert os.path.exists(path), f\"Error: normalization path {path} does not exist.\"\n",
    "        with open(path, \"r\") as f:\n",
    "            stats = yaml.safe_load(f)['statistics']\n",
    "\n",
    "            self.mean = stats['mean']\n",
    "            self.std = stats['std']\n",
    "        return self.mean, self.std\n",
    "    \n",
    "    def calc_stats_from_dataset(self, path=None):\n",
    "        dataset = HIT2dDataset(path=path, load_data=True, *self.args, **self.kwargs).load_data()\n",
    "        self.mean = dataset.data.mean().item()\n",
    "        self.std = dataset.data.std().item()\n",
    "        return self.mean, self.std\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.normalize(x, params=None)\n",
    "    \n",
    "    def normalize(self, inpt: Any, params: Dict[str, Any]):\n",
    "        return (inpt - self.mean) / self.std\n",
    "\n",
    "    def denormalize(self, inpt: Any, params: Dict[str, Any]):\n",
    "        return inpt * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PltLogger:\n",
    "    def __init__(self, \n",
    "                 fname: Optional[str]=None,\n",
    "                 save_dir:Union[str, Path]='./', \n",
    "                 prefix:str='',\n",
    "                 format:str='ascii', \n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        self.fname = fname\n",
    "        self.save_dir = save_dir \n",
    "        self.prefix=prefix \n",
    "        self.format = format\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def log(self, \n",
    "            name:str, \n",
    "            value: Union[Tensor, int, float], \n",
    "            ):\n",
    "        pass\n",
    "\n",
    "    def log_dict(self, \n",
    "                 dictionary: Mapping[str, Union[Tensor, int, float]], \n",
    "                 ):\n",
    "        # ì´ê²ƒë„ ë§Œë“¤ê¸°! log ì¶œë ¥í• ë•Œ ì¢‹ìž–ì•„ \n",
    "        pass\n",
    "    \n",
    "    def save_2d_dict(self, \n",
    "                dictionary: Mapping[str, Union[Tensor, np.ndarray]],  \n",
    "                time_step:Union[int, str]=0,\n",
    "                isheader:bool=True,\n",
    "                title:str=None,\n",
    "                prefix:str=None, \n",
    "                save_dir:Union[str, Path]=None,\n",
    "                fname:str=None, \n",
    "                format:str=None,\n",
    "                fmt:str=\"%.6f\",\n",
    "                domain_mesh:list=None,\n",
    "                domain_size:list=None, \n",
    "                ):\n",
    "        pass\n",
    "        \n",
    "    def save_2d(self, \n",
    "                name:Union[list, str], \n",
    "                value: Union[torch.Tensor, np.ndarray], \n",
    "                time_step:Union[int, str]=0,\n",
    "                isheader:bool=True,\n",
    "                title:str=None,\n",
    "                prefix:str=None, \n",
    "                save_dir:Union[str, Path]=None,\n",
    "                fname:str=None, \n",
    "                format:str=None,\n",
    "                fmt:str=\"%.6f\",\n",
    "                domain_mesh:list=None,\n",
    "                domain_size:list=None, \n",
    "                ):\n",
    "        if not format: format = self.format\n",
    "        if not save_dir: save_dir = self.save_dir\n",
    "        if not fname: fname = self.fname\n",
    "        if not prefix: prefix = self.prefix\n",
    "        if isinstance(value, torch.Tensor): value = value.detach().cpu().numpy()\n",
    "        if isinstance(name, str): name = [name]\n",
    "        for i in range(value.shape[0] - len(name)): name.append(f'var{i}')\n",
    "        \n",
    "        if value.ndim == 2: value.unsqueeze(0)\n",
    "        if value.ndim != 3: \n",
    "            raise ValueError(f'Dimension Error {value.ndim-1}!=2')\n",
    "            \n",
    "        channels, nx, ny = value.shape \n",
    "        if domain_mesh is None:\n",
    "            Lx, Ly = domain_size if domain_size else [nx+1, ny+1] # Domain size\n",
    "            x = np.linspace(0., Lx, nx, endpoint=True)\n",
    "            y = np.linspace(0., Ly, ny, endpoint=True)\n",
    "            domain_mesh = np.meshgrid(x, y, indexing=\"ij\")\n",
    "        X, Y = domain_mesh\n",
    "\n",
    "        if format == 'ascii':\n",
    "            if isheader:\n",
    "                with open(save_dir + fname + prefix + '.plt', 'w') as f:\n",
    "                    if title: f.write(f'TITLE = \"{title}\"\\n')\n",
    "                    f.write(\"VARIABLES = x, y, \" + \", \".join([f'\"{na}\"' for na in name]) + \"\\n\")\n",
    "            with open(save_dir + fname + prefix + '.plt', 'a') as f:\n",
    "                f.write(f\"ZONE T=\\\"T={time_step}\\\", I={nx}, J={ny}, DATAPACKING=POINT\\n\")\n",
    "                data = np.column_stack([X.ravel(), Y.ravel(), value.reshape(channels, nx * ny).T])\n",
    "                np.savetxt(f, data, fmt=fmt)\n",
    "\n",
    "        elif format == 'binary':\n",
    "            pass\n",
    "\n",
    "    def save_1d_dict(self, \n",
    "                dictionary: Mapping[str, Union[Tensor, np.ndarray]], \n",
    "                time_step:Union[int, str]=0,\n",
    "                iszone:bool=True,\n",
    "                isheader:bool=True,\n",
    "                title:str=None,\n",
    "                prefix:str=None, \n",
    "                save_dir:Union[str, Path]=None,\n",
    "                fname:str=None, \n",
    "                format:str=None,\n",
    "                fmt:str=\"%.6f\",\n",
    "                ):\n",
    "        if not format: format = self.format\n",
    "        if not save_dir: save_dir = self.save_dir\n",
    "        if not fname: fname = self.fname\n",
    "        if not prefix: prefix = self.prefix\n",
    "        dictionary = {key: value.detach().cpu().numpy() if isinstance(value, torch.Tensor) else value for key, value in dictionary.items()}\n",
    "\n",
    "        if format == 'ascii':\n",
    "            if isheader:\n",
    "                with open(save_dir + fname + prefix + '.plt', 'w') as f:\n",
    "                    if title: f.write(f'TITLE = \"{title}\"\\n')\n",
    "                    f.write(\"VARIABLES = \" + \", \".join([f'\"{na}\"' for na in dictionary.keys()]) + \"\\n\")\n",
    "            with open(save_dir + fname + prefix + '.plt', 'a') as f:\n",
    "                if iszone: f.write(f\"ZONE T=\\\"T={time_step}\\\", I={len(next(iter(dictionary.values())))}, DATAPACKING=POINT\\n\")\n",
    "                data = np.column_stack(dictionary.values())\n",
    "                np.savetxt(f, data, fmt=fmt)\n",
    "        elif format == 'binary':\n",
    "            pass\n",
    "\n",
    "    def save_1d(self, \n",
    "                name:Union[list, str], \n",
    "                value: Union[torch.Tensor, np.ndarray], \n",
    "                time_step:Union[int, str]=0,\n",
    "                isheader:bool=True,\n",
    "                title:str=None,\n",
    "                prefix:str=None, \n",
    "                save_dir:Union[str, Path]=None,\n",
    "                fname:str=None, \n",
    "                format:str=None,\n",
    "                fmt:str=\"%.6f\",\n",
    "                ):\n",
    "        if isinstance(name, str): name = [name]\n",
    "        for i in range(value.shape[0] - len(name)): name.append(f'var{i}')\n",
    "        dictionary = {na: val for na, val in zip(name, value)}\n",
    "        self.save_1d_dict(dictionary, \n",
    "                     time_step=time_step, \n",
    "                     isheader=isheader, \n",
    "                     title=title, \n",
    "                     prefix=prefix, \n",
    "                     save_dir=save_dir, \n",
    "                     fname=fname, \n",
    "                     format=format, \n",
    "                     fmt=fmt\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_relative_error(output:torch.Tensor, target:torch.Tensor, dim:int=2) -> torch.Tensor: \n",
    "    flag_dim = False\n",
    "    if not dim: \n",
    "        dim = output.ndim - 1\n",
    "        output = output.unsqueeze(0) \n",
    "        target = target.unsqueeze(0) \n",
    "        flag_dim = True\n",
    "    batch_shape, channels, shape = output.shape[:-(dim+1)], output.shape[-(dim+1)], output.shape[-dim:] \n",
    "    fft_dim = tuple(range(-dim, 0))\n",
    "\n",
    "    ## Compute the wavenumbers\n",
    "    k = [torch.fft.fftfreq(n, 1/n, device=output.device) for n in shape] \n",
    "    k = torch.meshgrid(k, indexing='ij')\n",
    "    k = torch.stack(k)\n",
    "    k = torch.sqrt(torch.sum(k**2, axis=0)) \n",
    "    Nk = min(shape)//2\n",
    "\n",
    "    ## Compute spectral relative error\n",
    "    output_fft = torch.fft.fftn(output, dim=fft_dim, norm='forward')\n",
    "    target_fft = torch.fft.fftn(target, dim=fft_dim, norm='forward')\n",
    "    E_k = torch.sqrt((output_fft - target_fft)**2 / (target_fft**2 + 1e-8)).abs()\n",
    "\n",
    "    ## Compute the spectrum\n",
    "    spectrum = torch.zeros((*batch_shape, Nk + 1), device=output.device)\n",
    "    for i in range(Nk + 1): spectrum[..., i] = (E_k[..., k == i]).mean(dim=fft_dim)\n",
    "    \n",
    "    spectrum = spectrum.mean(dim=0)\n",
    "    if flag_dim: spectrum = spectrum.squeeze(0)\n",
    "    return spectrum\n",
    "\n",
    "def plot_histogram(tensor, title, xlabel, bins=50):\n",
    "    bins = min(bins, len(tensor.flatten()))\n",
    "    plt.hist(torch.abs(tensor).detach().cpu().numpy().flatten(), bins=bins, density=True, log=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "\n",
    "def compute_spectrum(tensor, dim=2):\n",
    "    batch_shape, channels, shape = tensor.shape[:-(dim+1)], tensor.shape[-(dim+1)], tensor.shape[-dim:]\n",
    "    fft_dim = tuple(range(-dim, 0))\n",
    "\n",
    "    k = [torch.fft.fftfreq(n, 1/n, device=tensor.device) for n in shape]\n",
    "    k = torch.meshgrid(k, indexing='ij')\n",
    "    k = torch.stack(k)\n",
    "    k = torch.sqrt(torch.sum(k**2, axis=0)) \n",
    "    Nk = min(shape) // 2\n",
    "\n",
    "    spectrum = torch.zeros((*batch_shape, Nk + 1), device=tensor.device)\n",
    "    tensor = torch.fft.fftshift(tensor, dim=fft_dim)\n",
    "    E_k = torch.abs(tensor)# .view(*batch_shape, -1)\n",
    "    for i in range(Nk + 1): spectrum[..., i] = (E_k[..., k == i]).mean(dim=fft_dim)\n",
    "\n",
    "    spectrum = spectrum.mean(dim=0)\n",
    "    return spectrum\n",
    "\n",
    "def plot_param_trends(log_history, param_type='params', \n",
    "                      title:str=None,\n",
    "                fname:str=None, \n",
    "                xlabel:str='epoch',\n",
    "                name:str='parameter trends',\n",
    "                ):\n",
    "    \"\"\"\n",
    "    log_history: list of logs collected over iterations\n",
    "    param_type: 'params' or 'grad'\n",
    "    \"\"\"\n",
    "    param_names = log_history[0][param_type].keys()\n",
    "    x = np.arange(len(log_history))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for param in param_names:\n",
    "        means = np.array([log[param_type][param]['mean'] for log in log_history])\n",
    "        stds = np.array([log[param_type][param]['std'] for log in log_history])\n",
    "\n",
    "        ax.plot(x, means, label=f'{param_type}_{param}')\n",
    "        ax.fill_between(x, means - stds, means + stds, alpha=0.3)\n",
    "    \n",
    "    ax.set_xlabel(xlabel if xlabel else 'epoch')\n",
    "    ax.set_ylabel('parameter value')\n",
    "    ax.set_title(title if title else f'{name}')\n",
    "\n",
    "    # Log the figure to wandb\n",
    "    fig.savefig(f'./result/{fname}.png')\n",
    "    # wandb.log({fname if title else f'{name}': wandb.Image(f'./result/{fname}.png')})\n",
    "    plt.close(fig)\n",
    "\n",
    "log_history = []\n",
    "def hook(model: nn.Module):\n",
    "    if not hasattr(model, 'iteration'): model.iteration = 1\n",
    "    else: model.iteration += 1\n",
    "    log = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        weight = param.clone()\n",
    "        # plot_histogram(torch.abs(weight), f'{name} Weight Distribution {weight.shape}', 'Weight')\n",
    "        # plt.savefig('./result/'+f'{name}_WeightDistribution_{model.iteration}.png')\n",
    "        # plt.close()\n",
    "        wandb.log({f'{name}_WeightDistribution': wandb.Histogram(torch.abs(weight).detach().cpu().numpy())})\n",
    "        # log['weight'] = {f'{name}': {'mean': weight.mean().item(), 'std': weight.std().item()}}\n",
    "\n",
    "        if weight.is_complex(): \n",
    "            weight = rfft2fft(weight)\n",
    "            weight_spectrum = compute_spectrum(weight)\n",
    "            # plot_spectrum(weight_spectrum, title=f'{name} Weight Spectrum')\n",
    "            # plt.savefig('./result/'+f'{name}_WeightSpectrum_{model.iteration}.png')\n",
    "            # plt.close()\n",
    "            # wandb.log({\n",
    "            #     f'{name}_WeightSpectrum': wandb.plot.line(weight_spectrum.detach().cpu().numpy(), title=f'{name} Weight Spectrum'), \n",
    "            # })\n",
    "            log_1d_dict_wandb(\n",
    "                    dictionary={'spectrum': weight_spectrum.detach().cpu().numpy()}, \n",
    "                    title=f'{name}_WeightSpectrum',\n",
    "                    fname=f'{name}_WeightSpectrum', \n",
    "                    xlabel='iteration', ylabel='value', \n",
    "                    xscale='log', yscale='log',\n",
    "                    )\n",
    "\n",
    "        if param.grad is not None: \n",
    "            grad = param.grad.clone()\n",
    "            plot_histogram(torch.abs(grad), f'{name} Gradient Distribution {grad.shape}', 'Gradient')\n",
    "            plt.savefig('./result/'+f'{name}_GradientDistribution.png')\n",
    "            plt.close()\n",
    "            wandb.log({f'{name}_GradientDistribution': wandb.Histogram(torch.abs(grad).detach().cpu().numpy())})\n",
    "            \n",
    "\n",
    "            if grad.is_complex(): \n",
    "                grad = rfft2fft(grad)\n",
    "                grad_spectrum = compute_spectrum(grad)\n",
    "                # plot_spectrum(grad_spectrum, title=f'{name} Gradient Spectrum')\n",
    "                # plt.savefig('./result/'+f'{name}_GradientSpectrum_{model.iteration}.png')\n",
    "                # plt.close()\n",
    "                # wandb.log({\n",
    "                # f'{name}_GradientSpectrum': wandb.plot.line(grad_spectrum.detach().cpu().numpy(), title=f'{name} Weight Spectrum'), \n",
    "                # })\n",
    "                log_1d_dict_wandb(\n",
    "                    dictionary={'spectrum': grad_spectrum}, \n",
    "                    title=f'{name}_GradientSpectrum',\n",
    "                    fname=f'{name}_GradientSpectrum', \n",
    "                    xlabel='iteration', ylabel='value', \n",
    "                    xscale='log', yscale='log',\n",
    "                    )\n",
    "            # log['grad'] = {f'{name}': {'mean': grad.abs().mean().item(), 'std': grad.abs().std().item()}} if grad.is_complex() else {'mean': grad.mean().item(), 'std': grad.std().item()}\n",
    "\n",
    "            grad_weight_ratio = torch.abs(grad) / (torch.abs(weight) + 1e-8)\n",
    "            plot_histogram(grad_weight_ratio, f'{name} Gradient-Weight ratio Distribution {grad_weight_ratio.shape}', 'Gradient-Weight ratio' )\n",
    "            plt.savefig('./result/'+f'{name}_RatioDistribution_{model.iteration}.png')\n",
    "            plt.close()\n",
    "            wandb.log({f'{name}_RatioDistribution': wandb.Histogram(torch.abs(grad_weight_ratio).detach().cpu().numpy())})\n",
    "\n",
    "            if grad.is_complex(): \n",
    "                ratio_spectrum = compute_spectrum(grad_weight_ratio)\n",
    "                # plot_spectrum(ratio_spectrum, title=f'{name} Gradient-Weight ratio Spectrum')\n",
    "                # plt.savefig('./result/'+f'{name}_RatioSpectrum_{model.iteration}.png')\n",
    "                # plt.close()\n",
    "                # wandb.log({\n",
    "                # f'{name}_RatioSpectrum': wandb.plot.line(ratio_spectrum.detach().cpu().numpy(), title=f'{name} Weight Spectrum'), \n",
    "                # })\n",
    "                log_1d_dict_wandb(\n",
    "                    dictionary={'spectrum': ratio_spectrum}, \n",
    "                    title=f'{name} Gradient-Weight ratio Spectrum',\n",
    "                    fname=f'{name} Gradient-Weight ratio Spectrum', \n",
    "                    xlabel='iteration', ylabel='value', \n",
    "                    xscale='log', yscale='log',\n",
    "                    )\n",
    "                \n",
    "        # wandb.log(log)\n",
    "        \n",
    "        # log_history.append(log)\n",
    "        # if len(log_history) > 1:\n",
    "        #     plot_param_trends(log_history, param_type='weight')\n",
    "        #     plot_param_trends(log_history, param_type='grad')\n",
    "def log_1d_colormap_wandb( \n",
    "                name:Union[list, str], \n",
    "                value: Union[torch.Tensor, np.ndarray], \n",
    "                title:str=None,\n",
    "                fname:str=None, \n",
    "                xlabel:str='epoch',\n",
    "                vmin:float=None, \n",
    "                vmax:float=None,\n",
    "                ): \n",
    "    value = value.detach().cpu().numpy() if isinstance(value, torch.Tensor) else value\n",
    "    if vmin is None: vmin = max(value.min(), 1e-8) \n",
    "    if vmax is None: vmax = value.max()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    c = ax.imshow(\n",
    "        value.T, \n",
    "        aspect='auto', \n",
    "        cmap='coolwarm', \n",
    "        # extent=[1, value.shape[0], 0.5, value.shape[1] + 0.5],\n",
    "        origin='lower',\n",
    "        norm = matplotlib.colors.LogNorm(vmin=vmin, vmax=vmax),\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel(xlabel if xlabel else 'epoch')\n",
    "    ax.set_ylabel('wavenumber')\n",
    "    ax.set_title(title if title else f'{name} Colormap')\n",
    "    fig.colorbar(c, ax=ax)\n",
    "\n",
    "    # Log the figure to wandb\n",
    "    fig.savefig(f'./result/{fname}.png')\n",
    "    wandb.log({fname if title else f'{name} Colormap': wandb.Image(f'./result/{fname}.png')})\n",
    "    plt.close(fig)\n",
    "\n",
    "def log_1d_dict_wandb(\n",
    "                dictionary: Mapping[str, Union[Tensor, np.ndarray]], \n",
    "                title:str=None,\n",
    "                fname:str=None, \n",
    "                xlabel:str='epoch', ylabel:str='value', \n",
    "                xscale:str='linear', yscale:str='linear',\n",
    "                ): \n",
    "    if isinstance(dictionary, dict): dictionary = {k: v.detach().cpu().numpy() if isinstance(v, torch.Tensor) else v for k, v in dictionary.items()}\n",
    "    fig, ax = plt.subplots()\n",
    "    for key, value in dictionary.items(): ax.plot(value, label=key)\n",
    "    ax.set(xscale=xscale, yscale=yscale, \n",
    "           xlabel=xlabel, ylabel=ylabel, \n",
    "           title=title if title else f'{fname}',\n",
    "           )\n",
    "    if xlabel == 'log' and ylabel == 'log': \n",
    "        # ax.set_xticks(xticks = [2 ** (np.round(np.log2(Nk)).astype(np.int32) - i) for i in range(min(np.round(np.log2(Nk)).astype(np.int32), 7))],)\n",
    "        ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    if len(dictionary) > 1: ax.legend()\n",
    "\n",
    "    # Log the figure to wandb\n",
    "    fig.savefig(f'./result/{fname}.png')\n",
    "    wandb.log({fname if title else f'{fname}': wandb.Image(f'./result/{fname}.png')})\n",
    "    plt.close(fig)\n",
    "\n",
    "def rfft2fft(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a complex-valued rfft tensor to a complex-valued fft tensor.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): A complex-valued rfft tensor of shape (batch, channel, *data_shape).\n",
    "        dim (int): Number of spatial dimensions.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A complex-valued fft tensor with the same shape as x.\n",
    "    \"\"\"\n",
    "    if (x.shape[-1] - 1) * 2 % 2 == 0: \n",
    "        # ì§ìˆ˜ì¸ ê²½ìš°: DC ì„±ë¶„ë§Œ ê³ ìœ í•˜ë¯€ë¡œ ì œì™¸í•˜ê³  ë°˜ì „ í›„ ì¼¤ë ˆ ì·¨í•¨\n",
    "        conj_part = torch.conj(x[..., 1:-1].flip(-1))\n",
    "    else: \n",
    "        # í™€ìˆ˜ì¸ ê²½ìš°: DCì™€ Nyquist ì„±ë¶„ì€ ê³ ìœ í•˜ë¯€ë¡œ rfft_tensorì˜ [1:-1] êµ¬ê°„ë§Œ ë°˜ì „ í›„ ì¼¤ë ˆ ì·¨í•¨ \n",
    "        conj_part = torch.conj(x[..., 1:].flip(-1)) # x = torch.cat([x, torch.flip(x[..., 1:-1], dims=fft_dim)], dim=-1)\n",
    "    return torch.cat([x, conj_part], dim=-1)\n",
    "\n",
    "def fft2rfft(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a complex-valued fft tensor to a complex-valued rfft tensor.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): A complex-valued fft tensor of shape (batch, channel, *data_shape).\n",
    "        dim (int): Number of spatial dimensions.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A complex-valued rfft tensor with the same shape as x.\n",
    "    \"\"\"\n",
    "    rfft_last_dim = x.shape[-1] // 2 + 1\n",
    "\n",
    "    return x[..., :rfft_last_dim]\n",
    "\n",
    "def enforce_conjugate_symmetry(x: torch.Tensor, isrfft:bool=False, isshift:bool=False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Enforces conjugate symmetry on a 2D complex tensor in Fourier space.\n",
    "    The input tensor x is expected to have shape:\n",
    "        (batch_size, channels, n_x, n_y)\n",
    "        \n",
    "    For each element, this function sets:\n",
    "        x[..., i, j] = 0.5 * (x[..., i, j] + conj(x[..., i_sym, j_sym]))\n",
    "    where (i_sym, j_sym) are the symmetric indices computed as:\n",
    "        i_sym = (-i) % n_x,  j_sym = (-j) % n_y\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input complex-valued tensor with shape (B, C, n_x, n_y)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The tensor with enforced conjugate symmetry.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, torch.Tensor): x = x.to_tensor()\n",
    "    \n",
    "    if isrfft: x = rfft2fft(x)\n",
    "    # if isshift: x = torch.fft.fftshift(x, dim=(-2, -1))\n",
    "    # Get spatial dimensions.\n",
    "    n_x, n_y = x.shape[-2:]\n",
    "    # Create index grids for the spatial dimensions.\n",
    "    ii, jj = torch.meshgrid(torch.arange(n_x, device=x.device),\n",
    "                             torch.arange(n_y, device=x.device),\n",
    "                             indexing='ij')\n",
    "    # Compute symmetric indices.\n",
    "    ii_sym = (-ii) % n_x\n",
    "    jj_sym = (-jj) % n_y\n",
    "\n",
    "    # Compute the symmetric average for every spatial element\n",
    "    # This operation broadcasts over the batch and channel dimensions.\n",
    "    x = 0.5 * (x[..., ii, jj] + torch.conj(x[..., ii_sym, jj_sym]))\n",
    "    # if isshift: x = torch.fft.ifftshift(x, dim=(-2, -1))\n",
    "    if isrfft: x = fft2rfft(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def check_conjugate_symmetry(x: torch.Tensor, isrfft:bool=False, isshift:bool=False) -> float:\n",
    "    \"\"\"\n",
    "    Checks whether a 2D complex tensor in Fourier space satisfies\n",
    "    the conjugate symmetry condition for real-valued physical data:\n",
    "        x(-k) == conj(x(k))\n",
    "    The tensor is expected to have shape:\n",
    "        (batch_size, channels, n_x, n_y)\n",
    "        \n",
    "    Args:\n",
    "        x (torch.Tensor): Complex-valued tensor with shape (B, C, n_x, n_y)\n",
    "        tol (float): Tolerance used for comparing floating-point values.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the tensor satisfies conjugate symmetry within the given tolerance,\n",
    "              otherwise False.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, torch.Tensor): x = x.to_tensor()\n",
    "    # if isshift: x = torch.fft.fftshift(x, dim=(-2, -1))\n",
    "    if isrfft: x = rfft2fft(x)\n",
    "    # Get spatial dimensions.\n",
    "    n_x, n_y = x.shape[-2:]\n",
    "    # Create index grids.\n",
    "    ii, jj = torch.meshgrid(torch.arange(n_x, device=x.device),\n",
    "                             torch.arange(n_y, device=x.device),\n",
    "                             indexing='ij')\n",
    "    # Compute symmetric indices.\n",
    "    ii_sym = (-ii) % n_x\n",
    "    jj_sym = (-jj) % n_y\n",
    "\n",
    "    # Compute the absolute difference between each element and its conjugate symmetric\n",
    "    diff = torch.abs(x[..., ii, jj] - torch.conj(x[..., ii_sym, jj_sym]))\n",
    "    # Check if all differences are within the tolerance.\n",
    "    # return torch.all(diff < tol).item()\n",
    "    return diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "NORM_LAYERS = {\n",
    "    'none': lambda out_channels: nn.Identity(),\n",
    "    'batchnorm': lambda out_channels: nn.BatchNorm2d(out_channels),\n",
    "    'instancenorm': lambda out_channels: nn.InstanceNorm2d(out_channels),\n",
    "}\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    'LeakyReLU': lambda: nn.LeakyReLU(0.2),\n",
    "    'GELU': lambda: nn.GELU(),\n",
    "    'ReLU': lambda: nn.ReLU(),\n",
    "}\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv â†’ Act) Ã— 2, padding=1 to keep HÃ—W.\"\"\"\n",
    "    def __init__(self, in_channel:int, out_channel:int, \n",
    "                 activation=nn.ReLU(),\n",
    "                 norm=nn.Identity(),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(activation, str):\n",
    "            activation = ACTIVATIONS[activation]()\n",
    "        if isinstance(norm, str):\n",
    "            norm = NORM_LAYERS[norm](out_channel)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, padding_mode='circular'),\n",
    "            norm, \n",
    "            activation,\n",
    "            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, padding_mode='circular'),\n",
    "            norm,\n",
    "            activation\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Pool â†’ DoubleConv.\"\"\"\n",
    "    def __init__(self, in_channel:int, out_channel:int, \n",
    "                 activation=nn.ReLU(), \n",
    "                 pooling:callable=nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "                 norm=nn.Identity(),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if isinstance(activation, str):\n",
    "            activation = ACTIVATIONS[activation]()\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2) # pooling\n",
    "        self.conv = DoubleConv(in_channel, out_channel, activation=activation, norm=norm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)  # [B, in_channel, H/2, W/2]\n",
    "        x = self.conv(x)  # [B, out_channel, H/2, W/2]\n",
    "        return x\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"TransposedConv(in_channelâ†’out_channel) â†’ concat â†’ DoubleConv(2*out_channelâ†’out_channel).\"\"\"\n",
    "    def __init__(self, in_channel:int, out_channel:int, \n",
    "                 activation=nn.ReLU(), \n",
    "                 norm=nn.Identity(),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(activation, str):\n",
    "            activation = ACTIVATIONS[activation]()\n",
    "        if isinstance(norm, str):\n",
    "            norm = NORM_LAYERS[norm](out_channel)\n",
    "        self.norm = norm\n",
    "        # in_channel: e.g. 1024 â†’ out_channel: 512\n",
    "        self.up   = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(2*out_channel, out_channel, activation=activation, norm=norm)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_dec, x_enc):\n",
    "        x = self.up(x_dec)      # [B, out_channel, H*2, W*2]\n",
    "        x = self.norm(x)\n",
    "        # no crop needed since padding=1 kept encoder dims the same\n",
    "        x = torch.cat([x_enc, x], dim=1)  # [B, 2*out_channel, H*2, W*2]\n",
    "        x = self.conv(x)     # [B, out_channel, H*2, W*2]\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        activation = nn.ReLU(),\n",
    "        pooling: callable = nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "        **kwargs,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        channels_ratio = kwargs.get('embedding_channels', 1024)/1024\n",
    "        norm = kwargs.get('norm', 'none')\n",
    "        \n",
    "        \n",
    "        # encoder\n",
    "        self.inc   = DoubleConv(in_channels,  \n",
    "                                int(64*channels_ratio),  \n",
    "                                activation=activation, norm=norm)\n",
    "        self.down1 = Down(int(64*channels_ratio),   \n",
    "                          int(128*channels_ratio), \n",
    "                          activation=activation, pooling=pooling, norm=norm)\n",
    "        self.down2 = Down(int(128*channels_ratio),  \n",
    "                          int(256*channels_ratio), \n",
    "                          activation=activation, pooling=pooling, norm=norm)\n",
    "        self.down3 = Down(int(256*channels_ratio),  \n",
    "                          int(512*channels_ratio), \n",
    "                          activation=activation, pooling=pooling, norm=norm)\n",
    "        self.down4 = Down(int(512*channels_ratio), \n",
    "                          int(1024*channels_ratio), \n",
    "                          activation=activation, pooling=pooling, norm=norm)\n",
    "        # decoder\n",
    "        self.up1   = Up(int(1024*channels_ratio), \n",
    "                        int(512*channels_ratio), \n",
    "                        activation=activation, norm=norm)\n",
    "        self.up2   = Up(int(512*channels_ratio),  \n",
    "                        int(256*channels_ratio), \n",
    "                        activation=activation, norm=norm)\n",
    "        self.up3   = Up(int(256*channels_ratio),  \n",
    "                        int(128*channels_ratio), \n",
    "                        activation=activation, norm=norm)\n",
    "        self.up4   = Up(int(128*channels_ratio),   \n",
    "                        int(64*channels_ratio), \n",
    "                        activation=activation, norm=norm)\n",
    "        # final 1Ã—1 conv\n",
    "        self.outc  = nn.Conv2d(int(64*channels_ratio), out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)      # [B,64, H,  W]\n",
    "        x2 = self.down1(x1)   # [B,128,H/2,W/2]\n",
    "        x3 = self.down2(x2)   # [B,256,H/4,W/4]\n",
    "        x4 = self.down3(x3)   # [B,512,H/8,W/8]\n",
    "        x5 = self.down4(x4)   # [B,1024,H/16,W/16]\n",
    "        x  = self.up1(x5, x4) # [B,512,H/8,W/8]\n",
    "        x  = self.up2(x,  x3) # [B,256,H/4,W/4]\n",
    "        x  = self.up3(x,  x2) # [B,128,H/2,W/2]\n",
    "        x  = self.up4(x,  x1) # [B,64, H,  W]\n",
    "        return self.outc(x)   # [B,out_channels,H,W]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModule(nn.Module):\n",
    "    def __init__(self, \n",
    "                 modelconfig, \n",
    "                 optconfig, \n",
    "                 **kwargs, \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.modelconfig = modelconfig\n",
    "        self.optconfig = optconfig\n",
    "        self.hparams = kwargs\n",
    "        self.device = device\n",
    "\n",
    "        self.model = None\n",
    "        self.build_model()\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.configure_optimizers()\n",
    "\n",
    "        self.loss_fn = test_criterion[optconfig['loss_fn']]\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = UNet(\n",
    "            **self.modelconfig['params'], \n",
    "            ).to(self.device)\n",
    "        # self.model = neuralop.models.FNO(**self.modelconfig['params'], ).to(self.device)\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer is not None: \n",
    "            return self.optimizer, self.scheduler\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            **self.optconfig['params']\n",
    "        )\n",
    "\n",
    "        self.scheduler = []\n",
    "        SCHEDULERS = {\n",
    "            'StepLR': torch.optim.lr_scheduler.StepLR, \n",
    "            'ExponentialLR': torch.optim.lr_scheduler.ExponentialLR, \n",
    "        }\n",
    "        if self.optconfig['scheduler']: \n",
    "            SCHEDULERS[self.optconfig['scheduler']['name']](\n",
    "                self.optimizer, \n",
    "                **self.optconfig['scheduler']['params'],\n",
    "                ) \n",
    "        return self.optimizer, self.scheduler\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.model(data.to(self.device))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, ret_log:bool=False):\n",
    "        data, target = batch\n",
    "\n",
    "        # data = self.high2low(data, scale_factor=0.5, keepdim=True)\n",
    "        # target = self.high2low(target, scale_factor=0.5, keepdim=True)\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.forward(data.to(self.device))\n",
    "        loss = self.loss_fn(output, target.to(self.device))\n",
    "        loss.backward()\n",
    "\n",
    "        # if batch_idx % 3000 == 0:\n",
    "        #     if not hasattr(self, 'error'): self.error = []\n",
    "            \n",
    "        #     # hook(self.model)\n",
    "            \n",
    "        #     self.error.append(spectral_relative_error(output, target.to(self.device), dim=2)[1:].detach().cpu().numpy())\n",
    "        #     log_1d_colormap_wandb( \n",
    "        #         name='wavenumber', \n",
    "        #         value=np.stack(self.error), \n",
    "        #         title='spectral_relative_error',\n",
    "        #         fname='spectral_relative_error', \n",
    "        #         xlabel='iteration',\n",
    "        #         vmin=5e-2, vmax=2e1,\n",
    "        #         )\n",
    "            \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.log('loss', loss.item(), on_step=True)\n",
    "        # log = {}\n",
    "        # log.update({\"loss\": loss.item(), \"output\": output.detach().cpu(), 'batch': [data, target.detach().cpu()], # batch, \n",
    "        #        'learning_rate': [], })\n",
    "        # for i in range(len(self.scheduler)): log['learning_rate'].append(self.scheduler[i].get_last_lr())\n",
    "        # return log\n",
    "        return loss.item()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, ret_log:bool=True):\n",
    "        # self.model.eval()\n",
    "        data, target = batch\n",
    "\n",
    "        # data = self.high2low(data, scale_factor=0.5, keepdim=True)\n",
    "        # target = self.high2low(target, scale_factor=0.5, keepdim=True)\n",
    "\n",
    "        output = self.forward(data.to(self.device))\n",
    "        loss = self.loss_fn(output, target.to(self.device))\n",
    "        self.log('val_loss', loss.item(), on_step=False)\n",
    "        if ret_log: \n",
    "            log = {}\n",
    "            log.update({\"loss\": loss.item(), \"output\": output.detach().cpu(), 'batch': [data, target.detach().cpu()]})\n",
    "            return log\n",
    "        return output\n",
    "\n",
    "    def test_step(self, batch, batch_idx, ret_log:bool=False):\n",
    "        # self.model.eval()\n",
    "        data, target = batch\n",
    "\n",
    "        output = self.forward(data.to(self.device))\n",
    "        loss = self.loss_fn(output, target.to(self.device))\n",
    "\n",
    "        if ret_log: output = {\"loss\": loss.item(), \"output\": output.detach().cpu()}\n",
    "        return output\n",
    "    \n",
    "    def save(self, path, checkpoint:dict={}):\n",
    "        checkpoint['model_state_dict'] = self.model.state_dict()\n",
    "        checkpoint['optimizer_state_dict'] = self.optimizer.state_dict()\n",
    "        checkpoint['scheduler'] = []\n",
    "        for i in range(len(self.scheduler)):checkpoint[f'scheduler'].append(self.scheduler[i].state_dict())\n",
    "        torch.save(checkpoint, path + '.pth')\n",
    "\n",
    "    def load(self,\n",
    "        path, \n",
    "        verbose=True, \n",
    "        load_opt:bool=True,\n",
    "        freeze:bool=False,\n",
    "        ):\n",
    "        if os.path.exists(path + '.pth') and os.path.getsize(path + '.pth'):\n",
    "            checkpoint = torch.load(path + '.pth', weights_only=False)\n",
    "\n",
    "            self.model.load_state_dict(\n",
    "                checkpoint['model_state_dict'], \n",
    "                strict=False,\n",
    "                )\n",
    "            if freeze: \n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if name in checkpoint['model_state_dict']:\n",
    "                        param.requires_grad = False\n",
    "            if verbose: print(f'model loaded from {path}')\n",
    "            if load_opt:\n",
    "                if self.optimizer and 'optimizer_state_dict' in checkpoint: \n",
    "                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                    if verbose: print(f'optimizer loaded from {path}')\n",
    "                if self.scheduler and 'scheduler' in checkpoint: \n",
    "                    for i in len(self.scheduler): self.scheduler[i].load_state_dict(checkpoint['scheduler'][i])\n",
    "                    if verbose: print(f'scheduler loaded from {path}')\n",
    "    \n",
    "    def log(self, name, value, prog_bar=False, logger=None, on_step=None, on_epoch=None, reduce_fx='mean', enable_graph=False, sync_dist=False, sync_dist_group=None, add_dataloader_idx=True, batch_size=None, metric_attribute=None, rank_zero_only=False):\n",
    "        commit = False if on_step is False else True\n",
    "        wandb.log({name: value}, commit=commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCallback:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def setup(self, trainer, pl_module, stage): pass\n",
    "    def state_dict(self): pass\n",
    "\n",
    "    def load_state_dict(self, state_dict): pass\n",
    "    def on_load_checkpoint(self, trainer, pl_module, checkpoint): pass\n",
    "    def on_save_checkpoint(self, trainer, pl_module, checkpoint): pass\n",
    "    \n",
    "    def on_fit_start(self, trainer, pl_module): pass\n",
    "    def on_fit_end(self, trainer, pl_module): pass\n",
    "\n",
    "    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx): pass\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx): pass\n",
    "    def on_train_epoch_start(self, trainer, pl_module): pass\n",
    "    def on_train_epoch_end(self, trainer, pl_module): pass\n",
    "    def on_train_start(self, trainer, pl_module): pass\n",
    "    def on_train_end(self, trainer, pl_module): pass\n",
    "\n",
    "    def on_validation_batch_start(self, trainer, pl_module, batch, batch_idx): pass\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx): pass\n",
    "    def on_validation_epoch_start(self, trainer, pl_module): pass\n",
    "    def on_validation_epoch_end(self, trainer, pl_module): pass\n",
    "    def on_validation_start(self, trainer, pl_module): pass\n",
    "    def on_validation_end(self, trainer, pl_module): pass\n",
    "\n",
    "    def on_predict_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0): pass\n",
    "    def on_predict_batch_end(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0): pass\n",
    "    def on_predict_epoch_start(self, trainer, pl_module): pass\n",
    "    def on_predict_epoch_end(self, trainer, pl_module): pass\n",
    "    def on_predict_start(self, trainer, pl_module): pass\n",
    "    def on_predict_end(self, trainer, pl_module): pass\n",
    "\n",
    "    def on_test_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0): pass\n",
    "    def on_test_batch_end(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0): pass\n",
    "    def on_test_epoch_start(self, trainer, pl_module): pass\n",
    "    def on_test_epoch_end(self, trainer, pl_module): pass\n",
    "    def on_test_start(self, trainer, pl_module): pass\n",
    "    def on_test_end(self, trainer, pl_module): pass\n",
    "\n",
    "    def on_after_backward(self, trainer, pl_module): pass\n",
    "    def on_before_backward(self, trainer, pl_module, loss): pass\n",
    "    def on_before_optimizer_step(self, trainer, pl_module, optimizer): pass\n",
    "    def on_before_zero_grad(self, trainer, pl_module, optimizer): pass\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, \n",
    "                 criterions, \n",
    "                 file_dir:str='', \n",
    "                 fname:str ='',\n",
    "                 device:str =None, \n",
    "                 val_every_n_iter:int=None,\n",
    "                 ):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.criterions = criterions # DL criterion + CFD criterion + DL part-of-loss\n",
    "        self.file_dir = file_dir\n",
    "        self.fname = fname\n",
    "        self.device=device\n",
    "        \n",
    "    def on_validation_epoch_start(self, trainer, pl_module):\n",
    "        self.time_stamp = time.time()\n",
    "        trainer._current_val_return = {'loss': [], 'time': []}\n",
    "        for key in self.criterions.keys():\n",
    "            trainer._current_val_return[key] = []\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n",
    "        data, target = outputs['batch'] # batch\n",
    "        # trainer._current_val_return['batch'] = outputs['batch'] # batch\n",
    "        # trainer._current_val_return['output'] = outputs['output']\n",
    "        # trainer._current_val_return['loss'].append(outputs['loss'])\n",
    "\n",
    "        for key in self.criterions.keys():\n",
    "            value = self.criterions[key](outputs['output'].to(self.device), target.to(self.device)).item()\n",
    "\n",
    "            trainer._current_val_return[key].append(value)\n",
    "    \n",
    "    def on_validation_end(self, trainer, pl_module, fname='') -> None:\n",
    "        fname = self.fname + fname\n",
    "        # data, target = trainer._current_val_return['batch']\n",
    "        # output = trainer._current_val_return['output']\n",
    "        # if trainer.logger: trainer.logger.log({'val_loss': np.mean(trainer._current_val_return['loss'])})\n",
    "\n",
    "        epoch = trainer.current_epoch\n",
    "        print(f'  Validation Epoch: {epoch}', end='')\n",
    "        for key in self.criterions.keys():\n",
    "            value = np.mean(trainer._current_val_return[key])\n",
    "            print(f\", {key}: {value:.6f}\", end='')\n",
    "\n",
    "            if trainer.logger: trainer.logger.log({'Val/epoch/'+key: value})\n",
    "        print()\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        # ## enstrophy spectrum\n",
    "        # spectrum1 = calc_energy_spectrum(target[:,0,:], dim=2) # np.mean([calc_enstrophy_spectrum(target[i,0,:]) for i in range(len(target))], axis=0); \n",
    "        # spectrum2 = calc_energy_spectrum(output[:,0,:], dim=2) # np.mean([calc_enstrophy_spectrum(output[i,0,:]) for i in range(len(output))], axis=0)\n",
    "        # ## vorticity pdf\n",
    "        # bin1, pdf1 = calc_pdf(target[:,0,:], dim=2) # np.mean([calc_pdf(target[i,0,:]) for i in range(len(target))], axis=0); \n",
    "        # bin2, pdf2 = calc_pdf(output[:,0,:], dim=2) # np.mean([calc_pdf(output[i,0,:]) for i in range(len(output))], axis=0)\n",
    "\n",
    "        # # ### solution field \n",
    "        # plot_2d_surface(target[0,0,...,0], output[0,0,...,0], # axes=[ax1,ax2], \n",
    "        #                 kwargs={'suptitle': f'epoch={epoch}', 'figsize': (8, 4)}\n",
    "        #                 )\n",
    "        # plt.savefig(self.file_dir + fname + f'_Val_epoch={epoch}_field.png')\n",
    "        # # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "        # fig = plt.figure(figsize=(8,4))\n",
    "        # fig.suptitle(f'epoch={epoch}')\n",
    "        # ax = fig.add_subplot(1, 2, 1)\n",
    "        # plot_spectrum(spectrum1, spectrum2, ax=ax, kwargs={'title': f'enstrophy spectrum'})\n",
    "\n",
    "        # ax = fig.add_subplot(1, 2, 2)\n",
    "        # plot_pdf(bin1, pdf1, bin2, pdf2, ax=ax, kwargs={'title': f'pdf'})\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(self.file_dir + fname + f'_Val_epoch={epoch}_stat.png')\n",
    "        # # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "        ## frequency solution field \n",
    "        # self.n_modes = [[0, 8], [8, 16], [16, 32], ]\n",
    "        # data_freq = self.preprocessing(data)\n",
    "        # target_freq = self.preprocessing(target)\n",
    "        # output_freq = self.preprocessing(output)\n",
    "        # for i in range(len(target_freq)):\n",
    "        #     _data = data_freq[i][0,0].detach().cpu().numpy()\n",
    "        #     _target = target_freq[i][0,0].detach().cpu().numpy()\n",
    "        #     _output = output_freq[i][0,0].detach().cpu().numpy()\n",
    "\n",
    "        #     plot_2d_surface(\n",
    "        #         u1=_data, \n",
    "        #         u2=[_target, _output],  \n",
    "        #         cm1=True, cm2=True, vmax=None, vmin=None, \n",
    "        #         suptitle=f'epoch={epoch}',\n",
    "        #         title1=f'input',\n",
    "        #         title2=f'target',\n",
    "        #         title3=f'prediction',\n",
    "        #     )\n",
    "        #     plt.savefig(self.file_dir + fname + f'_Val_epoch={epoch}_freq={i}_stat.png')\n",
    "        #     plt.close()\n",
    "\n",
    "        #     key = f'loss_{i}'\n",
    "        #     value = ((_target - _output)**2).mean()\n",
    "        #     print(f\", {key}: {value:.6f}\", end='')\n",
    "        #     if trainer.logger: \n",
    "        #         trainer.logger.log({'Val/epoch/'+key: value})\n",
    "\n",
    "        #         image = np.concatenate((_target, _output), axis=1)\n",
    "        #         trainer.logger.log({'Val/field': wandb.Image(image)})\n",
    "\n",
    "        ## save .wandb\n",
    "        # image = np.concatenate((target[0,0,...,0], output[0,0,...,0]), axis=1)\n",
    "        # if trainer.logger: trainer.logger.log({'Val/field': wandb.Image(image)})\n",
    "        # if trainer.logger: trainer.logger.log({'Val/spectrum': wandb.plot.line_series(\n",
    "        #     xs = [np.arange(0, spectrum1.shape[-1]+1), np.arange(0, spectrum2.shape[-1]+1)], \n",
    "        #     ys = [spectrum1, spectrum2],\n",
    "        #     keys = ['ground truth', 'prediction'],\n",
    "        #     xname = ['k', 'enstrophy spectrum'],\n",
    "        #     title='enstrophy_spectrum', \n",
    "        #     )})\n",
    "        # if trainer.logger: trainer.logger.log({'Val/pdf': wandb.plot.line_series(\n",
    "        #     xs=[bin1, bin2], \n",
    "        #     ys = [pdf1, pdf2],\n",
    "        #     keys = ['ground truth', 'prediction'],\n",
    "        #     xname = ['w', 'pdf'],\n",
    "        #     title='pdf', \n",
    "        #     )})\n",
    "\n",
    "        # ## save .plt \n",
    "        # w1 = target[0,0,...,0]; w2 = output[0, 0, :]\n",
    "        # Nx, Ny = target[0,0,...,0].shape\n",
    "        # x = np.linspace(0., 2.*np.pi, Nx, endpoint=True)\n",
    "        # y = np.linspace(0., 2.*np.pi, Ny, endpoint=True)\n",
    "        # with open(self.file_dir + self.fname + '_Val_field' + '.plt', 'a') as f:\n",
    "        #     f.write(f'Zone T=\"T={epoch}\" I={Nx} J={Ny}\\n')\n",
    "        #     for ix in range(Nx):\n",
    "        #         for iy in range(Ny):\n",
    "        #             f.write(f'{x[ix]:.10f} {y[iy]:.10f} {w1[ix, iy]:.10f} {w2[ix, iy]:.10f}\\n')\n",
    "\n",
    "        # \n",
    "        # with open(self.file_dir + self.fname + '_Val_spectrum' + '.plt', 'a') as f:\n",
    "        #     f.write(f'Zone T=\"T={epoch}\" I={Nk}\\n')\n",
    "        #     for ik in range(Nk):\n",
    "        #         f.write(f'{k_1d[ik]} {spectrum1[ik]:.10f} {spectrum2[ik]:.10f}\\n')\n",
    "\n",
    "        # n = bin1.shape[-1]\n",
    "        # with open(self.file_dir + self.fname + '_pdf' + '.plt', 'a') as f:\n",
    "        #     f.write(f'Zone T=\"T={epoch}\" I={n}\\n')\n",
    "        #     for i in range(n):\n",
    "        #         f.write(f'{bin1[i]:.10f} {pdf1[i]:.10f} {bin2[i]:.10f} {pdf2[i]:.10f}\\n')\n",
    "\n",
    "\n",
    "    def on_test_batch_start(self, trainer, pl_module, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n",
    "        self.time_stamp = time.time()\n",
    "\n",
    "    def on_test_batch_end(self, trainer, pl_module, outputs, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n",
    "        data, target = batch\n",
    "        trainer._current_test_return['batch'] = batch\n",
    "        output = trainer._current_test_return['output'] = outputs['output']\n",
    "        \n",
    "        runtime = time.time() - self.time_stamp\n",
    "        if trainer.logger: trainer.logger.log({'Test/time': runtime})\n",
    "        print(f'Test sample {batch_idx}(time: {runtime:.6f}, batch size: {len(output)})', end='')\n",
    "        for key in self.criterions.keys():\n",
    "            value = self.criterions[key](outputs['output'].to(self.device), target.to(self.device)).item()\n",
    "\n",
    "            trainer._current_test_return[key].append(value)\n",
    "            print(f\", {key}: {value:.6f}\", end='')\n",
    "        print()\n",
    "        \n",
    "        ## enstrophy spectrum\n",
    "        spectrum1 = calc_energy_spectrum(target[:,0,:], dim=2) # np.mean([calc_enstrophy_spectrum(target[i,0,:]) for i in range(len(target))], axis=0); \n",
    "        spectrum2 = calc_energy_spectrum(output[:,0,:], dim=2) # np.mean([calc_enstrophy_spectrum(output[i,0,:]) for i in range(len(output))], axis=0)\n",
    "        ## vorticity pdf\n",
    "        bin1, pdf1 = calc_pdf(target[:,0,:], dim=2) # np.mean([calc_pdf(target[i,0,:]) for i in range(len(target))], axis=0); \n",
    "        bin2, pdf2 = calc_pdf(output[:,0,:], dim=2) # np.mean([calc_pdf(output[i,0,:]) for i in range(len(output))], axis=0)\n",
    "\n",
    "        # ### solution field \n",
    "        plot_2d_surface(target[0,0,...,0], output[0,0,...,0], # axes=[ax1,ax2], \n",
    "                        kwargs={'figsize': (8, 4)}\n",
    "                        )\n",
    "        plt.savefig(self.file_dir + self.fname + f'_Test_sample={batch_idx}_field.png')\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        ax = fig.add_subplot(1, 2, 1)\n",
    "        plot_spectrum(spectrum1, spectrum2, ax=ax, kwargs={'title': f'enstrophy spectrum'})\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, 2)\n",
    "        plot_pdf(bin1, pdf1, bin2, pdf2, ax=ax, kwargs={'title': f'pdf'})\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.file_dir + self.fname + f'_Test_sample={batch_idx}_stat.png')\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        ## save .wandb\n",
    "        Nt = len(output)\n",
    "        for it in range(Nt):\n",
    "            image = np.concatenate((target[it,0,:], output[it,0,:]), axis=1)\n",
    "            if trainer.logger: trainer.logger.log({'Test/'+f'sample_{batch_idx}/'+'field': wandb.Image(image)})\n",
    "        if trainer.logger: trainer.logger.log({'Test/spectrum': wandb.plot.line_series(\n",
    "            xs=[np.arange(0, spectrum1.shape[-1]+1), np.arange(0, spectrum2.shape[-1]+1)], \n",
    "            ys = [spectrum1, spectrum2],\n",
    "            keys = ['ground truth', 'prediction'],\n",
    "            xname = ['k', 'enstrophy spectrum'],\n",
    "            title='enstrophy_spectrum', \n",
    "            )})\n",
    "        if trainer.logger: trainer.logger.log({'Test/pdf': wandb.plot.line_series(\n",
    "            xs=[bin1, bin2], \n",
    "            ys = [pdf1, pdf2],\n",
    "            keys = ['ground truth', 'prediction'],\n",
    "            xname = ['w', 'pdf'],\n",
    "            title='pdf', \n",
    "            )})\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module) -> None:\n",
    "        data, target = trainer._current_test_return['batch']\n",
    "        output = trainer._current_test_return['output']\n",
    "\n",
    "        print('Test ')\n",
    "        for key in self.criterions.keys():\n",
    "            value = np.mean(trainer._current_test_return[key])\n",
    "            print(f\", {key}: {value:.6f}\", end='')\n",
    "\n",
    "            if trainer.logger: trainer.logger.log({'Test/'+key: value})\n",
    "        print()\n",
    "\n",
    "    def preprocessing(self, x):\n",
    "        n_modes = self.n_modes\n",
    "        x = self.split(x, modes=n_modes)\n",
    "        # x = self.scaling(x, dim=2, isNormalize=False)\n",
    "        return x \n",
    "    \n",
    "    def postprocessing(self, x):\n",
    "        n_modes = self.n_modes\n",
    "        # x = self.unscaling(x)\n",
    "        x = self.unsplit(x, modes=n_modes)\n",
    "        return x\n",
    "    \n",
    "    def scaling(self, x, \n",
    "                dim=1, \n",
    "                isNormalize:bool=False):\n",
    "        dim = list(range(-dim, 0))\n",
    "        if isNormalize:\n",
    "            self.mean = x.amin(dim=dim, keepdim=True)\n",
    "            self.std = x.amax(dim=dim, keepdim=True) - self.mean\n",
    "        else: \n",
    "            self.mean = x.mean(dim=dim, keepdim=True)\n",
    "            self.std = x.std(dim=dim, keepdim=True)\n",
    "        return (x - self.mean) / self.std\n",
    "    \n",
    "    def unscaling(self, x):\n",
    "        return x * self.std + self.mean\n",
    "    \n",
    "    def split(self, x, \n",
    "              modes=None\n",
    "              ):\n",
    "\n",
    "        b, c, *data_shape = x.shape\n",
    "        fft_dims = list(range(-len(data_shape), 0))\n",
    "        Fx = torch.fft.fft2(x, dim=fft_dims, norm='forward')\n",
    "        Fx_ = torch.zeros((len(modes), b, c, *data_shape), dtype=Fx.dtype, device=x.device)\n",
    "        \n",
    "        k = [torch.fft.fftfreq(n, d=1./n) for n in data_shape]\n",
    "        # k += [torch.fft.rfftfreq(data_shape[-1], d=1./data_shape[-1])] if Fx.dtype in [torch.float16, torch.float32, torch.float64] else [torch.fft.fftfreq(data_shape[-1], d=1./data_shape[-1])]\n",
    "\n",
    "\n",
    "        k = torch.meshgrid(k, indexing='ij')\n",
    "        k = torch.stack(k)\n",
    "        k = torch.sqrt(torch.sum(k**2, axis=0)).to(x.device)\n",
    "        for i in range(len(modes)):\n",
    "            k1, k2 = modes[i]\n",
    "            idx = (k1 <= k) & (k <= k2)\n",
    "            Fx_[i] = Fx * idx\n",
    "\n",
    "        x_ = torch.fft.ifft2(Fx_, dim=fft_dims, norm='forward').real # x_ = Fx_ # # \n",
    "        \n",
    "        return x_\n",
    "\n",
    "    def unsplit(self, x_, \n",
    "              modes=None\n",
    "              ):\n",
    "        n, b, c, *data_shape = x_.shape\n",
    "        fft_dims = list(range(-len(data_shape), 0))\n",
    "        \n",
    "        Fx_ = torch.fft.fft2(x_, dim=fft_dims, norm='forward') # Fx_ = x_ # \n",
    "        \n",
    "        Fx = torch.zeros((b, c, *data_shape), dtype=Fx_.dtype, device=x_.device)\n",
    "        k = [torch.fft.fftfreq(n, d=1./n) for n in data_shape[:-1]]\n",
    "        k += [torch.fft.rfftfreq(data_shape[-1], d=1./data_shape[-1])] if Fx.dtype in [torch.float16, torch.float32, torch.float64] else [torch.fft.fftfreq(data_shape[-1], d=1./data_shape[-1])]\n",
    "        k = torch.meshgrid(k, indexing='ij')\n",
    "        k = torch.stack(k)\n",
    "        k = torch.sqrt(torch.sum(k**2, axis=0)).to(x_.device)\n",
    "        for i in range(len(modes)):\n",
    "            k1, k2 = modes[i]\n",
    "            idx = (k1 <= k) & (k <= k2)\n",
    "            Fx[..., idx] = Fx_[i, ..., idx]\n",
    "\n",
    "        x = torch.fft.ifft2(Fx, dim=fft_dims, norm='forward').real\n",
    "        return x\n",
    "\n",
    "class EarlyStopping(BaseCallback):\n",
    "    def __init__(self, \n",
    "        criterion=nn.MSELoss(reduction='mean'), # monitor: str,\n",
    "        min_delta: float = 0.0,\n",
    "        patience: int = 3,\n",
    "        verbose: bool = False,\n",
    "        mode: str = \"min\",\n",
    "        strict: bool = True,\n",
    "        check_finite: bool = True,\n",
    "        stopping_threshold: Optional[float] = None,\n",
    "        divergence_threshold: Optional[float] = None,\n",
    "        check_on_train_epoch_end: Optional[bool] = None,\n",
    "        log_rank_zero_only: bool = False,\n",
    "        ):\n",
    "        super(EarlyStopping, self).__init__()\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.mode = mode\n",
    "        self.strict = strict\n",
    "        self.check_finite = check_finite\n",
    "        self.stopping_threshold = stopping_threshold\n",
    "        self.divergence_threshold = divergence_threshold\n",
    "        self.check_on_train_epoch_end = check_on_train_epoch_end\n",
    "        self.log_rank_zero_only = log_rank_zero_only\n",
    "        \n",
    "        self._current_return = []\n",
    "        self.value_prev = None\n",
    "        self.patience_count = 0\n",
    "\n",
    "    def _stopping_criterion(self, value, value_prev):\n",
    "        flags = []\n",
    "        if self.mode == 'min':\n",
    "            flags.append((value_prev - value) < self.min_delta)\n",
    "        if self.mode == 'max':\n",
    "            flags.append((value - value_prev) < self.min_delta)\n",
    "\n",
    "        if self.divergence_threshold is not None:\n",
    "            flags.append(abs(value) >= self.divergence_threshold)\n",
    "\n",
    "        if self.stopping_threshold is not None:\n",
    "            if self.mode == 'min':\n",
    "                flags.append((value < self.stopping_threshold))\n",
    "            if self.mode == 'max':\n",
    "                flags.append((value > self.stopping_threshold))\n",
    "        return bool(np.sum(flags))\n",
    "    \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch: Any, batch_idx: int) -> None:\n",
    "        if self.check_on_train_epoch_end:\n",
    "            return self.on_validation_batch_end(trainer, pl_module, outputs, batch, batch_idx)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if self.check_on_train_epoch_end:\n",
    "            return self.on_validation_end(trainer, pl_module)\n",
    "    \n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n",
    "        data, target = outputs['batch'] # batch\n",
    "        value = self.criterion(outputs['output'], target)\n",
    "        self._current_return.append(value)\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        value = np.mean(self._current_return); self._current_return = []\n",
    "        if self.value_prev is None: \n",
    "            self.value_prev = value\n",
    "            return \n",
    "\n",
    "        if self._stopping_criterion(value, self.value_prev):\n",
    "            if self.patience_count < self.patience:\n",
    "                self.patience_count += 1\n",
    "                self.value_prev = value\n",
    "                return \n",
    "            \n",
    "            trainer.should_stop = True\n",
    "            self.patience_count = 0\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping. epoch: {trainer.current_epoch}, value: {value}, delta: {abs(self.value_prev - value)}')\n",
    "        else:\n",
    "            self.patience_count = 0\n",
    "            self.value_prev = value\n",
    "\n",
    "class CheckPoint(BaseCallback):\n",
    "    def __init__(self, \n",
    "                 ckpt_name:str = None,\n",
    "                 ckpt_path = './checkpoint/',\n",
    "                 every_n_epoch: Optional[int] = None,\n",
    "                 criterion=nn.MSELoss(reduction='mean'), # monitor: str,\n",
    "                 mode:str = 'min',\n",
    "                 load_ckpt:bool = True,\n",
    "\n",
    "                 save_top_k:int=1, \n",
    "                 ):\n",
    "        super(CheckPoint, self).__init__()\n",
    "        self.ckpt_name = ckpt_name\n",
    "        self.ckpt_path = ckpt_path \n",
    "        os.makedirs(self.ckpt_path, exist_ok=True)\n",
    "\n",
    "        self.every_n_epoch = every_n_epoch\n",
    "        self.criterion = criterion\n",
    "        self.mode = mode \n",
    "        self.value_prev = None\n",
    "\n",
    "        self.load_ckpt = load_ckpt\n",
    "\n",
    "        self.save_top_k = save_top_k\n",
    "        self._topk = []\n",
    "\n",
    "    def _stopping_criterion(self, value, value_prev):\n",
    "        if self.mode == 'min': return value_prev > value\n",
    "        if self.mode == 'max': return value_prev < value\n",
    "    \n",
    "    def _save(self, trainer, pl_module, suffix):\n",
    "        name = f'{str(pl_module)}' if self.ckpt_name is None else self.ckpt_name \n",
    "        path = os.path.join(self.ckpt_path, name + suffix)\n",
    "        trainer.save(model=pl_module, path=path)\n",
    "        return path\n",
    "    \n",
    "    def _save_top_k(self, trainer, pl_module, value, epoch):\n",
    "        def _cmp_key(x):\n",
    "            return x['value'] if self.mode == 'min' else -x['value']\n",
    "        \n",
    "        if not self._topk:\n",
    "            path = self._save(trainer, pl_module, \"_top0\")  # must create ckpt at ..._top0.pth\n",
    "            self._topk = [{\"value\": value, \"epoch\": epoch, \"path\": path}]\n",
    "            return True\n",
    "        \n",
    "        best = self._topk[0]\n",
    "        if self._stopping_criterion(value, best['value']):\n",
    "            last_idx = min(len(self._topk), self.save_top_k - 1)\n",
    "            for i in range(last_idx, 0, -1):\n",
    "                src_path = self._topk[i - 1][\"path\"]\n",
    "                dst_path = os.path.join(self.ckpt_path, f\"{self.ckpt_name}_top{i}.pth\")\n",
    "                os.replace(src_path, dst_path)\n",
    "\n",
    "                if i < len(self._topk):\n",
    "                    self._topk[i][\"value\"] = self._topk[i - 1][\"value\"]\n",
    "                    self._topk[i][\"epoch\"] = self._topk[i - 1][\"epoch\"]\n",
    "                    self._topk[i][\"path\"] = dst_path\n",
    "                else:\n",
    "                    # When list was shorter than save_top_k, append a new entry\n",
    "                    self._topk.append({\n",
    "                        \"value\": self._topk[i - 1][\"value\"],\n",
    "                        \"epoch\": self._topk[i - 1][\"epoch\"],\n",
    "                        \"path\": dst_path,\n",
    "                    })\n",
    "            \n",
    "            top0_path = os.path.join(self.ckpt_path, f\"{self.ckpt_name}_top0.pth\") \n",
    "            path = self._save(trainer, pl_module, \"_top0\")\n",
    "            self._topk[0] = {\"value\": value, \"epoch\": epoch, \"path\": path}\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        if self.load_ckpt:\n",
    "            ckpt_name = f'{str(pl_module)}' if self.ckpt_name is None else self.ckpt_name\n",
    "            epoch = trainer.load(model=pl_module, path=os.path.join(self.ckpt_path, ckpt_name))\n",
    "            if isinstance(epoch, int) and epoch >= 0:\n",
    "                trainer.current_epoch = epoch\n",
    "\n",
    "    def on_validation_epoch_start(self, trainer, pl_module):\n",
    "        # trainer._current_train_return['Checkpoint'] = []\n",
    "        self._current_return = []\n",
    "    \n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        input, target = outputs['batch']\n",
    "        output = outputs['output']\n",
    "        with torch.no_grad():\n",
    "            v = self.criterion(output, target)\n",
    "            v = float(v.detach().item()) if torch.is_tensor(v) else float(v)\n",
    "        self._current_return.append(v)\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "        ckpt_name = f'{str(pl_module)}' if self.ckpt_name is None else self.ckpt_name\n",
    "\n",
    "        ## save every_n_epoch \n",
    "        if self.every_n_epoch is not None and epoch % self.every_n_epoch == 0:\n",
    "            self._save(trainer, pl_module, f'_epoch={epoch}')  # epoch-tagged\n",
    "\n",
    "        ## save best k case\n",
    "        value = np.mean(self._current_return) \n",
    "        self._save_top_k(trainer, pl_module, value=value, epoch=epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _CallbackRunner:\n",
    "    \"\"\"Pre-binds existing hooks once to minimize per-batch attribute lookups.\"\"\"\n",
    "    __slots__ = (\"hooks\",)\n",
    "    def __init__(self, callbacks: List[BaseCallback]):\n",
    "        names = [\n",
    "            \"on_fit_start\", \"on_fit_end\",\n",
    "            \"on_train_start\", \"on_train_end\",\n",
    "            \"on_train_epoch_start\", \"on_train_epoch_end\",\n",
    "            \"on_train_batch_start\", \"on_train_batch_end\",\n",
    "            \"on_validation_start\", \"on_validation_end\",\n",
    "            \"on_validation_batch_start\", \"on_validation_batch_end\",\n",
    "            \"on_validation_epoch_start\", \"on_validation_epoch_end\",\n",
    "            \"on_test_start\", \"on_test_end\",\n",
    "            \"on_test_epoch_start\", \"on_test_epoch_end\",\n",
    "            \"on_test_batch_start\", \"on_test_batch_end\",\n",
    "        ]\n",
    "        self.hooks = {n: [] for n in names}\n",
    "        for cb in callbacks or []:\n",
    "            for n in names:\n",
    "                fn = getattr(cb, n, None)\n",
    "                if fn is not None and callable(fn):\n",
    "                    self.hooks[n].append(fn)\n",
    "\n",
    "    def call(self, name: str, *args, **kwargs):\n",
    "        for fn in self.hooks.get(name, ()):\n",
    "            fn(*args, **kwargs)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, \n",
    "                 max_epochs, \n",
    "                 check_val_every_n_epochs:int=1,\n",
    "                 check_val_every_n_iter:int=None,\n",
    "                 callbacks:List=None, \n",
    "                 enable_progress_bar: bool = False, \n",
    "                 logger = False,\n",
    "                 ):\n",
    "        self.max_epochs = max_epochs\n",
    "        self._callbacks = _CallbackRunner(callbacks or []) # self.callbacks=callbacks\n",
    "        self.current_epoch=0\n",
    "        self.check_val_every_n_epochs = check_val_every_n_epochs\n",
    "        self.check_val_every_n_iter = check_val_every_n_iter\n",
    "        self.enable_progress_bar = enable_progress_bar\n",
    "        self.logger = logger\n",
    "\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "    \n",
    "    def progbar_wrapper(self, iterable, total: int, **kwargs: Any):\n",
    "        if self.enable_progress_bar:\n",
    "            return tqdm(iterable, total=total, **kwargs)\n",
    "        return iterable\n",
    "    \n",
    "    def fit(self, \n",
    "            model, \n",
    "            train_dataloaders=None, val_dataloaders=None, \n",
    "            datamodule=None, \n",
    "            ckpt_path=None\n",
    "        ):\n",
    "        self._callbacks.call(\"on_fit_start\", trainer=self, pl_module=model) # for cb in self.callbacks: cb.on_fit_start(trainer=self, pl_module=model)\n",
    "        if datamodule is not None:\n",
    "            if not isinstance(datamodule, (list, tuple)): datamodule = [datamodule]\n",
    "            train_dataloaders, val_dataloaders = [], []\n",
    "            for dm in datamodule:\n",
    "                train_dataloaders.append(dm.train_dataloader())\n",
    "                if dm.val_dataloader(): val_dataloaders.append(dm.val_dataloader())\n",
    "        if not isinstance(train_dataloaders, (list, tuple)): train_dataloaders = [train_dataloaders]\n",
    "        if not isinstance(val_dataloaders, (list, tuple)) and val_dataloaders is not None: val_dataloaders = [val_dataloaders]\n",
    "        \n",
    "        self.should_stop = False\n",
    "        while not self.should_stop:\n",
    "            ## train\n",
    "            for train_loader in train_dataloaders:\n",
    "                model.train()\n",
    "                self.train(model, train_loader)\n",
    "\n",
    "            ## validation \n",
    "            if val_dataloaders and (self.current_epoch % self.check_val_every_n_epochs == 0):\n",
    "                model.eval()\n",
    "                for val_loader in val_dataloaders or []:\n",
    "                    self.validate(model, val_loader)\n",
    "            \n",
    "            ## stopping criterion \n",
    "            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\n",
    "                self.should_stop = True\n",
    "            self.current_epoch += 1\n",
    "\n",
    "        # for cb in self.callbacks: cb.on_train_end(trainer=self, pl_module=model)\n",
    "        self._callbacks.call(\"on_fit_end\", trainer=self, pl_module=model)  \n",
    "    \n",
    "    def train(self, model, dataloaders, datamodule=None):\n",
    "        self._callbacks.call(\"on_train_start\", trainer=self, pl_module=model) \n",
    "        if datamodule: dataloaders = datamodule.train_dataloader()\n",
    "\n",
    "        model.train()\n",
    "        self._callbacks.call(\"on_train_epoch_start\", trainer=self, pl_module=model) \n",
    "        self._current_loss = []\n",
    "        for batch_idx, batch in self.progbar_wrapper(enumerate(dataloaders), total=len(dataloaders)):\n",
    "            self._callbacks.call(\"on_train_batch_start\", trainer=self, pl_module=model, batch=batch, batch_idx=batch_idx) \n",
    "            outputs = model.training_step(batch, batch_idx)\n",
    "            if isinstance(outputs, float): self._current_loss.append(outputs)\n",
    "            self._callbacks.call(\"on_train_batch_end\", trainer=self, pl_module=model, outputs=outputs, batch=batch, batch_idx=batch_idx)\n",
    "        \n",
    "        self._callbacks.call(\"on_train_epoch_end\", trainer=self, pl_module=model) \n",
    "        self.loss.append(np.mean(self._current_loss))\n",
    "            \n",
    "        self._callbacks.call(\"on_train_end\", trainer=self, pl_module=model) \n",
    "\n",
    "    def validate(self, model, dataloaders=None, ckpt_path=None, verbose=True, datamodule=None):\n",
    "        self._callbacks.call(\"on_validation_start\", trainer=self, pl_module=model) \n",
    "        if datamodule: dataloaders = datamodule.val_dataloader()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            self._callbacks.call(\"on_validation_epoch_start\", trainer=self, pl_module=model) \n",
    "            self._current_val_loss = []\n",
    "            for batch_idx, batch in self.progbar_wrapper(enumerate(dataloaders), total=len(dataloaders)): # enumerate(dataloaders):\n",
    "                self._callbacks.call(\"on_validation_batch_start\", trainer=self, pl_module=model, batch=batch, batch_idx=batch_idx) \n",
    "                outputs = model.validation_step(batch, batch_idx)\n",
    "\n",
    "                val_loss = outputs['loss'] if isinstance(outputs, dict) and 'loss' in outputs else outputs\n",
    "                self._current_val_loss.append(val_loss)\n",
    "                \n",
    "                self._callbacks.call(\"on_validation_batch_end\", trainer=self, pl_module=model, outputs=outputs, batch=batch, batch_idx=batch_idx) \n",
    "            \n",
    "            val_loss = np.mean(self._current_val_loss)\n",
    "            self.val_loss.append(val_loss)\n",
    "            if self.logger: self.logger.log({'Val/iter/loss':val_loss}, commit=True)\n",
    "            self._callbacks.call(\"on_validation_epoch_end\", trainer=self, pl_module=model)\n",
    "        self._callbacks.call(\"on_validation_end\", trainer=self, pl_module=model) \n",
    "    \n",
    "    def test(self, model, dataloaders=None, ckpt_path=None, verbose=True, datamodule=None):\n",
    "        self._callbacks.call(\"on_test_start\", trainer=self, pl_module=model) \n",
    "        if datamodule: dataloaders = datamodule.test_dataloader()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            self._callbacks.call(\"on_test_epoch_start\", trainer=self, pl_module=model) \n",
    "            for batch_idx, batch in self.progbar_wrapper(enumerate(dataloaders), total=len(dataloaders)):\n",
    "                self._callbacks.call(\"on_test_batch_start\", trainer=self, pl_module=model, batch=batch, batch_idx=batch_idx) \n",
    "                outputs = model.test_step(batch, batch_idx)\n",
    "                self._callbacks.call(\"on_test_batch_end\", trainer=self, pl_module=model, outputs=outputs, batch=batch, batch_idx=batch_idx) \n",
    "            self._callbacks.call(\"on_test_epoch_end\", trainer=self, pl_module=model) \n",
    "        self._callbacks.call(\"on_test_end\", trainer=self, pl_module=model)\n",
    "\n",
    "    def save(self, model, path):\n",
    "        checkpoint = {}\n",
    "        checkpoint['epoch'] = self.current_epoch\n",
    "        if self.loss: checkpoint['loss'] = self.loss\n",
    "        if self.val_loss: checkpoint['val_loss'] = self.val_loss\n",
    "        model.save(path, checkpoint)\n",
    "\n",
    "    def load(self, model, path):\n",
    "        model.load(path)\n",
    "        if os.path.exists(path + '.pth') and os.path.getsize(path + '.pth'):\n",
    "            checkpoint = torch.load(path + '.pth')\n",
    "            self.current_epoch = checkpoint['epoch']\n",
    "            if 'loss' in checkpoint: self.loss = checkpoint['loss']\n",
    "            if 'val_loss' in checkpoint: self.val_loss = checkpoint['val_loss']\n",
    "            return self.current_epoch\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset metadata: dict_keys(['nu', 'Lx', 'Ly', 'dx', 'dy', 'Nx', 'Ny', 'x', 'y', 'dt', 'Nt', 'T', 't0', 't'])\n",
      "{'nu': 5e-05, 'Lx': 6.234097921967246, 'Ly': 6.234097921967246, 'dx': 0.04908738521234052, 'dy': 0.04908738521234052, 'Nx': 128, 'Ny': 128, 'x': array([0.        , 0.04908739, 0.09817477, 0.14726216, 0.19634954,\n",
      "       0.24543693, 0.29452431, 0.3436117 , 0.39269908, 0.44178647,\n",
      "       0.49087385, 0.53996124, 0.58904862, 0.63813601, 0.68722339,\n",
      "       0.73631078, 0.78539816, 0.83448555, 0.88357293, 0.93266032,\n",
      "       0.9817477 , 1.03083509, 1.07992247, 1.12900986, 1.17809725,\n",
      "       1.22718463, 1.27627202, 1.3253594 , 1.37444679, 1.42353417,\n",
      "       1.47262156, 1.52170894, 1.57079633, 1.61988371, 1.6689711 ,\n",
      "       1.71805848, 1.76714587, 1.81623325, 1.86532064, 1.91440802,\n",
      "       1.96349541, 2.01258279, 2.06167018, 2.11075756, 2.15984495,\n",
      "       2.20893233, 2.25801972, 2.3071071 , 2.35619449, 2.40528188,\n",
      "       2.45436926, 2.50345665, 2.55254403, 2.60163142, 2.6507188 ,\n",
      "       2.69980619, 2.74889357, 2.79798096, 2.84706834, 2.89615573,\n",
      "       2.94524311, 2.9943305 , 3.04341788, 3.09250527, 3.14159265,\n",
      "       3.19068004, 3.23976742, 3.28885481, 3.33794219, 3.38702958,\n",
      "       3.43611696, 3.48520435, 3.53429174, 3.58337912, 3.63246651,\n",
      "       3.68155389, 3.73064128, 3.77972866, 3.82881605, 3.87790343,\n",
      "       3.92699082, 3.9760782 , 4.02516559, 4.07425297, 4.12334036,\n",
      "       4.17242774, 4.22151513, 4.27060251, 4.3196899 , 4.36877728,\n",
      "       4.41786467, 4.46695205, 4.51603944, 4.56512682, 4.61421421,\n",
      "       4.6633016 , 4.71238898, 4.76147637, 4.81056375, 4.85965114,\n",
      "       4.90873852, 4.95782591, 5.00691329, 5.05600068, 5.10508806,\n",
      "       5.15417545, 5.20326283, 5.25235022, 5.3014376 , 5.35052499,\n",
      "       5.39961237, 5.44869976, 5.49778714, 5.54687453, 5.59596191,\n",
      "       5.6450493 , 5.69413668, 5.74322407, 5.79231146, 5.84139884,\n",
      "       5.89048623, 5.93957361, 5.988661  , 6.03774838, 6.08683577,\n",
      "       6.13592315, 6.18501054, 6.23409792]), 'y': array([0.        , 0.04908739, 0.09817477, 0.14726216, 0.19634954,\n",
      "       0.24543693, 0.29452431, 0.3436117 , 0.39269908, 0.44178647,\n",
      "       0.49087385, 0.53996124, 0.58904862, 0.63813601, 0.68722339,\n",
      "       0.73631078, 0.78539816, 0.83448555, 0.88357293, 0.93266032,\n",
      "       0.9817477 , 1.03083509, 1.07992247, 1.12900986, 1.17809725,\n",
      "       1.22718463, 1.27627202, 1.3253594 , 1.37444679, 1.42353417,\n",
      "       1.47262156, 1.52170894, 1.57079633, 1.61988371, 1.6689711 ,\n",
      "       1.71805848, 1.76714587, 1.81623325, 1.86532064, 1.91440802,\n",
      "       1.96349541, 2.01258279, 2.06167018, 2.11075756, 2.15984495,\n",
      "       2.20893233, 2.25801972, 2.3071071 , 2.35619449, 2.40528188,\n",
      "       2.45436926, 2.50345665, 2.55254403, 2.60163142, 2.6507188 ,\n",
      "       2.69980619, 2.74889357, 2.79798096, 2.84706834, 2.89615573,\n",
      "       2.94524311, 2.9943305 , 3.04341788, 3.09250527, 3.14159265,\n",
      "       3.19068004, 3.23976742, 3.28885481, 3.33794219, 3.38702958,\n",
      "       3.43611696, 3.48520435, 3.53429174, 3.58337912, 3.63246651,\n",
      "       3.68155389, 3.73064128, 3.77972866, 3.82881605, 3.87790343,\n",
      "       3.92699082, 3.9760782 , 4.02516559, 4.07425297, 4.12334036,\n",
      "       4.17242774, 4.22151513, 4.27060251, 4.3196899 , 4.36877728,\n",
      "       4.41786467, 4.46695205, 4.51603944, 4.56512682, 4.61421421,\n",
      "       4.6633016 , 4.71238898, 4.76147637, 4.81056375, 4.85965114,\n",
      "       4.90873852, 4.95782591, 5.00691329, 5.05600068, 5.10508806,\n",
      "       5.15417545, 5.20326283, 5.25235022, 5.3014376 , 5.35052499,\n",
      "       5.39961237, 5.44869976, 5.49778714, 5.54687453, 5.59596191,\n",
      "       5.6450493 , 5.69413668, 5.74322407, 5.79231146, 5.84139884,\n",
      "       5.89048623, 5.93957361, 5.988661  , 6.03774838, 6.08683577,\n",
      "       6.13592315, 6.18501054, 6.23409792]), 'dt': 0.009999999999999787, 'Nt': 600, 'T': 16.990000000000002, 't0': 11.0, 't': array([11.  , 11.01, 11.02, 11.03, 11.04, 11.05, 11.06, 11.07, 11.08,\n",
      "       11.09, 11.1 , 11.11, 11.12, 11.13, 11.14, 11.15, 11.16, 11.17,\n",
      "       11.18, 11.19, 11.2 , 11.21, 11.22, 11.23, 11.24, 11.25, 11.26,\n",
      "       11.27, 11.28, 11.29, 11.3 , 11.31, 11.32, 11.33, 11.34, 11.35,\n",
      "       11.36, 11.37, 11.38, 11.39, 11.4 , 11.41, 11.42, 11.43, 11.44,\n",
      "       11.45, 11.46, 11.47, 11.48, 11.49, 11.5 , 11.51, 11.52, 11.53,\n",
      "       11.54, 11.55, 11.56, 11.57, 11.58, 11.59, 11.6 , 11.61, 11.62,\n",
      "       11.63, 11.64, 11.65, 11.66, 11.67, 11.68, 11.69, 11.7 , 11.71,\n",
      "       11.72, 11.73, 11.74, 11.75, 11.76, 11.77, 11.78, 11.79, 11.8 ,\n",
      "       11.81, 11.82, 11.83, 11.84, 11.85, 11.86, 11.87, 11.88, 11.89,\n",
      "       11.9 , 11.91, 11.92, 11.93, 11.94, 11.95, 11.96, 11.97, 11.98,\n",
      "       11.99, 12.  , 12.01, 12.02, 12.03, 12.04, 12.05, 12.06, 12.07,\n",
      "       12.08, 12.09, 12.1 , 12.11, 12.12, 12.13, 12.14, 12.15, 12.16,\n",
      "       12.17, 12.18, 12.19, 12.2 , 12.21, 12.22, 12.23, 12.24, 12.25,\n",
      "       12.26, 12.27, 12.28, 12.29, 12.3 , 12.31, 12.32, 12.33, 12.34,\n",
      "       12.35, 12.36, 12.37, 12.38, 12.39, 12.4 , 12.41, 12.42, 12.43,\n",
      "       12.44, 12.45, 12.46, 12.47, 12.48, 12.49, 12.5 , 12.51, 12.52,\n",
      "       12.53, 12.54, 12.55, 12.56, 12.57, 12.58, 12.59, 12.6 , 12.61,\n",
      "       12.62, 12.63, 12.64, 12.65, 12.66, 12.67, 12.68, 12.69, 12.7 ,\n",
      "       12.71, 12.72, 12.73, 12.74, 12.75, 12.76, 12.77, 12.78, 12.79,\n",
      "       12.8 , 12.81, 12.82, 12.83, 12.84, 12.85, 12.86, 12.87, 12.88,\n",
      "       12.89, 12.9 , 12.91, 12.92, 12.93, 12.94, 12.95, 12.96, 12.97,\n",
      "       12.98, 12.99, 13.  , 13.01, 13.02, 13.03, 13.04, 13.05, 13.06,\n",
      "       13.07, 13.08, 13.09, 13.1 , 13.11, 13.12, 13.13, 13.14, 13.15,\n",
      "       13.16, 13.17, 13.18, 13.19, 13.2 , 13.21, 13.22, 13.23, 13.24,\n",
      "       13.25, 13.26, 13.27, 13.28, 13.29, 13.3 , 13.31, 13.32, 13.33,\n",
      "       13.34, 13.35, 13.36, 13.37, 13.38, 13.39, 13.4 , 13.41, 13.42,\n",
      "       13.43, 13.44, 13.45, 13.46, 13.47, 13.48, 13.49, 13.5 , 13.51,\n",
      "       13.52, 13.53, 13.54, 13.55, 13.56, 13.57, 13.58, 13.59, 13.6 ,\n",
      "       13.61, 13.62, 13.63, 13.64, 13.65, 13.66, 13.67, 13.68, 13.69,\n",
      "       13.7 , 13.71, 13.72, 13.73, 13.74, 13.75, 13.76, 13.77, 13.78,\n",
      "       13.79, 13.8 , 13.81, 13.82, 13.83, 13.84, 13.85, 13.86, 13.87,\n",
      "       13.88, 13.89, 13.9 , 13.91, 13.92, 13.93, 13.94, 13.95, 13.96,\n",
      "       13.97, 13.98, 13.99, 14.  , 14.01, 14.02, 14.03, 14.04, 14.05,\n",
      "       14.06, 14.07, 14.08, 14.09, 14.1 , 14.11, 14.12, 14.13, 14.14,\n",
      "       14.15, 14.16, 14.17, 14.18, 14.19, 14.2 , 14.21, 14.22, 14.23,\n",
      "       14.24, 14.25, 14.26, 14.27, 14.28, 14.29, 14.3 , 14.31, 14.32,\n",
      "       14.33, 14.34, 14.35, 14.36, 14.37, 14.38, 14.39, 14.4 , 14.41,\n",
      "       14.42, 14.43, 14.44, 14.45, 14.46, 14.47, 14.48, 14.49, 14.5 ,\n",
      "       14.51, 14.52, 14.53, 14.54, 14.55, 14.56, 14.57, 14.58, 14.59,\n",
      "       14.6 , 14.61, 14.62, 14.63, 14.64, 14.65, 14.66, 14.67, 14.68,\n",
      "       14.69, 14.7 , 14.71, 14.72, 14.73, 14.74, 14.75, 14.76, 14.77,\n",
      "       14.78, 14.79, 14.8 , 14.81, 14.82, 14.83, 14.84, 14.85, 14.86,\n",
      "       14.87, 14.88, 14.89, 14.9 , 14.91, 14.92, 14.93, 14.94, 14.95,\n",
      "       14.96, 14.97, 14.98, 14.99, 15.  , 15.01, 15.02, 15.03, 15.04,\n",
      "       15.05, 15.06, 15.07, 15.08, 15.09, 15.1 , 15.11, 15.12, 15.13,\n",
      "       15.14, 15.15, 15.16, 15.17, 15.18, 15.19, 15.2 , 15.21, 15.22,\n",
      "       15.23, 15.24, 15.25, 15.26, 15.27, 15.28, 15.29, 15.3 , 15.31,\n",
      "       15.32, 15.33, 15.34, 15.35, 15.36, 15.37, 15.38, 15.39, 15.4 ,\n",
      "       15.41, 15.42, 15.43, 15.44, 15.45, 15.46, 15.47, 15.48, 15.49,\n",
      "       15.5 , 15.51, 15.52, 15.53, 15.54, 15.55, 15.56, 15.57, 15.58,\n",
      "       15.59, 15.6 , 15.61, 15.62, 15.63, 15.64, 15.65, 15.66, 15.67,\n",
      "       15.68, 15.69, 15.7 , 15.71, 15.72, 15.73, 15.74, 15.75, 15.76,\n",
      "       15.77, 15.78, 15.79, 15.8 , 15.81, 15.82, 15.83, 15.84, 15.85,\n",
      "       15.86, 15.87, 15.88, 15.89, 15.9 , 15.91, 15.92, 15.93, 15.94,\n",
      "       15.95, 15.96, 15.97, 15.98, 15.99, 16.  , 16.01, 16.02, 16.03,\n",
      "       16.04, 16.05, 16.06, 16.07, 16.08, 16.09, 16.1 , 16.11, 16.12,\n",
      "       16.13, 16.14, 16.15, 16.16, 16.17, 16.18, 16.19, 16.2 , 16.21,\n",
      "       16.22, 16.23, 16.24, 16.25, 16.26, 16.27, 16.28, 16.29, 16.3 ,\n",
      "       16.31, 16.32, 16.33, 16.34, 16.35, 16.36, 16.37, 16.38, 16.39,\n",
      "       16.4 , 16.41, 16.42, 16.43, 16.44, 16.45, 16.46, 16.47, 16.48,\n",
      "       16.49, 16.5 , 16.51, 16.52, 16.53, 16.54, 16.55, 16.56, 16.57,\n",
      "       16.58, 16.59, 16.6 , 16.61, 16.62, 16.63, 16.64, 16.65, 16.66,\n",
      "       16.67, 16.68, 16.69, 16.7 , 16.71, 16.72, 16.73, 16.74, 16.75,\n",
      "       16.76, 16.77, 16.78, 16.79, 16.8 , 16.81, 16.82, 16.83, 16.84,\n",
      "       16.85, 16.86, 16.87, 16.88, 16.89, 16.9 , 16.91, 16.92, 16.93,\n",
      "       16.94, 16.95, 16.96, 16.97, 16.98, 16.99])}\n"
     ]
    }
   ],
   "source": [
    "##### set parameters #####\n",
    "### save parameters ### \n",
    "data_dir = config['data'][0]['params']['base_path'] # '../Data/nu=0.001_n=128/'\n",
    "data_fname = config['data'][0]['params']['dataset_name'] # f'2dHIT_nu=0.001_n=128_T=11.5'\n",
    "\n",
    "metadata = HIT2dDataset(path=data_dir + 'train/' + data_fname, load_data=False).metadata\n",
    "print(metadata)\n",
    "\n",
    "nu = metadata['nu']\n",
    "dt = metadata['dt']\n",
    "N = metadata['Nx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset metadata: dict_keys(['nu', 'Lx', 'Ly', 'dx', 'dy', 'Nx', 'Ny', 'x', 'y', 'dt', 'Nt', 'T', 't0', 't'])\n",
      "Data Loaded from../Data/nu=5e-05_n=512_fDNS=128/train\\2dHIT_nu=5e-05_n=512_T=17.0_fDNS=128_0.hdf5. shape: (250, 600, 1, 128, 128), memory: 9830.4 (MB)\n",
      "Data Loaded from../Data/nu=5e-05_n=512_fDNS=128/train\\2dHIT_nu=5e-05_n=512_T=17.0_fDNS=128_1.hdf5. shape: (250, 600, 1, 128, 128), memory: 9830.4 (MB)\n",
      "Data Loaded. shape: torch.Size([500, 600, 1, 128, 128]), dtype: torch.float32, time: 146.58249187469482 (sec), memory: 19660.8 (MB)\n",
      "Loaded dataset metadata: dict_keys(['nu', 'Lx', 'Ly', 'dx', 'dy', 'Nx', 'Ny', 'x', 'y', 'dt', 'Nt', 'T', 't0', 't'])\n",
      "Data Loaded from../Data/nu=5e-05_n=512_fDNS=128/valid\\2dHIT_nu=5e-05_n=512_T=17.0_fDNS=128_0.hdf5. shape: (30, 600, 1, 128, 128), memory: 1179.648 (MB)\n",
      "Data Loaded. shape: torch.Size([30, 600, 1, 128, 128]), dtype: torch.float32, time: 7.764067649841309 (sec), memory: 1179.648 (MB)\n",
      "Train dataset: 294000\n",
      "Val dataset: 17640\n"
     ]
    }
   ],
   "source": [
    "from utils.Losses import (RMSLoss, TKELoss, DissipationLoss, RelambdaLoss, R2, \n",
    "                          BSMSE, )\n",
    "from utils.utilities import HsLoss\n",
    "\n",
    "##### setup dataset #####\n",
    "train_dataloaders = [] # datamodules = []\n",
    "val_dataloaders = []\n",
    "for i, data_config in enumerate(config['data']):\n",
    "    transform = CustomTransform()\n",
    "    normalization = Standardize(normalization_path=data_config['normalization']['normalization_path'])\n",
    "\n",
    "    idx_leadtime = data_config['idx_leadtime'] if 'idx_leadtime' in data_config else int(data_config['leadtime'] * integral_timescales[nu] / dt) \n",
    "    dm = CustomDataModule(**data_config['params'], \n",
    "        n_stride=idx_leadtime, \n",
    "        # Ndata_train=500, # 500, \n",
    "        # Ndata_val=50, \n",
    "        # Ndata_test=50,\n",
    "        # transform=transform,\n",
    "        normalization=normalization,\n",
    "        load_data=True,\n",
    "        )\n",
    "    stage = data_config['stage']\n",
    "    dm.setup(stage=stage)\n",
    "    if stage in ['fit', 'valid']:\n",
    "        val_dataloaders.append(dm.val_dataloader())\n",
    "    if stage in ['fit', 'train']:\n",
    "        train_dataloaders.append(dm.train_dataloader())\n",
    "\n",
    "test_criterion={\n",
    "    'l2': nn.MSELoss(), \n",
    "    'h1': H1Loss(d=2, reduction='mean'), \n",
    "    'h2': HsLoss(k=2, reduction='mean'), # HsLoss(k=2, group=False, size_average=True), \n",
    "    \n",
    "    'fRMS_k<8': BSMSE(kmax=8, dim=(-2, -1), mode='spectral', isRelative=True,),\n",
    "    'fRMS_8<k<16': BSMSE(kmin=8, kmax=16, dim=(-2, -1), mode='spectral', isRelative=True,),\n",
    "    'fRMS_k>16': BSMSE(kmin=16, dim=(-2, -1), mode='spectral', isRelative=True,),\n",
    "    'fRMS_k<kmax': BSMSE(kmax=config['model']['params']['n_modes'][0]//2, dim=(-2, -1), mode='spectral', isRelative=True,),\n",
    "    'fRMS_k>kmax': BSMSE(kmin=config['model']['params']['n_modes'][0]//2, dim=(-2, -1), mode='spectral', isRelative=True,),\n",
    "    'fRMS_k>train': BSMSE(kmin=64, dim=(-2, -1), mode='spectral', isRelative=True,),\n",
    "    'fRMS_k<train': BSMSE(kmax=64, dim=(-2, -1), mode='spectral', isRelative=True,),\n",
    "\n",
    "    'vor_rms': RMSLoss(dim=2, isRelative=True),\n",
    "    # 'tke': TKELoss(dim=2, isRelative=True), \n",
    "    # 'dissipation': DissipationLoss(nu=nu, dim=2, isRelative=True), \n",
    "    # 'R_lambda': RelambdaLoss(nu=nu, dim=2, isRelative=True), \n",
    "    'R_squared': R2(), \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "============================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
      "============================================================================================================================================\n",
      "FNO                                      [64, 1, 128, 128]         [64, 1, 128, 128]         --                        True\n",
      "â”œâ”€GridEmbeddingND: 1-1                   [64, 1, 128, 128]         [64, 3, 128, 128]         --                        --\n",
      "â”œâ”€ChannelMLP: 1-2                        [64, 3, 128, 128]         [64, 64, 128, 128]        --                        True\n",
      "â”‚    â””â”€ModuleList: 2-1                   --                        --                        --                        True\n",
      "â”‚    â”‚    â””â”€Conv1d: 3-1                  [64, 3, 16384]            [64, 256, 16384]          1,024                     True\n",
      "â”‚    â”‚    â””â”€Conv1d: 3-2                  [64, 256, 16384]          [64, 64, 16384]           16,448                    True\n",
      "â”œâ”€FNOBlocks: 1-3                         [64, 64, 128, 128]        [64, 64, 128, 128]        25,977,504                True\n",
      "â”‚    â””â”€ModuleList: 2-14                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€Flattened1dConv: 3-3         [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
      "â”‚    â”‚    â”‚    â””â”€Conv1d: 4-1             [64, 64, 16384]           [64, 64, 16384]           4,096                     True\n",
      "â”‚    â””â”€ModuleList: 2-15                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€SoftGating: 3-4              [64, 64, 128, 128]        [64, 64, 128, 128]        64                        True\n",
      "â”‚    â””â”€ModuleList: 2-16                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€SpectralConv: 3-5            [64, 64, 128, 128]        [64, 64, 128, 128]        8,650,816                 True\n",
      "â”‚    â””â”€ModuleList: 2-17                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€ChannelMLP: 3-6              [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
      "â”‚    â”‚    â”‚    â””â”€ModuleList: 4-2         --                        --                        --                        True\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-1        [64, 64, 16384]           [64, 32, 16384]           2,080                     True\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-2        [64, 32, 16384]           [64, 64, 16384]           2,112                     True\n",
      "â”œâ”€FNOBlocks: 1-4                         [64, 64, 128, 128]        [64, 64, 128, 128]        (recursive)               True\n",
      "â”‚    â””â”€ModuleList: 2-14                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€Flattened1dConv: 3-7         [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
      "â”‚    â”‚    â”‚    â””â”€Conv1d: 4-3             [64, 64, 16384]           [64, 64, 16384]           4,096                     True\n",
      "â”‚    â””â”€ModuleList: 2-15                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€SoftGating: 3-8              [64, 64, 128, 128]        [64, 64, 128, 128]        64                        True\n",
      "â”‚    â””â”€ModuleList: 2-16                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€SpectralConv: 3-9            [64, 64, 128, 128]        [64, 64, 128, 128]        8,650,816                 True\n",
      "â”‚    â””â”€ModuleList: 2-17                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€ChannelMLP: 3-10             [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
      "â”‚    â”‚    â”‚    â””â”€ModuleList: 4-4         --                        --                        --                        True\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-3        [64, 64, 16384]           [64, 32, 16384]           2,080                     True\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-4        [64, 32, 16384]           [64, 64, 16384]           2,112                     True\n",
      "â”œâ”€FNOBlocks: 1-5                         [64, 64, 128, 128]        [64, 64, 128, 128]        (recursive)               True\n",
      "â”‚    â””â”€ModuleList: 2-14                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€Flattened1dConv: 3-11        [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
      "â”‚    â”‚    â”‚    â””â”€Conv1d: 4-5             [64, 64, 16384]           [64, 64, 16384]           4,096                     True\n",
      "â”‚    â””â”€ModuleList: 2-15                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€SoftGating: 3-12             [64, 64, 128, 128]        [64, 64, 128, 128]        64                        True\n",
      "â”‚    â””â”€ModuleList: 2-16                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€SpectralConv: 3-13           [64, 64, 128, 128]        [64, 64, 128, 128]        8,650,816                 True\n",
      "â”‚    â””â”€ModuleList: 2-17                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€ChannelMLP: 3-14             [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
      "â”‚    â”‚    â”‚    â””â”€ModuleList: 4-6         --                        --                        --                        True\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-5        [64, 64, 16384]           [64, 32, 16384]           2,080                     True\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-6        [64, 32, 16384]           [64, 64, 16384]           2,112                     True\n",
      "â”œâ”€FNOBlocks: 1-6                         [64, 64, 128, 128]        [64, 64, 128, 128]        (recursive)               True\n",
      "â”‚    â””â”€ModuleList: 2-14                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€Flattened1dConv: 3-15        [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
      "â”‚    â”‚    â”‚    â””â”€Conv1d: 4-7             [64, 64, 16384]           [64, 64, 16384]           4,096                     True\n",
      "â”‚    â””â”€ModuleList: 2-15                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€SoftGating: 3-16             [64, 64, 128, 128]        [64, 64, 128, 128]        64                        True\n",
      "â”‚    â””â”€ModuleList: 2-16                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€SpectralConv: 3-17           [64, 64, 128, 128]        [64, 64, 128, 128]        8,650,816                 True\n",
      "â”‚    â””â”€ModuleList: 2-17                  --                        --                        (recursive)               True\n",
      "â”‚    â”‚    â””â”€ChannelMLP: 3-18             [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
      "â”‚    â”‚    â”‚    â””â”€ModuleList: 4-8         --                        --                        --                        True\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-7        [64, 64, 16384]           [64, 32, 16384]           2,080                     True\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-8        [64, 32, 16384]           [64, 64, 16384]           2,112                     True\n",
      "â”œâ”€ChannelMLP: 1-7                        [64, 64, 128, 128]        [64, 1, 128, 128]         --                        True\n",
      "â”‚    â””â”€ModuleList: 2-18                  --                        --                        --                        True\n",
      "â”‚    â”‚    â””â”€Conv1d: 3-19                 [64, 64, 16384]           [64, 128, 16384]          8,320                     True\n",
      "â”‚    â”‚    â””â”€Conv1d: 3-20                 [64, 128, 16384]          [64, 1, 16384]            129                       True\n",
      "============================================================================================================================================\n",
      "Total params: 60,640,097\n",
      "Trainable params: 60,640,097\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 61.94\n",
      "============================================================================================================================================\n",
      "Input size (MB): 4.19\n",
      "Forward/backward pass size (MB): 11282.68\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 11287.11\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
       "============================================================================================================================================\n",
       "FNO                                      [64, 1, 128, 128]         [64, 1, 128, 128]         --                        True\n",
       "â”œâ”€GridEmbeddingND: 1-1                   [64, 1, 128, 128]         [64, 3, 128, 128]         --                        --\n",
       "â”œâ”€ChannelMLP: 1-2                        [64, 3, 128, 128]         [64, 64, 128, 128]        --                        True\n",
       "â”‚    â””â”€ModuleList: 2-1                   --                        --                        --                        True\n",
       "â”‚    â”‚    â””â”€Conv1d: 3-1                  [64, 3, 16384]            [64, 256, 16384]          1,024                     True\n",
       "â”‚    â”‚    â””â”€Conv1d: 3-2                  [64, 256, 16384]          [64, 64, 16384]           16,448                    True\n",
       "â”œâ”€FNOBlocks: 1-3                         [64, 64, 128, 128]        [64, 64, 128, 128]        25,977,504                True\n",
       "â”‚    â””â”€ModuleList: 2-14                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€Flattened1dConv: 3-3         [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
       "â”‚    â”‚    â”‚    â””â”€Conv1d: 4-1             [64, 64, 16384]           [64, 64, 16384]           4,096                     True\n",
       "â”‚    â””â”€ModuleList: 2-15                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€SoftGating: 3-4              [64, 64, 128, 128]        [64, 64, 128, 128]        64                        True\n",
       "â”‚    â””â”€ModuleList: 2-16                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€SpectralConv: 3-5            [64, 64, 128, 128]        [64, 64, 128, 128]        8,650,816                 True\n",
       "â”‚    â””â”€ModuleList: 2-17                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€ChannelMLP: 3-6              [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
       "â”‚    â”‚    â”‚    â””â”€ModuleList: 4-2         --                        --                        --                        True\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-1        [64, 64, 16384]           [64, 32, 16384]           2,080                     True\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-2        [64, 32, 16384]           [64, 64, 16384]           2,112                     True\n",
       "â”œâ”€FNOBlocks: 1-4                         [64, 64, 128, 128]        [64, 64, 128, 128]        (recursive)               True\n",
       "â”‚    â””â”€ModuleList: 2-14                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€Flattened1dConv: 3-7         [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
       "â”‚    â”‚    â”‚    â””â”€Conv1d: 4-3             [64, 64, 16384]           [64, 64, 16384]           4,096                     True\n",
       "â”‚    â””â”€ModuleList: 2-15                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€SoftGating: 3-8              [64, 64, 128, 128]        [64, 64, 128, 128]        64                        True\n",
       "â”‚    â””â”€ModuleList: 2-16                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€SpectralConv: 3-9            [64, 64, 128, 128]        [64, 64, 128, 128]        8,650,816                 True\n",
       "â”‚    â””â”€ModuleList: 2-17                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€ChannelMLP: 3-10             [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
       "â”‚    â”‚    â”‚    â””â”€ModuleList: 4-4         --                        --                        --                        True\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-3        [64, 64, 16384]           [64, 32, 16384]           2,080                     True\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-4        [64, 32, 16384]           [64, 64, 16384]           2,112                     True\n",
       "â”œâ”€FNOBlocks: 1-5                         [64, 64, 128, 128]        [64, 64, 128, 128]        (recursive)               True\n",
       "â”‚    â””â”€ModuleList: 2-14                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€Flattened1dConv: 3-11        [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
       "â”‚    â”‚    â”‚    â””â”€Conv1d: 4-5             [64, 64, 16384]           [64, 64, 16384]           4,096                     True\n",
       "â”‚    â””â”€ModuleList: 2-15                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€SoftGating: 3-12             [64, 64, 128, 128]        [64, 64, 128, 128]        64                        True\n",
       "â”‚    â””â”€ModuleList: 2-16                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€SpectralConv: 3-13           [64, 64, 128, 128]        [64, 64, 128, 128]        8,650,816                 True\n",
       "â”‚    â””â”€ModuleList: 2-17                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€ChannelMLP: 3-14             [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
       "â”‚    â”‚    â”‚    â””â”€ModuleList: 4-6         --                        --                        --                        True\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-5        [64, 64, 16384]           [64, 32, 16384]           2,080                     True\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-6        [64, 32, 16384]           [64, 64, 16384]           2,112                     True\n",
       "â”œâ”€FNOBlocks: 1-6                         [64, 64, 128, 128]        [64, 64, 128, 128]        (recursive)               True\n",
       "â”‚    â””â”€ModuleList: 2-14                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€Flattened1dConv: 3-15        [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
       "â”‚    â”‚    â”‚    â””â”€Conv1d: 4-7             [64, 64, 16384]           [64, 64, 16384]           4,096                     True\n",
       "â”‚    â””â”€ModuleList: 2-15                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€SoftGating: 3-16             [64, 64, 128, 128]        [64, 64, 128, 128]        64                        True\n",
       "â”‚    â””â”€ModuleList: 2-16                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€SpectralConv: 3-17           [64, 64, 128, 128]        [64, 64, 128, 128]        8,650,816                 True\n",
       "â”‚    â””â”€ModuleList: 2-17                  --                        --                        (recursive)               True\n",
       "â”‚    â”‚    â””â”€ChannelMLP: 3-18             [64, 64, 128, 128]        [64, 64, 128, 128]        --                        True\n",
       "â”‚    â”‚    â”‚    â””â”€ModuleList: 4-8         --                        --                        --                        True\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-7        [64, 64, 16384]           [64, 32, 16384]           2,080                     True\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Conv1d: 5-8        [64, 32, 16384]           [64, 64, 16384]           2,112                     True\n",
       "â”œâ”€ChannelMLP: 1-7                        [64, 64, 128, 128]        [64, 1, 128, 128]         --                        True\n",
       "â”‚    â””â”€ModuleList: 2-18                  --                        --                        --                        True\n",
       "â”‚    â”‚    â””â”€Conv1d: 3-19                 [64, 64, 16384]           [64, 128, 16384]          8,320                     True\n",
       "â”‚    â”‚    â””â”€Conv1d: 3-20                 [64, 128, 16384]          [64, 1, 16384]            129                       True\n",
       "============================================================================================================================================\n",
       "Total params: 60,640,097\n",
       "Trainable params: 60,640,097\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 61.94\n",
       "============================================================================================================================================\n",
       "Input size (MB): 4.19\n",
       "Forward/backward pass size (MB): 11282.68\n",
       "Params size (MB): 0.24\n",
       "Estimated Total Size (MB): 11287.11\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "pl_module = CustomModule(\n",
    "        modelconfig=config['model'], \n",
    "        optconfig=config['optimizer'], \n",
    "        )\n",
    "batch_size = config['data'][0]['params']['batch_size']\n",
    "print(pl_module.device)\n",
    "summary(\n",
    "        pl_module.model,\n",
    "        input_size=(batch_size, 1, N, N),       # batch_size, in_channels, H, W\n",
    "        col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    "        depth=5,                           # how many nested modules to show\n",
    "        verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.CFDCallback_Spectrum import CFDCallback_Spectrum\n",
    "from utils.CFDCallback_PhaseError import CFDCallback_PhaseError\n",
    "from utils.CFDCallback_PDF import CFDCallback_PDF\n",
    "from utils.CFDCallback_Field import CFDCallback_Field\n",
    "\n",
    "def main(device='cuda'):\n",
    "    logger = wandb.init(project=config['name'], \n",
    "                        # name=config['name'], \n",
    "                        config=config\n",
    "                        )\n",
    "    if 'seed' in config: torch.manual_seed(config['seed'])\n",
    "\n",
    "    data_config = config['data'][0]\n",
    "    fname=f\"2dHIT_{config['model']['name']}_nu={data_config['nu']}\"\n",
    "    fname += f\"_T={data_config['idx_leadtime']}\" if 'idx_leadtime' in data_config else f\"_T={data_config['leadtime']}TL\"\n",
    "    if logger: fname += f'_{logger.name}'\n",
    "\n",
    "    optconfig = config['optimizer']\n",
    "    if 'loss_fn' in config: optconfig['loss_fn'] = config['loss_fn']\n",
    "    if 'lr' in config: optconfig['params']['lr'] = config['lr']\n",
    "    modelconfig = {**config['model'], }\n",
    "    modelconfig['params'].update({k: v for k, v in config.items() if k in config['model']['params']})\n",
    "    \n",
    "\n",
    "    pl_module = CustomModule(\n",
    "        modelconfig=config['model'], \n",
    "        optconfig=optconfig,  \n",
    "        )\n",
    "    # wandb.watch(pl_module, log=\"all\", log_freq=100)\n",
    "    # pl_module.load('./pretrain/2dHIT_FNOKernel_nu=5e-05_T=0.1TL_scarlet-surf-2_top0', \n",
    "    #     load_opt=False, \n",
    "    #     freeze=True, \n",
    "    #     )\n",
    "    callbacks = [\n",
    "            CFDCallback_Spectrum(file_dir='./result/', fname=fname), \n",
    "            CFDCallback_PhaseError(file_dir='./result/', fname=fname), \n",
    "            CFDCallback_PDF(file_dir='./result/', fname=fname), \n",
    "            CFDCallback_Field(file_dir='./result/', fname=fname), \n",
    "            CustomCallback(criterions=test_criterion, file_dir='./result/', fname=fname, device=device,\n",
    "                        ),\n",
    "            CheckPoint(\n",
    "                ckpt_name=fname, ckpt_path='./checkpoint/', \n",
    "                every_n_epoch = 1,\n",
    "                criterion=nn.MSELoss(), mode = 'min', \n",
    "                load_ckpt=False, \n",
    "                ),\n",
    "            EarlyStopping(criterion=nn.MSELoss(), # neuralop.H1Loss(d=2, reductions='mean'),# \n",
    "                            mode='min', min_delta=1e-5, \n",
    "                            patience=5, verbose=True, divergence_threshold=1e3, \n",
    "                            stopping_threshold=1e-3, \n",
    "                            ),\n",
    "            ]\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config['epochs'],\n",
    "        check_val_every_n_epochs=1,\n",
    "        # check_val_every_n_iter=30000, \n",
    "        enable_progress_bar=True,\n",
    "        callbacks=callbacks, \n",
    "        logger=logger,\n",
    "        )\n",
    "    \n",
    "    trainer.fit(\n",
    "        model=pl_module,\n",
    "        train_dataloaders=train_dataloaders, \n",
    "        val_dataloaders=val_dataloaders,\n",
    "        # datamodule = datamodules,\n",
    "        )\n",
    "    if logger: wandb.finish()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: hjungwon034 (jungwonheo) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\RESEARCH\\2DIso\\DL_FNO\\wandb\\run-20250902_211544-6vjkquku</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jungwonheo/2dHIT_FNO/runs/6vjkquku' target=\"_blank\">pretty-serenity-230</a></strong> to <a href='https://wandb.ai/jungwonheo/2dHIT_FNO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jungwonheo/2dHIT_FNO' target=\"_blank\">https://wandb.ai/jungwonheo/2dHIT_FNO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jungwonheo/2dHIT_FNO/runs/6vjkquku' target=\"_blank\">https://wandb.ai/jungwonheo/2dHIT_FNO/runs/6vjkquku</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4593/4593 [16:07<00:00,  4.75it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275/275 [00:42<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FNOv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
